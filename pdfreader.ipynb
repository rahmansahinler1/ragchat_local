{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "nlp =  spacy.load(\"en_core_web_sm\")\n",
    "import pdfplumber \n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_checker(text):\n",
    "        spell = SpellChecker()\n",
    "        docs = nlp(text)\n",
    "        correct_words = []    \n",
    "        for token in docs:\n",
    "            word = token.text\n",
    "\n",
    "            if token.is_alpha:\n",
    "                if word.lower() in spell.unknown([word]):\n",
    "                    correction = spell.correction(word)\n",
    "                    correct_words.append(correction)\n",
    "                else:\n",
    "                    correct_words.append(word)\n",
    "            else:\n",
    "                correct_words.append(word)\n",
    "        return ' '.join(correct_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"/Users/ozgur.sahin/Documents/ragchat_local/docs/domain1/2406.16563v2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.1\n"
     ]
    }
   ],
   "source": [
    "print(PyPDF2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEDICAL GRAPH RAG: TOWARDS SAFE MEDICAL LARGE LANGUAGE MODEL VIA GRAPH RETRIEVAL- AUGMENTED GENERATION JundeWu JiayuanZhu YunliQi UniversityofOxford UniversityofOxford UniversityofOxford jundewu@ieee.org ABSTRACT Weintroduceanovelgraph-basedRetrieval-AugmentedGeneration(RAG)frame- workspecificallydesignedforthemedicaldomain,calledMedGraphRAG,aimed atenhancingLargeLanguageModel(LLM)capabilitiesandgeneratingevidence- based results, thereby improving safety and reliability when handling private medicaldata. Ourcomprehensivepipelinebeginswithahybridstatic-semantic approach to document chunking, significantly improving context capture over traditional methods. Extracted entities are used to create a three-tier hierarchi- calgraphstructure,linkingentitiestofoundationalmedicalknowledgesourced frommedicalpapersanddictionaries. Theseentitiesaretheninterconnectedto formmeta-graphs,whicharemergedbasedonsemanticsimilaritiestodevelopa comprehensiveglobalgraph. Thisstructuresupportspreciseinformationretrieval andresponsegeneration. TheretrievalprocessemploysaU-retrievemethodto balanceglobalawarenessandindexingefficiencyoftheLLM.Ourapproachis validatedthroughacomprehensiveablationstudycomparingvariousmethodsfor documentchunking,graphconstruction,andinformationretrieval. Theresultsnot onlydemonstratethatourhierarchicalgraphconstructionmethodconsistentlyout- performsstate-of-the-artmodelsonmultiplemedicalQ&Abenchmarks,butalso confirmsthattheresponsesgeneratedincludesourcedocumentation,significantly enhancingthereliabilityofmedicalLLMsinpracticalapplications. 1 INTRODUCTION The rapid advancement of large language models (LLMs), such as OpenAI’s ChatGPT OpenAI (2023a) and GPT-4 OpenAI (2023b), has significantly transformed research in natural language processingandsparkednumerousAIapplicationsineverydayscenarios. However,thesemodels stillfacelimitationswhenappliedtofieldsrequiringspecializedknowledge,suchasfinance,law, andmedicine. Therearetwoprimarychallenges: First,deployingtrainedLLMsforspecificusesis complexduetotheirstruggleswithextremelylongcontextsandthehighcostsorimpracticalityof fine-tuninglargemodelsonspecializeddatasets. Second,indomainslikemedicinewhereprecision is crucial, LLMs may produce hallucinations—outputs that seem accurate but lead to incorrect conclusions, which can be dangerous. Additionally, they sometimes provide overly simplistic answerswithoutofferingnewinsightsordiscoveries,whichfallsshortinfieldsthatdemandhigh- levelreasoningtoderivecorrectanswers. Retrieval-augmentedgeneration(RAG)Lewisetal.(2021)isatechniquethatanswersuserqueries usingspecificandprivatedatasetswithoutrequiringfurthertrainingofthemodel.Originallydesigned forsituationswherethenecessaryanswersarefoundwithinspecifictextregions,RAGsometimes strugglestosynthesizenewinsightsfromdisparatepiecesofinformationlinkedbysharedattributes. Additionally,itunderperformsintasksrequiringaholisticunderstandingofsummarizedsemantic conceptsacrosslargedatasetsorextensivedocuments. Toaddresstheselimitations,thegraphRAG Huetal.(2024)methodhasbeenintroduced. ThisapproachleveragesLLMstocreateaknowledge graphfromtheprivatedataset,which,inconjunctionwithgraphmachinelearning,enhancesprompt augmentation during query processing. GraphRAG has demonstrated significant improvements, 1 4202 guA 8 ]VC.sc[ 1v78140.8042:viXra\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(path) as pdf:\n",
    "    all_text = ''\n",
    "    page = pdf.pages[0]\n",
    "    text = page.extract_text()\n",
    "    clean_text = text.replace('\\n',' ')\n",
    "    print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.7 Detailed error results N1alter and N2alter are sequence errors. 18\n"
     ]
    }
   ],
   "source": [
    "with path.open('rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page in pdf_reader.pages:\n",
    "              page_text = page.extract_text(orientations=0)\n",
    "              clean_page_text = page_text.replace('\\n',' ').strip()\n",
    "              clean_page_text = re.sub(r'\\bwww\\.[^\\s]*?\\.(com|org|net)\\b.*?\\.','', clean_page_text,flags=re.IGNORECASE | re.DOTALL) # clean links \n",
    "              clean_page_text = re.sub(r'^[\\d*]+[^\\s]+\\s+.*$','', clean_page_text,flags=re.MULTILINE)\n",
    "              clean_page_text = re.sub(r'[^A-Za-z0-9\\s.,!?\\'\"-]', '',clean_page_text) #\n",
    "              clean_page_text = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', clean_page_text)#edit 'T OWARDS' to 'TOWARDS'\n",
    "              clean_page_text = re.sub(r'\\s+([A-Z])\\s+([A-Z])', r' \\1\\2', clean_page_text)#edit 'A TOWARDS'\n",
    "              clean_page_text = re.sub(r'(\\d+)\\s*\\n\\s*([A-Za-z])', r'\\1 \\2', clean_page_text)\n",
    "              clean_page_text = re.sub(r'^[\\w\\.-]+[a-zA-Z0-9\\.-]+\\.[a-zA-Z]{2,}$', r' ', clean_page_text) # clean emails\n",
    "              clean_page_text = re.sub(r'https?[\\w\\.-]+\\.[\\w\\.-]+', r' ', clean_page_text) # clean urls\n",
    "              clean_page_text = re.sub(r'(?i)\\btable\\s+\\d+[:.]*\\s*[^.]*\\.',r' ', clean_page_text) # clean tables\n",
    "              clean_page_text = re.sub(r'\\b(Fig|Figure)\\b.*?\\.','', clean_page_text,flags=re.IGNORECASE | re.DOTALL) #clean fig or figure texts\n",
    "              clean_page_text = re.sub(r'\\s\\d\\s','', clean_page_text) #clean ' 1 ' strings \n",
    "              clean_page_text = re.sub(r'\\s+', ' ',clean_page_text)\n",
    "              text += clean_page_text + ' '\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are\n",
      "there\n",
      "identifiable\n",
      "structural\n",
      "parts\n",
      "in\n",
      "the\n",
      "sentence\n",
      "embedding\n",
      "whole\n",
      "?\n",
      "Vivi\n",
      "Nastase1and\n",
      "Paola\n",
      "Merlo1,2\n",
      "1Idiap\n",
      "Research\n",
      "Institute\n",
      ",\n",
      "Martigny\n",
      ",\n",
      "Switzerland\n",
      "2University\n",
      "of\n",
      "Geneva\n",
      ",\n",
      "Swizerland\n",
      "vivi.a.nastasegmail.com\n",
      ",\n",
      "Paola.Merlounige.ch\n",
      "Abstract\n",
      "Sentence\n",
      "embeddings\n",
      "from\n",
      "transformer\n",
      "models\n",
      "encode\n",
      "in\n",
      "a\n",
      "fixed\n",
      "length\n",
      "vector\n",
      "much\n",
      "linguistic\n",
      "information\n",
      ".\n",
      "We\n",
      "explore\n",
      "the\n",
      "hypothesis\n",
      "that\n",
      "these\n",
      "embeddings\n",
      "consist\n",
      "of\n",
      "overlapping\n",
      "layers\n",
      "of\n",
      "information\n",
      "that\n",
      "can\n",
      "be\n",
      "separated\n",
      ",\n",
      "and\n",
      "on\n",
      "which\n",
      "specific\n",
      "types\n",
      "of\n",
      "information\n",
      "such\n",
      "as\n",
      "information\n",
      "about\n",
      "chunks\n",
      "and\n",
      "their\n",
      "structural\n",
      "and\n",
      "semantic\n",
      "properties\n",
      "can\n",
      "be\n",
      "detected\n",
      ".\n",
      "We\n",
      "show\n",
      "that\n",
      "this\n",
      "is\n",
      "the\n",
      "case\n",
      "using\n",
      "a\n",
      "dataset\n",
      "consisting\n",
      "of\n",
      "sentences\n",
      "with\n",
      "known\n",
      "chunk\n",
      "structure\n",
      ",\n",
      "and\n",
      "two\n",
      "linguistic\n",
      "intelligence\n",
      "datasets\n",
      ",\n",
      "solving\n",
      "which\n",
      "relies\n",
      "on\n",
      "detecting\n",
      "chunks\n",
      "and\n",
      "their\n",
      "grammatical\n",
      "number\n",
      ",\n",
      "and\n",
      "respectively\n",
      ",\n",
      "their\n",
      "semantic\n",
      "roles\n",
      ",\n",
      "and\n",
      "through\n",
      "analyses\n",
      "of\n",
      "the\n",
      "performance\n",
      "on\n",
      "the\n",
      "tasks\n",
      "and\n",
      "of\n",
      "the\n",
      "internal\n",
      "representations\n",
      "built\n",
      "during\n",
      "learning\n",
      ".\n",
      "Introduction\n",
      "Transformer\n",
      "architectures\n",
      "compress\n",
      "the\n",
      "information\n",
      "in\n",
      "a\n",
      "sentence\n",
      "morphological\n",
      ",\n",
      "grammatical\n",
      ",\n",
      "semantic\n",
      ",\n",
      "pragmatic\n",
      "into\n",
      "a\n",
      "one\n",
      "dimensional\n",
      "array\n",
      "of\n",
      "real\n",
      "numbers\n",
      "of\n",
      "fixed\n",
      "length\n",
      ".\n",
      "Sentence\n",
      "embeddings\n",
      "usually\n",
      "fine\n",
      "-\n",
      "tuned\n",
      "have\n",
      "proven\n",
      "useful\n",
      "for\n",
      "a\n",
      "variety\n",
      "of\n",
      "high\n",
      "-\n",
      "level\n",
      "language\n",
      "processing\n",
      "tasks\n",
      "e.g.\n",
      "the\n",
      "GLUE\n",
      "tasks\n",
      "Clark\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2020\n",
      ",\n",
      "story\n",
      "continuation\n",
      "Ippolito\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2020\n",
      ".\n",
      "Such\n",
      "higher\n",
      "-\n",
      "level\n",
      "tasks\n",
      ",\n",
      "however\n",
      ",\n",
      "might\n",
      "not\n",
      "necessarily\n",
      "require\n",
      "specific\n",
      "structural\n",
      "information\n",
      ".\n",
      "Sentence\n",
      "embeddings\n",
      "built\n",
      "using\n",
      "a\n",
      "BiLSTM\n",
      "model\n",
      "do\n",
      "seem\n",
      "to\n",
      "encode\n",
      "a\n",
      "range\n",
      "of\n",
      "information\n",
      ",\n",
      "from\n",
      "shallow\n",
      "e.g.\n",
      "sentence\n",
      "length\n",
      ",\n",
      "word\n",
      "order\n",
      "to\n",
      "syntactic\n",
      "e.g.\n",
      "tree\n",
      "depth\n",
      ",\n",
      "top\n",
      "constituent\n",
      "and\n",
      "semantic\n",
      "e.g.\n",
      "tense\n",
      ",\n",
      "semantic\n",
      "mismatches\n",
      "Conneau\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2018\n",
      ".\n",
      "Investigation\n",
      ",\n",
      "or\n",
      "indeed\n",
      ",\n",
      "usage\n",
      ",\n",
      "of\n",
      "raw\n",
      "i.e.\n",
      "not\n",
      "fine\n",
      "-\n",
      "tuned\n",
      "sentence\n",
      "embeddings\n",
      "obtained\n",
      "from\n",
      "a\n",
      "transformer\n",
      "model\n",
      "are\n",
      "rare\n",
      ",\n",
      "possibly\n",
      "because\n",
      "most\n",
      "transformer\n",
      "models\n",
      "do\n",
      "not\n",
      "have\n",
      "a\n",
      "strong\n",
      "supervision\n",
      "signal\n",
      "on\n",
      "the\n",
      "sentence\n",
      "embedding\n",
      ".\n",
      "An\n",
      "investigation\n",
      "of\n",
      "the\n",
      "dimensions\n",
      "of\n",
      "BERT\n",
      "sentence\n",
      "embeddings\n",
      "using\n",
      "The\n",
      "work\n",
      "was\n",
      "done\n",
      "while\n",
      "the\n",
      "authors\n",
      "were\n",
      "at\n",
      "the\n",
      "University\n",
      "of\n",
      "Geneva.principal\n",
      "component\n",
      "analysis\n",
      "indicated\n",
      "that\n",
      "there\n",
      "is\n",
      "much\n",
      "correlation\n",
      "and\n",
      "redundancy\n",
      ",\n",
      "and\n",
      "that\n",
      "they\n",
      "encode\n",
      "more\n",
      "shallow\n",
      "information\n",
      "length\n",
      ",\n",
      "rather\n",
      "than\n",
      "morphological\n",
      ",\n",
      "syntactic\n",
      "or\n",
      "semantic\n",
      "features\n",
      "Nikolaev\n",
      "and\n",
      "Pad\n",
      ",\n",
      "2023c\n",
      ".\n",
      "Moreover\n",
      ",\n",
      "analysis\n",
      "of\n",
      "information\n",
      "propagation\n",
      "through\n",
      "the\n",
      "model\n",
      "layers\n",
      ",\n",
      "and\n",
      "analysis\n",
      "of\n",
      "the\n",
      "sentence\n",
      "embeddings\n",
      "seem\n",
      "to\n",
      "show\n",
      "that\n",
      "much\n",
      "specialized\n",
      "information\n",
      "e.g.\n",
      "POS\n",
      ",\n",
      "syntactic\n",
      "structure\n",
      "while\n",
      "quite\n",
      "apparent\n",
      "at\n",
      "lower\n",
      "levels\n",
      ",\n",
      "gets\n",
      "lost\n",
      "towards\n",
      "the\n",
      "highest\n",
      "levels\n",
      "of\n",
      "the\n",
      "models\n",
      "Rogers\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2020\n",
      ".\n",
      "We\n",
      "hypothesize\n",
      "that\n",
      "different\n",
      "types\n",
      "of\n",
      "information\n",
      "are\n",
      "melded\n",
      "together\n",
      ",\n",
      "and\n",
      "no\n",
      "longer\n",
      "overtly\n",
      "accessible\n",
      "in\n",
      "the\n",
      "sentence\n",
      "embeddings\n",
      ".\n",
      "A\n",
      "raw\n",
      "sentence\n",
      "embedding\n",
      "the\n",
      "encoding\n",
      "of\n",
      "the\n",
      "special\n",
      "CLS\n",
      "s\n",
      "token\n",
      "from\n",
      "the\n",
      "output\n",
      "of\n",
      "a\n",
      "pretrained\n",
      "transformer\n",
      ",\n",
      "not\n",
      "fine\n",
      "-\n",
      "tuned\n",
      "for\n",
      "a\n",
      "specific\n",
      "task\n",
      "consists\n",
      "of\n",
      "overlapping\n",
      "layers2of\n",
      "information\n",
      ",\n",
      "similarly\n",
      "to\n",
      "an\n",
      "audio\n",
      "signal\n",
      "that\n",
      "is\n",
      "a\n",
      "combination\n",
      "of\n",
      "waves\n",
      "of\n",
      "different\n",
      "frequencies\n",
      ".\n",
      "The\n",
      "various\n",
      "types\n",
      "of\n",
      "information\n",
      "from\n",
      "the\n",
      "sentence\n",
      "structural\n",
      ",\n",
      "semantic\n",
      ",\n",
      "etc\n",
      ".\n",
      "are\n",
      "encoded\n",
      "on\n",
      "some\n",
      "of\n",
      "these\n",
      "layers\n",
      ".\n",
      "We\n",
      "use\n",
      "a\n",
      "convolutional\n",
      "neural\n",
      "network\n",
      "to\n",
      "separate\n",
      "different\n",
      "layers\n",
      "of\n",
      "information\n",
      "in\n",
      "a\n",
      "sentence\n",
      "embedding\n",
      ",\n",
      "and\n",
      "test\n",
      "whether\n",
      "syntactic\n",
      "and\n",
      "semantic\n",
      "structure\n",
      "noun\n",
      ",\n",
      "verb\n",
      "and\n",
      "prepositional\n",
      "phrases\n",
      ",\n",
      "that\n",
      "may\n",
      "play\n",
      "different\n",
      "structural\n",
      "and\n",
      "semantic\n",
      "roles\n",
      "can\n",
      "be\n",
      "identified\n",
      "on\n",
      "these\n",
      "layers\n",
      ".\n",
      "Understanding\n",
      "what\n",
      "kind\n",
      "of\n",
      "information\n",
      "the\n",
      "sentence\n",
      "embeddings\n",
      "encode\n",
      ",\n",
      "and\n",
      "how\n",
      ",\n",
      "has\n",
      "multiple\n",
      "benefits\n",
      "it\n",
      "connects\n",
      "internal\n",
      "changes\n",
      "in\n",
      "the\n",
      "model\n",
      "parameters\n",
      "and\n",
      "structure\n",
      "with\n",
      "changes\n",
      "in\n",
      "its\n",
      "outputs\n",
      "it\n",
      "contributes\n",
      "to\n",
      "verifying\n",
      "the\n",
      "robustness\n",
      "of\n",
      "models\n",
      "and\n",
      "whether\n",
      "or\n",
      "not\n",
      "they\n",
      "rely\n",
      "on\n",
      "shallow\n",
      "or\n",
      "accidental\n",
      "regularities\n",
      "in\n",
      "the\n",
      "data\n",
      "it\n",
      "narrows\n",
      "down\n",
      "the\n",
      "field\n",
      "of\n",
      "search\n",
      "when\n",
      "a\n",
      "language\n",
      "model\n",
      "produces\n",
      "wrong\n",
      "outputs\n",
      ",\n",
      "and\n",
      "it\n",
      "helps\n",
      "maximize\n",
      "the\n",
      "use\n",
      "of\n",
      "training\n",
      "data\n",
      "for\n",
      "developing\n",
      "more\n",
      "robust\n",
      "models\n",
      "2Throughout\n",
      "this\n",
      "paper\n",
      ",\n",
      "by\n",
      "\"\n",
      "layer\n",
      "\"\n",
      "we\n",
      "mean\n",
      "\"\n",
      "a\n",
      "stratum\n",
      "of\n",
      "information\n",
      "\"\n",
      ",\n",
      "not\n",
      "the\n",
      "layers\n",
      "of\n",
      "a\n",
      "transformer\n",
      "architecture\n",
      ".\n",
      "1\n",
      "from\n",
      "smaller\n",
      "textual\n",
      "resources.3Related\n",
      "work\n",
      "How\n",
      "is\n",
      "the\n",
      "information\n",
      "from\n",
      "a\n",
      "textual\n",
      "input\n",
      "encoded\n",
      "by\n",
      "transformers\n",
      "?\n",
      "There\n",
      "are\n",
      "two\n",
      "main\n",
      "approaches\n",
      "to\n",
      "answer\n",
      "this\n",
      "question\n",
      "i\n",
      "tracing\n",
      "specific\n",
      "information\n",
      "from\n",
      "input\n",
      "to\n",
      "output\n",
      "through\n",
      "the\n",
      "models\n",
      "various\n",
      "layers\n",
      "and\n",
      "components\n",
      ",\n",
      "and\n",
      "ii\n",
      "investigating\n",
      "the\n",
      "generated\n",
      "embeddings\n",
      ".\n",
      "These\n",
      "investigations\n",
      "rely\n",
      "on\n",
      "probing\n",
      "the\n",
      "models\n",
      ",\n",
      "using\n",
      "purposefully\n",
      "built\n",
      "data\n",
      "that\n",
      "can\n",
      "implement\n",
      "different\n",
      "types\n",
      "of\n",
      "testing\n",
      ".\n",
      "Tracing\n",
      "information\n",
      "through\n",
      "a\n",
      "transformer\n",
      "Rogers\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2020\n",
      "have\n",
      "shown\n",
      "that\n",
      "from\n",
      "the\n",
      "unstructured\n",
      "textual\n",
      "input\n",
      ",\n",
      "BERT\n",
      "Devlin\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      "is\n",
      "able\n",
      "to\n",
      "infer\n",
      "POS\n",
      ",\n",
      "structural\n",
      ",\n",
      "entity\n",
      "-\n",
      "related\n",
      ",\n",
      "syntactic\n",
      "and\n",
      "semantic\n",
      "information\n",
      "at\n",
      "successively\n",
      "higher\n",
      "layers\n",
      "of\n",
      "the\n",
      "architecture\n",
      ",\n",
      "mirroring\n",
      "the\n",
      "classical\n",
      "NLP\n",
      "pipeline\n",
      "Tenney\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019a\n",
      ".\n",
      "Further\n",
      "studies\n",
      "have\n",
      "shown\n",
      "that\n",
      "the\n",
      "information\n",
      "is\n",
      "not\n",
      "sharply\n",
      "separated\n",
      ",\n",
      "information\n",
      "from\n",
      "higher\n",
      "level\n",
      "can\n",
      "influence\n",
      "information\n",
      "at\n",
      "lower\n",
      "levels\n",
      ",\n",
      "such\n",
      "as\n",
      "POS\n",
      "in\n",
      "multilingual\n",
      "models\n",
      "de\n",
      "Vries\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2020\n",
      ",\n",
      "or\n",
      "subject\n",
      "-\n",
      "verb\n",
      "agreement\n",
      "Jawahar\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      ".\n",
      "Surface\n",
      "syntactic\n",
      "and\n",
      "semantic\n",
      "information\n",
      "seem\n",
      "to\n",
      "be\n",
      "distributed\n",
      "throughout\n",
      "BERTs\n",
      "layers\n",
      "Niu\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2022\n",
      "Nikolaev\n",
      "and\n",
      "Pad\n",
      ",\n",
      "2023c\n",
      ".\n",
      "Attention\n",
      "is\n",
      "part\n",
      "of\n",
      "the\n",
      "process\n",
      ",\n",
      "as\n",
      "it\n",
      "helps\n",
      "encode\n",
      "various\n",
      "types\n",
      "of\n",
      "linguistic\n",
      "information\n",
      "Rogers\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2020\n",
      "Clark\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      ",\n",
      "syntactic\n",
      "dependencies\n",
      "Htut\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      ",\n",
      "grammatical\n",
      "structure\n",
      "Luo\n",
      ",\n",
      "2021\n",
      ",\n",
      "and\n",
      "can\n",
      "contribute\n",
      "towards\n",
      "semantic\n",
      "role\n",
      "labeling\n",
      "Tan\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2018\n",
      "Strubell\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2018\n",
      ".\n",
      "Word\n",
      "embeddings\n",
      "were\n",
      "shown\n",
      "to\n",
      "encode\n",
      "sentence\n",
      "-\n",
      "level\n",
      "information\n",
      "Tenney\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019b\n",
      ",\n",
      "including\n",
      "syntactic\n",
      "structure\n",
      "Hewitt\n",
      "and\n",
      "Manning\n",
      ",\n",
      "2019\n",
      ",\n",
      "even\n",
      "in\n",
      "multilingual\n",
      "models\n",
      "Chi\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2020\n",
      ".\n",
      "Predicate\n",
      "embeddings\n",
      "contain\n",
      "information\n",
      "about\n",
      "its\n",
      "semantic\n",
      "roles\n",
      "structure\n",
      "Conia\n",
      "and\n",
      "Navigli\n",
      ",\n",
      "2022\n",
      ",\n",
      "embeddings\n",
      "of\n",
      "nouns\n",
      "encode\n",
      "subjecthood\n",
      "and\n",
      "objecthood\n",
      "Papadimitriou\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2021\n",
      ".\n",
      "The\n",
      "averaged\n",
      "token\n",
      "embeddings\n",
      "are\n",
      "more\n",
      "commonly\n",
      "used\n",
      "as\n",
      "sentence\n",
      "embeddings\n",
      "e.g.\n",
      "Nikolaev\n",
      "and\n",
      "Pad\n",
      ",\n",
      "2023a\n",
      ",\n",
      "or\n",
      "the\n",
      "special\n",
      "token\n",
      "CLS\n",
      "s\n",
      "embeddings\n",
      "are\n",
      "fine\n",
      "-\n",
      "tuned\n",
      "for\n",
      "specific\n",
      "tasks\n",
      "such\n",
      "as\n",
      "story\n",
      "continuation\n",
      "Ippolito\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2020\n",
      ",\n",
      "sentence\n",
      "similarity\n",
      "Reimers\n",
      "and\n",
      "Gurevych\n",
      ",\n",
      "2019\n",
      ",\n",
      "alignment\n",
      "to\n",
      "semantic\n",
      "features\n",
      "3We\n",
      "will\n",
      "share\n",
      "the\n",
      "code\n",
      "and\n",
      "sentence\n",
      "data\n",
      "upon\n",
      "acceptance\n",
      ".\n",
      "The\n",
      "other\n",
      "datasets\n",
      "are\n",
      "publicly\n",
      "available\n",
      ".\n",
      "Opitz\n",
      "and\n",
      "Frank\n",
      ",\n",
      "2022\n",
      ".\n",
      "This\n",
      "token\n",
      "averaging\n",
      "is\n",
      "justifiable\n",
      "as\n",
      "the\n",
      "learning\n",
      "signal\n",
      "for\n",
      "transformer\n",
      "models\n",
      "is\n",
      "stronger\n",
      "at\n",
      "the\n",
      "token\n",
      "level\n",
      ",\n",
      "with\n",
      "a\n",
      "much\n",
      "weaker\n",
      "objective\n",
      "at\n",
      "the\n",
      "sentence\n",
      "level\n",
      "e.g.\n",
      "next\n",
      "sentence\n",
      "prediction\n",
      "Devlin\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2018\n",
      "Liu\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      ",\n",
      "sentence\n",
      "order\n",
      "prediction\n",
      "Lan\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      ".\n",
      "Electra\n",
      "Clark\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2020\n",
      "does\n",
      "not\n",
      "either\n",
      ",\n",
      "but\n",
      "it\n",
      "relies\n",
      "on\n",
      "replaced\n",
      "token\n",
      "detection\n",
      ",\n",
      "which\n",
      "uses\n",
      "the\n",
      "sentence\n",
      "context\n",
      "to\n",
      "determine\n",
      "whether\n",
      "a\n",
      "number\n",
      "of\n",
      "tokens\n",
      "in\n",
      "the\n",
      "given\n",
      "sentence\n",
      "were\n",
      "replaced\n",
      "by\n",
      "a\n",
      "generator\n",
      "sample\n",
      ".\n",
      "This\n",
      "training\n",
      "regime\n",
      "leads\n",
      "to\n",
      "sentence\n",
      "embeddings\n",
      "that\n",
      "perform\n",
      "well\n",
      "on\n",
      "the\n",
      "General\n",
      "Language\n",
      "Understanding\n",
      "Evaluation\n",
      "GLUE\n",
      "benchmark\n",
      "Wang\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2018\n",
      "and\n",
      "Stanford\n",
      "Question\n",
      "Answering\n",
      "SQuAD\n",
      "dataset\n",
      "Rajpurkar\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2016\n",
      ",\n",
      "or\n",
      "detecting\n",
      "verb\n",
      "classes\n",
      "Yi\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2022\n",
      ".\n",
      "Raw\n",
      "sentence\n",
      "embeddings\n",
      "also\n",
      "seemed\n",
      "to\n",
      "capture\n",
      "shallower\n",
      "information\n",
      "Nikolaev\n",
      "and\n",
      "Pad\n",
      ",\n",
      "2023c\n",
      ",\n",
      "but\n",
      "Nastase\n",
      "and\n",
      "Merlo\n",
      "2023\n",
      "show\n",
      "that\n",
      "raw\n",
      "sentence\n",
      "embeddings\n",
      "have\n",
      "internal\n",
      "structure\n",
      "that\n",
      "can\n",
      "encode\n",
      "grammatical\n",
      "sentence\n",
      "properties\n",
      ".\n",
      "Probing\n",
      "models\n",
      "Analysis\n",
      "of\n",
      "BERTs\n",
      "inner\n",
      "workings\n",
      "has\n",
      "been\n",
      "done\n",
      "using\n",
      "probing\n",
      "classifiers\n",
      "Belinkov\n",
      ",\n",
      "2022\n",
      ",\n",
      "or\n",
      "through\n",
      "clustering\n",
      "based\n",
      "on\n",
      "the\n",
      "representations\n",
      "at\n",
      "the\n",
      "different\n",
      "levels\n",
      "Jawahar\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      ".\n",
      "Probing\n",
      "has\n",
      "also\n",
      "been\n",
      "used\n",
      "to\n",
      "investigate\n",
      "the\n",
      "representations\n",
      "obtained\n",
      "from\n",
      "a\n",
      "pretrained\n",
      "transformer\n",
      "model\n",
      "Conneau\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2018\n",
      ".\n",
      "Elazar\n",
      "et\n",
      "al\n",
      ".\n",
      "2021\n",
      "propose\n",
      "amnesic\n",
      "probing\n",
      "to\n",
      "test\n",
      "both\n",
      "whether\n",
      "some\n",
      "information\n",
      "is\n",
      "encoded\n",
      ",\n",
      "and\n",
      "whether\n",
      "it\n",
      "is\n",
      "used\n",
      ".\n",
      "VAE\n",
      "-\n",
      "based\n",
      "methods\n",
      "Kingma\n",
      "and\n",
      "Welling\n",
      ",\n",
      "2013\n",
      "Bowman\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2016\n",
      "have\n",
      "been\n",
      "used\n",
      "to\n",
      "detect\n",
      "or\n",
      "separate\n",
      "specific\n",
      "information\n",
      "from\n",
      "input\n",
      "representations\n",
      ".\n",
      "Mercatali\n",
      "and\n",
      "Freitas\n",
      "2021\n",
      "capture\n",
      "discrete\n",
      "properties\n",
      "of\n",
      "sentences\n",
      "encoded\n",
      "with\n",
      "an\n",
      "LSTM\n",
      "e.g.\n",
      "number\n",
      "and\n",
      "aspect\n",
      "of\n",
      "verbs\n",
      "on\n",
      "the\n",
      "latent\n",
      "layer\n",
      ".\n",
      "Bao\n",
      "et\n",
      "al\n",
      ".\n",
      "2019\n",
      "and\n",
      "Chen\n",
      "et\n",
      "al\n",
      ".\n",
      "2019\n",
      "learn\n",
      "to\n",
      "disentangle\n",
      "syntactic\n",
      "and\n",
      "semantic\n",
      "information\n",
      ".\n",
      "Silva\n",
      "De\n",
      "Carvalho\n",
      "et\n",
      "al\n",
      ".\n",
      "2023\n",
      "learn\n",
      "to\n",
      "disentangle\n",
      "the\n",
      "semantic\n",
      "roles\n",
      "in\n",
      "natural\n",
      "language\n",
      "definitions\n",
      "from\n",
      "word\n",
      "embeddings\n",
      ".\n",
      "Data\n",
      "For\n",
      "probing\n",
      "transfomers\n",
      "embeddings\n",
      "and\n",
      "behaviour\n",
      ",\n",
      "most\n",
      "approaches\n",
      "use\n",
      "datasets\n",
      "built\n",
      "by\n",
      "selecting\n",
      ",\n",
      "or\n",
      "constructing\n",
      ",\n",
      "sentences\n",
      "that\n",
      "exhibit\n",
      "specific\n",
      "structure\n",
      "and\n",
      "properties\n",
      "definition\n",
      "sentences\n",
      "with\n",
      "annotated\n",
      "roles\n",
      "Silva\n",
      "De\n",
      "Carvalho\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2023\n",
      ",\n",
      "sentences\n",
      "built\n",
      "according\n",
      "to\n",
      "a\n",
      "given\n",
      "template\n",
      "Nikolaev\n",
      "and\n",
      "Pad\n",
      ",\n",
      "2023b\n",
      ",\n",
      "sentences\n",
      "with\n",
      "specific\n",
      "structures\n",
      "for\n",
      "investigating\n",
      "different\n",
      "tasks\n",
      ",\n",
      "in\n",
      "particular\n",
      "SentEval\n",
      "Conneau\n",
      "and\n",
      "Kiela\n",
      ",\n",
      "2018\n",
      "Jawahar\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      ",\n",
      "example\n",
      "sentences\n",
      "from\n",
      "2\n",
      "BLM\n",
      "agreement\n",
      "problem\n",
      "CONTEXT\n",
      "TEMPLATE\n",
      "NP\n",
      "-\n",
      "sg\n",
      "PP1\n",
      "-\n",
      "sg\n",
      "VP\n",
      "-\n",
      "sg\n",
      "NP\n",
      "-\n",
      "pl\n",
      "PP1\n",
      "-\n",
      "sg\n",
      "VP\n",
      "-\n",
      "pl\n",
      "NP\n",
      "-\n",
      "sg\n",
      "PP1\n",
      "-\n",
      "pl\n",
      "VP\n",
      "-\n",
      "sg\n",
      "NP\n",
      "-\n",
      "pl\n",
      "PP1\n",
      "-\n",
      "pl\n",
      "VP\n",
      "-\n",
      "pl\n",
      "NP\n",
      "-\n",
      "sg\n",
      "PP1\n",
      "-\n",
      "sg\n",
      "PP2\n",
      "-\n",
      "sg\n",
      "VP\n",
      "-\n",
      "sg\n",
      "NP\n",
      "-\n",
      "pl\n",
      "PP1\n",
      "-\n",
      "sg\n",
      "PP2\n",
      "-\n",
      "sg\n",
      "VP\n",
      "-\n",
      "pl\n",
      "NP\n",
      "-\n",
      "sg\n",
      "PP1\n",
      "-\n",
      "pl\n",
      "PP2\n",
      "-\n",
      "sg\n",
      "VP\n",
      "-\n",
      "sg\n",
      "ANSWER\n",
      "SET\n",
      "NP\n",
      "-\n",
      "sg\n",
      "PP1\n",
      "-\n",
      "sg\n",
      "et\n",
      "NP2\n",
      "VP\n",
      "-\n",
      "sg\n",
      "Coord\n",
      "NP\n",
      "-\n",
      "pl\n",
      "PP1\n",
      "-\n",
      "pl\n",
      "NP2\n",
      "-\n",
      "sg\n",
      "VP\n",
      "-\n",
      "pl\n",
      "correct\n",
      "NP\n",
      "-\n",
      "sg\n",
      "PP1\n",
      "-\n",
      "sg\n",
      "VP\n",
      "-\n",
      "sg\n",
      "WNA\n",
      "NP\n",
      "-\n",
      "pl\n",
      "PP1\n",
      "-\n",
      "pl\n",
      "NP2\n",
      "-\n",
      "pl\n",
      "VP\n",
      "-\n",
      "sg\n",
      "AEV\n",
      "NP\n",
      "-\n",
      "pl\n",
      "PP1\n",
      "-\n",
      "sg\n",
      "NP2\n",
      "-\n",
      "pl\n",
      "VP\n",
      "-\n",
      "sg\n",
      "AEN1\n",
      "NP\n",
      "-\n",
      "pl\n",
      "PP1\n",
      "-\n",
      "pl\n",
      "NP2\n",
      "-\n",
      "sg\n",
      "VP\n",
      "-\n",
      "sg\n",
      "AEN2\n",
      "NP\n",
      "-\n",
      "pl\n",
      "PP1\n",
      "-\n",
      "sg\n",
      "PP1\n",
      "-\n",
      "sg\n",
      "VP\n",
      "-\n",
      "pl\n",
      "WN1\n",
      "NP\n",
      "-\n",
      "pl\n",
      "PP1\n",
      "-\n",
      "pl\n",
      "PP2\n",
      "-\n",
      "pl\n",
      "VP\n",
      "-\n",
      "pl\n",
      "WN2BLM\n",
      "verb\n",
      "alternation\n",
      "problem\n",
      "CONTEXT\n",
      "TEMPLATE\n",
      "NP\n",
      "-\n",
      "Agent\n",
      "Verb\n",
      "NP\n",
      "-\n",
      "Loc\n",
      "PP\n",
      "-\n",
      "Theme\n",
      "NP\n",
      "-\n",
      "Theme\n",
      "VerbPass\n",
      "PP\n",
      "-\n",
      "Agent\n",
      "NP\n",
      "-\n",
      "Theme\n",
      "VerbPass\n",
      "PP\n",
      "-\n",
      "Loc\n",
      "PP\n",
      "-\n",
      "Agent\n",
      "NP\n",
      "-\n",
      "Theme\n",
      "VerbPass\n",
      "PP\n",
      "-\n",
      "Loc\n",
      "NP\n",
      "-\n",
      "Loc\n",
      "VerbPass\n",
      "PP\n",
      "-\n",
      "Agent\n",
      "NP\n",
      "-\n",
      "Loc\n",
      "VerbPass\n",
      "PP\n",
      "-\n",
      "Theme\n",
      "PP\n",
      "-\n",
      "Agent\n",
      "NP\n",
      "-\n",
      "Loc\n",
      "VerbPass\n",
      "PP\n",
      "-\n",
      "Theme\n",
      "ANSWER\n",
      "SET\n",
      "NP\n",
      "-\n",
      "Agent\n",
      "Verb\n",
      "NP\n",
      "-\n",
      "Theme\n",
      "PP\n",
      "-\n",
      "Loc\n",
      "CORRECT\n",
      "NP\n",
      "-\n",
      "Agent\n",
      "VerbPass\n",
      "NP\n",
      "-\n",
      "Theme\n",
      "PP\n",
      "-\n",
      "Loc\n",
      "AGENT\n",
      "ACT\n",
      "NP\n",
      "-\n",
      "Agent\n",
      "Verb\n",
      "NP\n",
      "-\n",
      "Theme\n",
      "NP\n",
      "-\n",
      "Loc\n",
      "ALT1\n",
      "NP\n",
      "-\n",
      "Agent\n",
      "Verb\n",
      "PP\n",
      "-\n",
      "Theme\n",
      "PP\n",
      "-\n",
      "Loc\n",
      "ALT2\n",
      "NP\n",
      "-\n",
      "Agent\n",
      "Verb\n",
      "NP\n",
      "-\n",
      "Theme\n",
      "PP\n",
      "-\n",
      "Loc\n",
      "NOEMB\n",
      "NP\n",
      "-\n",
      "Agent\n",
      "Verb\n",
      "NP\n",
      "-\n",
      "Theme\n",
      "PP\n",
      "-\n",
      "Loc\n",
      "LEXPREP\n",
      "NP\n",
      "-\n",
      "Theme\n",
      "Verb\n",
      "NP\n",
      "-\n",
      "Agent\n",
      "PP\n",
      "-\n",
      "Loc\n",
      "SSM1\n",
      "NP\n",
      "-\n",
      "Loc\n",
      "Verb\n",
      "NP\n",
      "-\n",
      "Agent\n",
      "PP\n",
      "-\n",
      "Theme\n",
      "SSM2\n",
      "NP\n",
      "-\n",
      "Theme\n",
      "Verb\n",
      "NP\n",
      "-\n",
      "Loc\n",
      "PP\n",
      "-\n",
      "Agent\n",
      "AASSM\n",
      "FrameNet\n",
      "Conia\n",
      "and\n",
      "Navigli\n",
      ",\n",
      "2022\n",
      ",\n",
      "a\n",
      "dataset\n",
      "with\n",
      "multi\n",
      "-\n",
      "level\n",
      "structure\n",
      "inspired\n",
      "by\n",
      "the\n",
      "Raven\n",
      "Progressive\n",
      "Matrices\n",
      "visual\n",
      "intelligence\n",
      "tests\n",
      "An\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2023.Data\n",
      "Our\n",
      "main\n",
      "object\n",
      "of\n",
      "investigation\n",
      "are\n",
      "chunks\n",
      ",\n",
      "sequence\n",
      "of\n",
      "adjacent\n",
      "words\n",
      "that\n",
      "segment\n",
      "a\n",
      "sentence\n",
      "as\n",
      "defined\n",
      "initially\n",
      "in\n",
      "Abney\n",
      ",\n",
      "1992\n",
      ",\n",
      "Collins\n",
      ",\n",
      "1997\n",
      "and\n",
      "then\n",
      "Tjong\n",
      "Kim\n",
      "Sang\n",
      "and\n",
      "Buchholz\n",
      ",\n",
      "2000\n",
      ".\n",
      "To\n",
      "investigate\n",
      "whether\n",
      "chunks\n",
      "and\n",
      "their\n",
      "properties\n",
      "are\n",
      "identifiable\n",
      "in\n",
      "sentence\n",
      "embeddings\n",
      ",\n",
      "we\n",
      "use\n",
      "two\n",
      "types\n",
      "of\n",
      "data\n",
      "i\n",
      "sentences\n",
      "with\n",
      "known\n",
      "chunk\n",
      "pattern\n",
      ",\n",
      "described\n",
      "in\n",
      "Section\n",
      "3.1\n",
      "ii\n",
      "two\n",
      "datasets\n",
      "with\n",
      "multi\n",
      "-\n",
      "level\n",
      "structure\n",
      "built\n",
      "for\n",
      "linguistic\n",
      "intelligence\n",
      "tests\n",
      "for\n",
      "language\n",
      "models\n",
      "Merlo\n",
      ",\n",
      "2023\n",
      ",\n",
      "described\n",
      "in\n",
      "Section\n",
      "3.2\n",
      ".\n",
      "3.1\n",
      "Sentences\n",
      "Sentences\n",
      "are\n",
      "built\n",
      "from\n",
      "a\n",
      "seed\n",
      "file\n",
      "containing\n",
      "noun\n",
      ",\n",
      "verb\n",
      "and\n",
      "prepositional\n",
      "phrases\n",
      ",\n",
      "including\n",
      "singularplural\n",
      "variations\n",
      ".\n",
      "From\n",
      "these\n",
      "chunks\n",
      ",\n",
      "we\n",
      "built\n",
      "sentences\n",
      "with\n",
      "all\n",
      "grammatically\n",
      "correct\n",
      "combinations\n",
      "of\n",
      "np\n",
      "pp\n",
      "1pp2\n",
      "vp4\n",
      ".\n",
      "For\n",
      "each\n",
      "chunk\n",
      "pattern\n",
      "pof\n",
      "the\n",
      "14\n",
      "possibilities\n",
      "for\n",
      "instance\n",
      ",\n",
      "p\n",
      "\"\n",
      "np\n",
      "-\n",
      "s\n",
      "pp1\n",
      "-\n",
      "s\n",
      "vp\n",
      "-\n",
      "s\n",
      "\"\n",
      ",\n",
      "all\n",
      "corresponding\n",
      "sentences\n",
      "are\n",
      "collected\n",
      "into\n",
      "a\n",
      "set\n",
      "Sp\n",
      ".\n",
      "We\n",
      "generate\n",
      "an\n",
      "instance\n",
      "for\n",
      "each\n",
      "sentence\n",
      "sfrom\n",
      "the\n",
      "sets\n",
      "Spas\n",
      "a\n",
      "triple\n",
      "in\n",
      ",\n",
      "out\n",
      ",\n",
      "out\n",
      ",\n",
      "where\n",
      "in\n",
      "sis\n",
      "the\n",
      "input\n",
      ",\n",
      "outis\n",
      "the\n",
      "correct\n",
      "output\n",
      ",\n",
      "which\n",
      "is\n",
      "a\n",
      "sentence\n",
      "different\n",
      "from\n",
      "sbut\n",
      "having\n",
      "the\n",
      "same\n",
      "chunk\n",
      "pattern\n",
      ".\n",
      "outareNnegs\n",
      "incorrect\n",
      "outputs\n",
      ",\n",
      "4We\n",
      "use\n",
      "BNF\n",
      "notation\n",
      "pp\n",
      "1and\n",
      "pp\n",
      "2may\n",
      "be\n",
      "included\n",
      "or\n",
      "not\n",
      ",\n",
      "pp\n",
      "2may\n",
      "be\n",
      "included\n",
      "only\n",
      "if\n",
      "pp1\n",
      "is\n",
      "includedrandomly\n",
      "chosen\n",
      "from\n",
      "the\n",
      "sentences\n",
      "that\n",
      "have\n",
      "a\n",
      "chunk\n",
      "pattern\n",
      "different\n",
      "from\n",
      "s.\n",
      "The\n",
      "algorithm\n",
      "for\n",
      "building\n",
      "the\n",
      "data\n",
      "and\n",
      "a\n",
      "sample\n",
      "line\n",
      "and\n",
      "generated\n",
      "sentences\n",
      "are\n",
      "shown\n",
      "in\n",
      "appendix\n",
      "A.1\n",
      ".\n",
      "From\n",
      "the\n",
      "generated\n",
      "instances\n",
      ",\n",
      "we\n",
      "sample\n",
      "uniformly\n",
      ",\n",
      "based\n",
      "on\n",
      "the\n",
      "pattern\n",
      "of\n",
      "the\n",
      "input\n",
      "sentence\n",
      ",\n",
      "approximately\n",
      "4000\n",
      "instances\n",
      ",\n",
      "randomly\n",
      "split\n",
      "8020\n",
      "into\n",
      "traintest\n",
      ".\n",
      "The\n",
      "train\n",
      "part\n",
      "is\n",
      "further\n",
      "split\n",
      "8020\n",
      "into\n",
      "traindev\n",
      ",\n",
      "resulting\n",
      "in\n",
      "a\n",
      "2576630798\n",
      "split\n",
      "for\n",
      "traindevtest\n",
      ".\n",
      "We\n",
      "use\n",
      "a\n",
      "French\n",
      "and\n",
      "an\n",
      "English\n",
      "seed\n",
      "file\n",
      "and\n",
      "generate\n",
      "French\n",
      "and\n",
      "English\n",
      "variations\n",
      "of\n",
      "the\n",
      "dataset\n",
      ",\n",
      "with\n",
      "the\n",
      "same\n",
      "statistics\n",
      ".\n",
      "3.2\n",
      "Blackbird\n",
      "Language\n",
      "Matrices\n",
      "Blackbird\n",
      "Language\n",
      "Matrices\n",
      "BLMs\n",
      "Merlo\n",
      ",\n",
      "2023\n",
      "are\n",
      "language\n",
      "versions\n",
      "of\n",
      "the\n",
      "visual\n",
      "Raven\n",
      "Progressive\n",
      "Matrices\n",
      ".\n",
      "They\n",
      "are\n",
      "multiple\n",
      "-\n",
      "choice\n",
      "problems\n",
      ",\n",
      "where\n",
      "the\n",
      "input\n",
      "is\n",
      "a\n",
      "sequence\n",
      "of\n",
      "sentences\n",
      "built\n",
      "using\n",
      "specific\n",
      "rules\n",
      ",\n",
      "and\n",
      "the\n",
      "answer\n",
      "set\n",
      "consists\n",
      "of\n",
      "a\n",
      "correct\n",
      "answer\n",
      "that\n",
      "continues\n",
      "the\n",
      "input\n",
      "sequence\n",
      ",\n",
      "and\n",
      "several\n",
      "incorrect\n",
      "options\n",
      "that\n",
      "are\n",
      "built\n",
      "by\n",
      "corrupting\n",
      "some\n",
      "of\n",
      "the\n",
      "underlying\n",
      "generating\n",
      "rules\n",
      "of\n",
      "the\n",
      "sentences\n",
      "in\n",
      "the\n",
      "input\n",
      "sequence\n",
      ".\n",
      "In\n",
      "a\n",
      "BLM\n",
      "matrix\n",
      ",\n",
      "all\n",
      "sentences\n",
      "share\n",
      "a\n",
      "targeted\n",
      "linguistic\n",
      "phenomenon\n",
      ",\n",
      "but\n",
      "differ\n",
      "in\n",
      "other\n",
      "aspects\n",
      "relevant\n",
      "for\n",
      "the\n",
      "phenomenon\n",
      "in\n",
      "question\n",
      ".\n",
      "Thus\n",
      ",\n",
      "BLMs\n",
      ",\n",
      "like\n",
      "their\n",
      "visual\n",
      "counterpart\n",
      "RPMs\n",
      ",\n",
      "require\n",
      "identifying\n",
      "the\n",
      "entities\n",
      "the\n",
      "chunks\n",
      ",\n",
      "their\n",
      "relevant\n",
      "attributes\n",
      "their\n",
      "morphological\n",
      "or\n",
      "semantic\n",
      "properties\n",
      "and\n",
      "their\n",
      "connecting\n",
      "operators\n",
      ",\n",
      "to\n",
      "find\n",
      "the\n",
      "underlying\n",
      "rules\n",
      "that\n",
      "guide\n",
      "to\n",
      "the\n",
      "correct\n",
      "answer\n",
      ".\n",
      "We\n",
      "use\n",
      "two\n",
      "BLM\n",
      "datasets\n",
      ",\n",
      "which\n",
      "encode\n",
      "two\n",
      "different\n",
      "linguistic\n",
      "phenomena\n",
      ",\n",
      "each\n",
      "in\n",
      "a\n",
      "different\n",
      "language\n",
      "i\n",
      "BLM\n",
      "-\n",
      "AgrF\n",
      "subject\n",
      "verb\n",
      "agreement\n",
      "3\n",
      "Subj.-verb\n",
      "agr\n",
      ".\n",
      "Verb\n",
      "alternations\n",
      "ALT\n",
      "-\n",
      "ATL\n",
      "ATL\n",
      "-\n",
      "ALT\n",
      "Type\n",
      "I\n",
      "2000252\n",
      "2000375\n",
      "2000375\n",
      "Type\n",
      "II\n",
      "20004866\n",
      "20001500\n",
      "20001500\n",
      "Type\n",
      "III\n",
      "20004869\n",
      "20001500\n",
      "20001500\n",
      "in\n",
      "French\n",
      "An\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2023\n",
      ",\n",
      "and\n",
      "ii\n",
      "BLM\n",
      "-\n",
      "slE\n",
      "verb\n",
      "alternations\n",
      "in\n",
      "English\n",
      "Samo\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2023\n",
      ".\n",
      "The\n",
      "structure\n",
      "of\n",
      "these\n",
      "datasets\n",
      "in\n",
      "terms\n",
      "of\n",
      "the\n",
      "sentence\n",
      "chunks\n",
      "and\n",
      "sequence\n",
      "structure\n",
      "is\n",
      "shown\n",
      "in\n",
      "2\n",
      ",\n",
      "A.3\n",
      ".\n",
      "BLM\n",
      "datasets\n",
      "also\n",
      "have\n",
      "a\n",
      "lexical\n",
      "variation\n",
      "dimension\n",
      ".\n",
      "There\n",
      "are\n",
      "three\n",
      "variants\n",
      "type\n",
      "I\n",
      "minimal\n",
      "lexical\n",
      "variation\n",
      "for\n",
      "sentences\n",
      "within\n",
      "an\n",
      "instance\n",
      ",\n",
      "type\n",
      "II\n",
      "one\n",
      "word\n",
      "difference\n",
      "across\n",
      "the\n",
      "sentences\n",
      "within\n",
      "an\n",
      "instance\n",
      ",\n",
      "type\n",
      "III\n",
      "maximal\n",
      "lexical\n",
      "variation\n",
      "within\n",
      "an\n",
      "instance\n",
      ".\n",
      "This\n",
      "allows\n",
      "for\n",
      "investigations\n",
      "in\n",
      "the\n",
      "impact\n",
      "of\n",
      "lexical\n",
      "variation\n",
      "on\n",
      "learning\n",
      "the\n",
      "relevant\n",
      "structures\n",
      "to\n",
      "solve\n",
      "the\n",
      "problems\n",
      ".\n",
      "We\n",
      "use\n",
      "the\n",
      "BLM\n",
      "-\n",
      "slE\n",
      "dataset\n",
      "as\n",
      "is\n",
      ".\n",
      "We\n",
      "built\n",
      "a\n",
      "variation\n",
      "of\n",
      "the\n",
      "BLM\n",
      "-\n",
      "AgrF\n",
      "An\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2023\n",
      "that\n",
      "separates\n",
      "clearly\n",
      "sequence\n",
      "-\n",
      "based\n",
      "errors\n",
      "WN1\n",
      "and\n",
      "WN2\n",
      "in\n",
      "the\n",
      "agreement\n",
      "scheme\n",
      "presented\n",
      "in\n",
      "We\n",
      "include\n",
      "erroneous\n",
      "answers\n",
      "that\n",
      "have\n",
      "correct\n",
      "agreement\n",
      ",\n",
      "but\n",
      "do\n",
      "not\n",
      "respect\n",
      "the\n",
      "pattern\n",
      "of\n",
      "the\n",
      "sequence\n",
      ",\n",
      "to\n",
      "be\n",
      "able\n",
      "to\n",
      "contrast\n",
      "linguistic\n",
      "errors\n",
      "from\n",
      "errors\n",
      "in\n",
      "identifying\n",
      "sentence\n",
      "parts\n",
      ".\n",
      "Datasets\n",
      "statistics\n",
      "After\n",
      "splitting\n",
      "each\n",
      "subset\n",
      "9010\n",
      "into\n",
      "traintest\n",
      "subsets\n",
      ",\n",
      "we\n",
      "randomly\n",
      "sample\n",
      "2000\n",
      "instances\n",
      "as\n",
      "train\n",
      "data\n",
      ".\n",
      "20\n",
      "of\n",
      "the\n",
      "train\n",
      "data\n",
      "is\n",
      "used\n",
      "for\n",
      "development\n",
      ".\n",
      "Types\n",
      "I\n",
      ",\n",
      "II\n",
      ",\n",
      "III\n",
      "correspond\n",
      "to\n",
      "different\n",
      "amounts\n",
      "of\n",
      "lexical\n",
      "variation\n",
      "within\n",
      "a\n",
      "problem\n",
      "instance\n",
      ".\n",
      "Experiments\n",
      "We\n",
      "aim\n",
      "to\n",
      "determine\n",
      "whether\n",
      "specific\n",
      "kinds\n",
      "of\n",
      "sentence\n",
      "parts\n",
      "chunks\n",
      "are\n",
      "identifiable\n",
      "in\n",
      "transformer\n",
      "-\n",
      "based\n",
      "sentence\n",
      "embeddings\n",
      ".\n",
      "We\n",
      "approach\n",
      "this\n",
      "problem\n",
      "from\n",
      "two\n",
      "angles\n",
      ".\n",
      "First\n",
      ",\n",
      "using\n",
      "sentences\n",
      "and\n",
      "a\n",
      "VAE\n",
      "-\n",
      "based\n",
      "system\n",
      ",\n",
      "we\n",
      "test\n",
      "whether\n",
      "we\n",
      "can\n",
      "compress\n",
      "sentences\n",
      "into\n",
      "a\n",
      "smaller\n",
      "representation\n",
      "on\n",
      "the\n",
      "latent\n",
      "layer\n",
      "that\n",
      "captures\n",
      "information\n",
      "about\n",
      "the\n",
      "chunk\n",
      "structure\n",
      "of\n",
      "the\n",
      "sentence\n",
      "Section\n",
      "4.1\n",
      "below\n",
      ".\n",
      "Second\n",
      ",\n",
      "to\n",
      "see\n",
      "if\n",
      "the\n",
      "chunks\n",
      "thus\n",
      "identified\n",
      "are\n",
      "being\n",
      "used\n",
      "in\n",
      "a\n",
      "separate\n",
      "task\n",
      ",\n",
      "we\n",
      "combine\n",
      "the\n",
      "compression\n",
      "of\n",
      "the\n",
      "sentence\n",
      "representation\n",
      "with\n",
      "the\n",
      "BLM\n",
      "problems\n",
      ",\n",
      "where\n",
      "a\n",
      "crucial\n",
      "part\n",
      "of\n",
      "the\n",
      "solution\n",
      "lies\n",
      "in\n",
      "identifying\n",
      "the\n",
      "structures\n",
      "ofsentences\n",
      "and\n",
      "their\n",
      "sequence\n",
      "in\n",
      "the\n",
      "input\n",
      "Section\n",
      "4.2\n",
      "below\n",
      ".\n",
      "As\n",
      "sentence\n",
      "representations\n",
      ",\n",
      "we\n",
      "use\n",
      "the\n",
      "embeddings\n",
      "of\n",
      "the\n",
      "s\n",
      "character\n",
      "read\n",
      "from\n",
      "the\n",
      "last\n",
      "layer\n",
      "of\n",
      "the\n",
      "Electra\n",
      "Clark\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2020\n",
      "pretrained\n",
      "model5\n",
      ".\n",
      "4.1\n",
      "Parts\n",
      "in\n",
      "sentences\n",
      "We\n",
      "test\n",
      "whether\n",
      "sentence\n",
      "embeddings\n",
      "contain\n",
      "information\n",
      "about\n",
      "the\n",
      "chunk\n",
      "structure\n",
      "of\n",
      "the\n",
      "corresponding\n",
      "sentences\n",
      "by\n",
      "compressing\n",
      "them\n",
      "into\n",
      "a\n",
      "lower\n",
      "dimensional\n",
      "representation\n",
      "in\n",
      "a\n",
      "VAE\n",
      "-\n",
      "like\n",
      "system\n",
      ".\n",
      "4.1.1\n",
      "Experimental\n",
      "set\n",
      "-\n",
      "up\n",
      "The\n",
      "architecture\n",
      "of\n",
      "the\n",
      "sentence\n",
      "-\n",
      "level\n",
      "VAE\n",
      "is\n",
      "similar\n",
      "to\n",
      "a\n",
      "previously\n",
      "proposed\n",
      "system\n",
      "Nastase\n",
      "and\n",
      "Merlo\n",
      ",\n",
      "2023\n",
      ".\n",
      "The\n",
      "encoder\n",
      "consists\n",
      "of\n",
      "a\n",
      "CNN\n",
      "layer\n",
      "with\n",
      "a\n",
      "15x15\n",
      "kernel\n",
      ",\n",
      "which\n",
      "is\n",
      "applied\n",
      "to\n",
      "a\n",
      "32x24shaped\n",
      "sentence\n",
      "embedding,6followed\n",
      "by\n",
      "a\n",
      "linear\n",
      "layer\n",
      "that\n",
      "compresses\n",
      "the\n",
      "output\n",
      "of\n",
      "the\n",
      "CNN\n",
      "into\n",
      "a\n",
      "latent\n",
      "layer\n",
      "of\n",
      "size\n",
      "5\n",
      ".\n",
      "The\n",
      "decoder\n",
      "is\n",
      "a\n",
      "mirrorimage\n",
      "of\n",
      "the\n",
      "encoder\n",
      ",\n",
      "and\n",
      "unpacks\n",
      "a\n",
      "sampled\n",
      "latent\n",
      "vector\n",
      "into\n",
      "a\n",
      "32x24\n",
      "sentence\n",
      "representation\n",
      ".\n",
      "An\n",
      "instance\n",
      "consists\n",
      "of\n",
      "a\n",
      "triple\n",
      "in\n",
      ",\n",
      "out\n",
      ",\n",
      "out\n",
      ",\n",
      "where\n",
      "inis\n",
      "an\n",
      "input\n",
      "sentence\n",
      "with\n",
      "embedding\n",
      "ei\n",
      "and\n",
      "chunk\n",
      "structure\n",
      "p\n",
      ",\n",
      "outis\n",
      "a\n",
      "sentence\n",
      "with\n",
      "embedding\n",
      "ejwith\n",
      "same\n",
      "chunk\n",
      "structure\n",
      "p\n",
      ",\n",
      "and\n",
      "outis\n",
      "a\n",
      "set\n",
      "of\n",
      "Nnegs\n",
      "sentences\n",
      "with\n",
      "embeddings\n",
      "ek\n",
      ",\n",
      "each\n",
      "of\n",
      "which\n",
      "has\n",
      "a\n",
      "chunk\n",
      "pattern\n",
      "different\n",
      "frompand\n",
      "different\n",
      "from\n",
      "each\n",
      "other\n",
      ".\n",
      "The\n",
      "input\n",
      "eiis\n",
      "encoded\n",
      "into\n",
      "a\n",
      "latent\n",
      "representation\n",
      "zi\n",
      ",\n",
      "from\n",
      "which\n",
      "we\n",
      "sample\n",
      "a\n",
      "vector\n",
      "zi\n",
      ",\n",
      "which\n",
      "is\n",
      "decoded\n",
      "into\n",
      "the\n",
      "output\n",
      "ei\n",
      ".\n",
      "We\n",
      "enforce\n",
      "that\n",
      "the\n",
      "latent\n",
      "encodes\n",
      "the\n",
      "structure\n",
      "of\n",
      "the\n",
      "input\n",
      "sentence\n",
      "by\n",
      "using\n",
      "a\n",
      "maxmargin\n",
      "loss\n",
      "function\n",
      ".\n",
      "This\n",
      "loss\n",
      "function\n",
      "assigns\n",
      "a\n",
      "higher\n",
      "score\n",
      "to\n",
      "ejthan\n",
      "to\n",
      "ek\n",
      ",\n",
      "relative\n",
      "to\n",
      "ei\n",
      ".\n",
      "Recall\n",
      "thatejhas\n",
      "the\n",
      "same\n",
      "chunk\n",
      "structure\n",
      "as\n",
      "the\n",
      "input\n",
      "ei\n",
      ".\n",
      "lossei\n",
      "maxmargin\n",
      "ei\n",
      ",\n",
      "ej\n",
      ",\n",
      "ek\n",
      "KLziN0,1\n",
      "maxmargin\n",
      "ei\n",
      ",\n",
      "ej\n",
      ",\n",
      "ek\n",
      "max0,1score\n",
      "ei\n",
      ",\n",
      "ej\n",
      "PNnegs\n",
      "k1score\n",
      "ei\n",
      ",\n",
      "ek\n",
      "Nnegs\n",
      "Thescore\n",
      "between\n",
      "two\n",
      "embeddings\n",
      "is\n",
      "the\n",
      "cosine\n",
      "similarity\n",
      ".\n",
      "At\n",
      "prediction\n",
      "time\n",
      ",\n",
      "the\n",
      "sentence\n",
      "from\n",
      "theout\n",
      "outoptions\n",
      "that\n",
      "has\n",
      "the\n",
      "highest\n",
      "5Electra\n",
      "pretrained\n",
      "model\n",
      "googleelectra\n",
      "-\n",
      "basediscriminator\n",
      "6Nastase\n",
      "and\n",
      "Merlo\n",
      "2023\n",
      "show\n",
      "that\n",
      "task\n",
      "-\n",
      "relevant\n",
      "information\n",
      "is\n",
      "more\n",
      "easily\n",
      "accessible\n",
      "in\n",
      "transformer\n",
      "-\n",
      "based\n",
      "sentence\n",
      "embeddings\n",
      "reshaped\n",
      "as\n",
      "two\n",
      "-\n",
      "dimensional\n",
      "arrays\n",
      ",\n",
      "which\n",
      "indicates\n",
      "that\n",
      "patterns\n",
      "are\n",
      "encoded\n",
      "periodically\n",
      ",\n",
      "and\n",
      "are\n",
      "best\n",
      "detected\n",
      "with\n",
      "a\n",
      "15x15\n",
      "kernel\n",
      ".\n",
      "4\n",
      "score\n",
      "relative\n",
      "to\n",
      "the\n",
      "input\n",
      "sentence\n",
      "is\n",
      "taken\n",
      "as\n",
      "the\n",
      "correct\n",
      "answer\n",
      ".\n",
      "4.1.2\n",
      "Analysis\n",
      "To\n",
      "assess\n",
      "whether\n",
      "the\n",
      "correct\n",
      "patterns\n",
      "of\n",
      "chunks\n",
      "are\n",
      "detected\n",
      "in\n",
      "sentences\n",
      ",\n",
      "we\n",
      "analyze\n",
      "the\n",
      "results\n",
      "for\n",
      "the\n",
      "experiments\n",
      "described\n",
      "in\n",
      "the\n",
      "previous\n",
      "section\n",
      "in\n",
      "two\n",
      "ways\n",
      "i\n",
      "analyze\n",
      "the\n",
      "output\n",
      "of\n",
      "the\n",
      "system\n",
      ",\n",
      "in\n",
      "terms\n",
      "of\n",
      "average\n",
      "F1\n",
      "score\n",
      "over\n",
      "three\n",
      "runs\n",
      "and\n",
      "confusion\n",
      "matrices\n",
      "ii\n",
      "analyze\n",
      "the\n",
      "latent\n",
      "layer\n",
      ",\n",
      "to\n",
      "determine\n",
      "whether\n",
      "chunk\n",
      "patterns\n",
      "are\n",
      "encoded\n",
      "in\n",
      "the\n",
      "latent\n",
      "vectors\n",
      "for\n",
      "instance\n",
      ",\n",
      "latent\n",
      "vectors\n",
      "cluster\n",
      "according\n",
      "to\n",
      "the\n",
      "pattern\n",
      "of\n",
      "their\n",
      "corresponding\n",
      "sentences\n",
      ".\n",
      "The\n",
      "results\n",
      "for\n",
      "English\n",
      "are\n",
      "similar\n",
      ".\n",
      "If\n",
      "we\n",
      "consider\n",
      "the\n",
      "multiple\n",
      "choice\n",
      "task\n",
      "as\n",
      "a\n",
      "binary\n",
      "task\n",
      "Has\n",
      "the\n",
      "system\n",
      "built\n",
      "a\n",
      "sentence\n",
      "representation\n",
      "that\n",
      "is\n",
      "closest\n",
      "to\n",
      "the\n",
      "correct\n",
      "answer\n",
      "?\n",
      ",\n",
      "the\n",
      "system\n",
      "achieves\n",
      "an\n",
      "average\n",
      "positive\n",
      "class\n",
      "F1\n",
      "score\n",
      "and\n",
      "standard\n",
      "deviation\n",
      "over\n",
      "three\n",
      "runs\n",
      "of\n",
      "0.9992\n",
      "0.01\n",
      "for\n",
      "the\n",
      "French\n",
      "dataset\n",
      ",\n",
      "and\n",
      "0.997\n",
      "0.0035\n",
      "for\n",
      "the\n",
      "English\n",
      "dataset\n",
      ".\n",
      "For\n",
      "added\n",
      "insight\n",
      ",\n",
      "for\n",
      "one\n",
      "trained\n",
      "model\n",
      "for\n",
      "each\n",
      "of\n",
      "the\n",
      "French\n",
      "and\n",
      "English\n",
      "data\n",
      ",\n",
      "we\n",
      "compute\n",
      "a\n",
      "confusion\n",
      "matrix\n",
      ",\n",
      "based\n",
      "on\n",
      "thepattern\n",
      "information\n",
      "for\n",
      "out\n",
      ",\n",
      "out\n",
      ".\n",
      "The\n",
      "results\n",
      "for\n",
      "French\n",
      "are\n",
      "presented\n",
      "in\n",
      "To\n",
      "check\n",
      "whether\n",
      "chunk\n",
      "information\n",
      "is\n",
      "present\n",
      "in\n",
      "the\n",
      "latent\n",
      "layer\n",
      ",\n",
      "we\n",
      "plot\n",
      "the\n",
      "projection\n",
      "in\n",
      "two\n",
      "dimensions\n",
      "of\n",
      "the\n",
      "latent\n",
      "vectors\n",
      ".\n",
      "The\n",
      "plot\n",
      "shows\n",
      "a\n",
      "very\n",
      "crisp\n",
      "clustering\n",
      "of\n",
      "latents\n",
      "that\n",
      "correspond\n",
      "to\n",
      "input\n",
      "sentences\n",
      "with\n",
      "the\n",
      "same\n",
      "chunk\n",
      "pattern\n",
      ",\n",
      "despite\n",
      "the\n",
      "fact\n",
      "that\n",
      "some\n",
      "patterns\n",
      "differ\n",
      "by\n",
      "only\n",
      "one\n",
      "attribute\n",
      "the\n",
      "grammatical\n",
      "number\n",
      "of\n",
      "one\n",
      "chunk\n",
      ".\n",
      "To\n",
      "understand\n",
      "how\n",
      "chunk\n",
      "information\n",
      "is\n",
      "encoded\n",
      "on\n",
      "the\n",
      "latent\n",
      "layer\n",
      "we\n",
      "perform\n",
      "latent\n",
      "traversals\n",
      "for\n",
      "each\n",
      "instance\n",
      "in\n",
      "the\n",
      "test\n",
      "data\n",
      ",\n",
      "we\n",
      "modify\n",
      "the\n",
      "value\n",
      "of\n",
      "each\n",
      "unit\n",
      "in\n",
      "the\n",
      "latent\n",
      "layer\n",
      "with\n",
      "ten\n",
      "values\n",
      "in\n",
      "the\n",
      "min\n",
      "-\n",
      "max\n",
      "range\n",
      "of\n",
      "that\n",
      "unit\n",
      ",\n",
      "based\n",
      "on\n",
      "the\n",
      "training\n",
      "data\n",
      ".\n",
      "A\n",
      "sample\n",
      "of\n",
      "confusion\n",
      "matrices\n",
      "with\n",
      "interventions\n",
      "on\n",
      "the\n",
      "latent\n",
      "layer\n",
      "is\n",
      "shown\n",
      "in\n",
      "The\n",
      "confusion\n",
      "matrices\n",
      "presented\n",
      "as\n",
      "heatmaps\n",
      "inshow\n",
      "that\n",
      "specific\n",
      "changes\n",
      "to\n",
      "the\n",
      "latent\n",
      "vectors\n",
      "decrease\n",
      "the\n",
      "differentiation\n",
      "among\n",
      "patterns\n",
      ",\n",
      "as\n",
      "expected\n",
      "if\n",
      "chunk\n",
      "pattern\n",
      "information\n",
      "were\n",
      "encoded\n",
      "in\n",
      "the\n",
      "latent\n",
      "vectors\n",
      ".\n",
      "Changes\n",
      "to\n",
      "latentcause\n",
      "patterns\n",
      "that\n",
      "differ\n",
      "in\n",
      "the\n",
      "grammatical\n",
      "number\n",
      "of\n",
      "pp2not\n",
      "to\n",
      "be\n",
      "distinguishable\n",
      "left\n",
      "matrix\n",
      ".\n",
      "Changes\n",
      "to\n",
      "latent\n",
      "unitsandlead\n",
      "to\n",
      "the\n",
      "matricesandin\n",
      "the\n",
      "4.1.3\n",
      "Electra\n",
      "vs.\n",
      "BERT\n",
      "and\n",
      "RoBERTa\n",
      ",\n",
      "and\n",
      "the\n",
      "price\n",
      "of\n",
      "fine\n",
      "-\n",
      "tuning\n",
      "We\n",
      "have\n",
      "chosen\n",
      "Electra\n",
      "for\n",
      "the\n",
      "investigations\n",
      "presented\n",
      "here\n",
      ",\n",
      "because\n",
      "with\n",
      "its\n",
      "use\n",
      "of\n",
      "the\n",
      "full\n",
      "context\n",
      "of\n",
      "the\n",
      "sentence\n",
      "to\n",
      "determine\n",
      "whether\n",
      "tokens\n",
      "in\n",
      "the\n",
      "input\n",
      "have\n",
      "been\n",
      "replaced\n",
      "by\n",
      "a\n",
      "generator\n",
      "model\n",
      ",\n",
      "it\n",
      "provides\n",
      "a\n",
      "stronger\n",
      "supervision\n",
      "signal\n",
      "than\n",
      "BERT\n",
      "Devlin\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      "and\n",
      "RoBERTa\n",
      "Liu\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      ".\n",
      "To\n",
      "check\n",
      "whether\n",
      "the\n",
      "sentence\n",
      "embeddings\n",
      "produced\n",
      "by\n",
      "the\n",
      "three\n",
      "systems\n",
      "differ\n",
      "in\n",
      "their\n",
      "encoding\n",
      "of\n",
      "chunks\n",
      ",\n",
      "we\n",
      "ran\n",
      "the\n",
      "same\n",
      "experiment\n",
      "on\n",
      "5\n",
      "the\n",
      "data\n",
      "encoded\n",
      "with\n",
      "BERT7and\n",
      "RoBERTa8\n",
      ".\n",
      "In\n",
      "terms\n",
      "of\n",
      "F1\n",
      "score\n",
      "on\n",
      "the\n",
      "task\n",
      "of\n",
      "reconstructing\n",
      "a\n",
      "sentence\n",
      "with\n",
      "the\n",
      "same\n",
      "chunk\n",
      "structure\n",
      ",\n",
      "BERT\n",
      "has\n",
      "a\n",
      "mean\n",
      "overruns\n",
      "of\n",
      "0.91\n",
      "std0.0346\n",
      ",\n",
      "while\n",
      "RoBERTA\n",
      "has\n",
      "0.8926\n",
      "std0.0166\n",
      ".\n",
      "Sentence\n",
      "embeddings\n",
      "are\n",
      "often\n",
      "fine\n",
      "-\n",
      "tuned\n",
      "for\n",
      "specific\n",
      "tasks\n",
      ".\n",
      "We\n",
      "tested\n",
      "two\n",
      "sentence\n",
      "transformer\n",
      "models\n",
      "LaBSE\n",
      "and\n",
      "MPNet9\n",
      ",\n",
      "and\n",
      "obtained\n",
      "an\n",
      "F1\n",
      "mean\n",
      "of\n",
      "0.43\n",
      "std0.0336\n",
      "and\n",
      "0.669\n",
      "std0.0407\n",
      "respectively\n",
      ".\n",
      "We\n",
      "chose\n",
      "LaBSE\n",
      "and\n",
      "MPNet\n",
      "because\n",
      "they\n",
      "are\n",
      "two\n",
      "differently\n",
      "tuned\n",
      "models\n",
      "LaBSE\n",
      "is\n",
      "trained\n",
      "with\n",
      "bilingual\n",
      "sentence\n",
      "pairs\n",
      "with\n",
      "high\n",
      "results\n",
      "on\n",
      "a\n",
      "cross\n",
      "-\n",
      "language\n",
      "sentence\n",
      "retrieval\n",
      "task\n",
      ",\n",
      "MPNet\n",
      "is\n",
      "optimized\n",
      "for\n",
      "sentence\n",
      "similarity\n",
      "and\n",
      "their\n",
      "representations\n",
      "have\n",
      "the\n",
      "same\n",
      "dimensionality\n",
      "768\n",
      "as\n",
      "the\n",
      "transformer\n",
      "models\n",
      "we\n",
      "used\n",
      ".\n",
      "The\n",
      "low\n",
      "results\n",
      "on\n",
      "detecting\n",
      "chunk\n",
      "structure\n",
      "in\n",
      "sentence\n",
      "embeddings\n",
      "after\n",
      "this\n",
      "tuning\n",
      "indicates\n",
      "that\n",
      "in\n",
      "the\n",
      "quest\n",
      "of\n",
      "optimizing\n",
      "the\n",
      "representation\n",
      "of\n",
      "the\n",
      "meaning\n",
      "of\n",
      "a\n",
      "sentence\n",
      ",\n",
      "structural\n",
      "information\n",
      "is\n",
      "lost\n",
      ".\n",
      "4.2\n",
      "Parts\n",
      "in\n",
      "sentences\n",
      "for\n",
      "BLM\n",
      "tasks\n",
      "The\n",
      "first\n",
      "experiment\n",
      "shows\n",
      "that\n",
      "compressing\n",
      "sentence\n",
      "representations\n",
      "results\n",
      "in\n",
      "latent\n",
      "vectors\n",
      "containing\n",
      "chunk\n",
      "information\n",
      ".\n",
      "To\n",
      "test\n",
      "if\n",
      "these\n",
      "latent\n",
      "representations\n",
      "also\n",
      "contain\n",
      "information\n",
      "about\n",
      "chunk\n",
      "properties\n",
      "relevant\n",
      "to\n",
      "a\n",
      "task\n",
      ",\n",
      "we\n",
      "solve\n",
      "the\n",
      "BLM\n",
      "task\n",
      ".\n",
      "4.2.1\n",
      "Experimental\n",
      "set\n",
      "-\n",
      "up\n",
      "To\n",
      "explore\n",
      "how\n",
      "chunk\n",
      "information\n",
      "in\n",
      "the\n",
      "sentence\n",
      "embeddings\n",
      "is\n",
      "used\n",
      "in\n",
      "a\n",
      "task\n",
      ",\n",
      "we\n",
      "solve\n",
      "the\n",
      "BLM\n",
      "problems\n",
      ".\n",
      "The\n",
      "BLM\n",
      "problems\n",
      "encode\n",
      "a\n",
      "linguistic\n",
      "phenomenon\n",
      "in\n",
      "a\n",
      "sequence\n",
      "of\n",
      "sentences\n",
      "that\n",
      "have\n",
      "regular\n",
      "and\n",
      "relevant\n",
      "structure\n",
      ",\n",
      "which\n",
      "serves\n",
      "to\n",
      "emphasize\n",
      "and\n",
      "reinforce\n",
      "the\n",
      "encoded\n",
      "phenomenon\n",
      ".\n",
      "BLMs\n",
      "are\n",
      "inspired\n",
      "by\n",
      "Raven\n",
      "Progressive\n",
      "Matrices\n",
      ",\n",
      "whose\n",
      "solution\n",
      "has\n",
      "been\n",
      "shown\n",
      "to\n",
      "require\n",
      "solving\n",
      "two\n",
      "main\n",
      "subtasks\n",
      "identifying\n",
      "the\n",
      "objects\n",
      "and\n",
      "object\n",
      "attributes\n",
      "that\n",
      "occur\n",
      "in\n",
      "the\n",
      "visual\n",
      "frames\n",
      ",\n",
      "and\n",
      "decomposing\n",
      "the\n",
      "main\n",
      "problem\n",
      "into\n",
      "subproblems\n",
      ",\n",
      "based\n",
      "on\n",
      "object\n",
      "and\n",
      "attribute\n",
      "identification\n",
      ",\n",
      "in\n",
      "a\n",
      "way\n",
      "that\n",
      "allows\n",
      "detecting\n",
      "the\n",
      "global\n",
      "pattern\n",
      "or\n",
      "underlying\n",
      "rules\n",
      ".\n",
      "It\n",
      "has\n",
      "also\n",
      "been\n",
      "shown\n",
      "that\n",
      "being\n",
      "able\n",
      "to\n",
      "solve\n",
      "RPMs\n",
      "requires\n",
      "being\n",
      "able\n",
      "to\n",
      "handle\n",
      "item\n",
      "novelty\n",
      "Carpenter\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "1990\n",
      ".\n",
      "We\n",
      "model\n",
      "bert\n",
      "-\n",
      "base\n",
      "-\n",
      "multilingual\n",
      "-\n",
      "cased\n",
      "xlm\n",
      "-\n",
      "roberta\n",
      "-\n",
      "base\n",
      "sentence\n",
      "-\n",
      "transformersLaBSE\n",
      ",\n",
      "httpshuggingface\n",
      ".\n",
      "cosentence\n",
      "-\n",
      "transformersall\n",
      "-\n",
      "mpnet\n",
      "-\n",
      "base\n",
      "-\n",
      "v2these\n",
      "ingredients\n",
      "of\n",
      "the\n",
      "solution\n",
      "of\n",
      "a\n",
      "RPMBLM\n",
      "explicitly\n",
      "by\n",
      "using\n",
      "the\n",
      "two\n",
      "-\n",
      "level\n",
      "intertwined\n",
      "architecture\n",
      "illustrated\n",
      "in\n",
      "Item\n",
      "novelty\n",
      "is\n",
      "modeled\n",
      "through\n",
      "the\n",
      "three\n",
      "levels\n",
      "of\n",
      "lexicalisation\n",
      "section\n",
      "3\n",
      ".\n",
      "The\n",
      "sentence\n",
      "level\n",
      "is\n",
      "essentially\n",
      "the\n",
      "system\n",
      "described\n",
      "above\n",
      ".\n",
      "The\n",
      "representation\n",
      "on\n",
      "the\n",
      "latent\n",
      "layer\n",
      "is\n",
      "used\n",
      "to\n",
      "represent\n",
      "each\n",
      "of\n",
      "the\n",
      "sentences\n",
      "in\n",
      "the\n",
      "input\n",
      "sequence\n",
      ",\n",
      "and\n",
      "to\n",
      "solve\n",
      "the\n",
      "problem\n",
      "at\n",
      "the\n",
      "task\n",
      "level\n",
      ".\n",
      "The\n",
      "two\n",
      "layers\n",
      "are\n",
      "trained\n",
      "together\n",
      ".\n",
      "An\n",
      "instance\n",
      "for\n",
      "a\n",
      "BLM\n",
      "problem\n",
      "consists\n",
      "of\n",
      "an\n",
      "ordered\n",
      "sequence\n",
      "Sof\n",
      "sentences\n",
      ",\n",
      "Ssii\n",
      "1,7\n",
      "as\n",
      "input\n",
      ",\n",
      "and\n",
      "an\n",
      "answer\n",
      "set\n",
      "Awith\n",
      "one\n",
      "correct\n",
      "answerac\n",
      ",\n",
      "and\n",
      "several\n",
      "incorrect\n",
      "answers\n",
      "aerr\n",
      ".\n",
      "The\n",
      "sentences\n",
      "in\n",
      "Sare\n",
      "passed\n",
      "as\n",
      "input\n",
      "to\n",
      "the\n",
      "sentencelevel\n",
      "VAE\n",
      ".\n",
      "The\n",
      "sampled\n",
      "latent\n",
      "representations\n",
      "from\n",
      "this\n",
      "VAE\n",
      "are\n",
      "used\n",
      "as\n",
      "the\n",
      "representations\n",
      "of\n",
      "the\n",
      "sentences\n",
      "in\n",
      "S.\n",
      "These\n",
      "representations\n",
      "are\n",
      "passed\n",
      "as\n",
      "input\n",
      "to\n",
      "the\n",
      "BLM\n",
      "-\n",
      "level\n",
      "VAE\n",
      ",\n",
      "in\n",
      "the\n",
      "same\n",
      "order\n",
      "as\n",
      "S.\n",
      "An\n",
      "instance\n",
      "for\n",
      "the\n",
      "sentence\n",
      "-\n",
      "level\n",
      "VAE\n",
      "consists\n",
      "of\n",
      "a\n",
      "triple\n",
      "in\n",
      ",\n",
      "out\n",
      ",\n",
      "out\n",
      ".\n",
      "For\n",
      "our\n",
      "twolevel\n",
      "system\n",
      ",\n",
      "we\n",
      "must\n",
      "construct\n",
      "this\n",
      "triple\n",
      "from\n",
      "the\n",
      "input\n",
      "BLM\n",
      "instance\n",
      "inS\n",
      ",\n",
      "outin\n",
      ",\n",
      "and\n",
      "outskskS\n",
      ",\n",
      "skin\n",
      ".\n",
      "The\n",
      "loss\n",
      "combines\n",
      "the\n",
      "loss\n",
      "signal\n",
      "from\n",
      "the\n",
      "two\n",
      "levels\n",
      "loss\n",
      "maxmargin\n",
      "sentKLsent\n",
      "maxmargin\n",
      "taskKLseq\n",
      "Themaxmargin\n",
      "and\n",
      "the\n",
      "scoring\n",
      "of\n",
      "the\n",
      "reconstructed\n",
      "sentence\n",
      "at\n",
      "the\n",
      "sentence\n",
      "level\n",
      ",\n",
      "and\n",
      "the\n",
      "constructed\n",
      "answer\n",
      "at\n",
      "the\n",
      "task\n",
      "level\n",
      "are\n",
      "computed\n",
      "as\n",
      "described\n",
      "in\n",
      "Section\n",
      "4.1\n",
      ".\n",
      "We\n",
      "run\n",
      "experiments\n",
      "on\n",
      "the\n",
      "BLMs\n",
      "for\n",
      "agreement\n",
      "and\n",
      "for\n",
      "verb\n",
      "alternation\n",
      ".\n",
      "While\n",
      "the\n",
      "information\n",
      "necessary\n",
      "to\n",
      "solve\n",
      "the\n",
      "agreement\n",
      "task\n",
      "is\n",
      "more\n",
      "structural\n",
      ",\n",
      "solving\n",
      "the\n",
      "verb\n",
      "alternation\n",
      "task\n",
      "requires\n",
      "not\n",
      "only\n",
      "structural\n",
      "information\n",
      "concerning\n",
      "chunks\n",
      ",\n",
      "but\n",
      "6\n",
      "TSNE\n",
      "projection\n",
      "of\n",
      "latent\n",
      "representations\n",
      "from\n",
      "the\n",
      "latent\n",
      "layer\n",
      "of\n",
      "the\n",
      "sentence\n",
      "level\n",
      "for\n",
      "the\n",
      "sentences\n",
      "in\n",
      "BLM\n",
      "contexts\n",
      "in\n",
      "the\n",
      "training\n",
      "data\n",
      ",\n",
      "coloured\n",
      "by\n",
      "the\n",
      "chunk\n",
      "pattern\n",
      ".\n",
      "Average\n",
      "F1\n",
      "score\n",
      "overruns\n",
      ",\n",
      "grouped\n",
      "by\n",
      "training\n",
      "data\n",
      "on\n",
      "the\n",
      "x\n",
      "-\n",
      "axis\n",
      ",\n",
      "tested\n",
      "on\n",
      "type\n",
      "I\n",
      ",\n",
      "II\n",
      ",\n",
      "III\n",
      "in\n",
      "different\n",
      "shades\n",
      ".\n",
      "Sequence\n",
      "vs.\n",
      "agreement\n",
      "errors\n",
      "analysis\n",
      ".\n",
      "4.2.2\n",
      "Analysis\n",
      "The\n",
      "results\n",
      "show\n",
      "that\n",
      "this\n",
      "organisation\n",
      "of\n",
      "the\n",
      "system\n",
      "leads\n",
      "to\n",
      "better\n",
      "results\n",
      "compared\n",
      "to\n",
      "the\n",
      "onelevel\n",
      "process\n",
      "for\n",
      "these\n",
      "structure\n",
      "-\n",
      "based\n",
      "linguistic\n",
      "problems\n",
      ",\n",
      "thereby\n",
      "providing\n",
      "additional\n",
      "support\n",
      "to\n",
      "our\n",
      "hypothesis\n",
      "that\n",
      "chunks\n",
      "and\n",
      "their\n",
      "attributes\n",
      "are\n",
      "detectable\n",
      "in\n",
      "sentence\n",
      "embeddings\n",
      ".\n",
      "We\n",
      "provide\n",
      "results\n",
      "in\n",
      "terms\n",
      "of\n",
      "F1\n",
      "scores\n",
      "on\n",
      "the\n",
      "task\n",
      ",\n",
      "and\n",
      "analysis\n",
      "of\n",
      "the\n",
      "representations\n",
      "on\n",
      "the\n",
      "latent\n",
      "layer\n",
      "of\n",
      "the\n",
      "sentence\n",
      "level\n",
      "of\n",
      "the\n",
      "system\n",
      ".\n",
      "The\n",
      "results\n",
      "on\n",
      "the\n",
      "task\n",
      "leftpanel\n",
      "provide\n",
      "several\n",
      "insights\n",
      ".\n",
      "First\n",
      ",\n",
      "from\n",
      "the\n",
      "latent\n",
      "representation\n",
      "analysis\n",
      ",\n",
      "we\n",
      "note\n",
      "that\n",
      "while\n",
      "the\n",
      "sentence\n",
      "representations\n",
      "on\n",
      "the\n",
      "latent\n",
      "layer\n",
      "are\n",
      "not\n",
      "as\n",
      "crisply\n",
      "separated\n",
      "by\n",
      "their\n",
      "chunk\n",
      "pattern\n",
      "as\n",
      "for\n",
      "the\n",
      "experiment\n",
      "in\n",
      "Section\n",
      "4.1\n",
      ",\n",
      "there\n",
      "is\n",
      "a\n",
      "clear\n",
      "separation\n",
      "in\n",
      "terms\n",
      "of\n",
      "the\n",
      "grammatical\n",
      "number\n",
      "of\n",
      "the\n",
      "subject\n",
      "and\n",
      "the\n",
      "verb\n",
      ".\n",
      "This\n",
      "is\n",
      "not\n",
      "surprising\n",
      "as\n",
      "the\n",
      "focus\n",
      "of\n",
      "the\n",
      "task\n",
      "is\n",
      "subject\n",
      "-\n",
      "verb\n",
      "agreement\n",
      ".\n",
      "However\n",
      ",\n",
      "as\n",
      "the\n",
      "further\n",
      "results\n",
      "in\n",
      "term\n",
      "of\n",
      "F1\n",
      "and\n",
      "error\n",
      "analysis\n",
      "on\n",
      "the\n",
      "task\n",
      "show\n",
      ",\n",
      "there\n",
      "is\n",
      "enough\n",
      "information\n",
      "in\n",
      "these\n",
      "compressed\n",
      "latent\n",
      "representation\n",
      "to\n",
      "capture\n",
      "the\n",
      "structural\n",
      "regularities\n",
      "imposed\n",
      "by\n",
      "the\n",
      "patterns\n",
      "of\n",
      "chunks\n",
      "in\n",
      "the\n",
      "input\n",
      "sequence\n",
      ".\n",
      "Second\n",
      ",\n",
      "from\n",
      "the\n",
      "results\n",
      "in\n",
      "terms\n",
      "of\n",
      "F1\n",
      ",\n",
      "we\n",
      "note\n",
      "that\n",
      "the\n",
      "two\n",
      "-\n",
      "level\n",
      "process\n",
      "generalizes\n",
      "better\n",
      "from\n",
      "simpler\n",
      "data\n",
      "learning\n",
      "on\n",
      "type\n",
      "I\n",
      "and\n",
      "type\n",
      "II\n",
      "leads\n",
      "to\n",
      "better\n",
      "results\n",
      "on\n",
      "all\n",
      "test\n",
      "data\n",
      ",\n",
      "with\n",
      "the\n",
      "highest\n",
      "improvement\n",
      "when\n",
      "tested\n",
      "on\n",
      "type\n",
      "III\n",
      "data\n",
      ",\n",
      "which\n",
      "has\n",
      "the\n",
      "highest\n",
      "lexical\n",
      "variation\n",
      ".\n",
      "Furthermore\n",
      ",\n",
      "the\n",
      "two\n",
      "-\n",
      "level\n",
      "models\n",
      "learned\n",
      "when\n",
      "training\n",
      "on\n",
      "the\n",
      "lexically\n",
      "simpler\n",
      "data\n",
      "perform\n",
      "better\n",
      "when\n",
      "tested\n",
      "on\n",
      "the\n",
      "type\n",
      "III\n",
      "data\n",
      "than\n",
      "the\n",
      "models\n",
      "learned\n",
      "on\n",
      "type\n",
      "III\n",
      "data\n",
      "itself\n",
      ".\n",
      "This\n",
      "result\n",
      "not\n",
      "only\n",
      "indicates\n",
      "that\n",
      "structure\n",
      "information\n",
      "is\n",
      "more\n",
      "easily\n",
      "detectable\n",
      "when\n",
      "lexical\n",
      "variation\n",
      "is\n",
      "less\n",
      "of\n",
      "a\n",
      "factor\n",
      ",\n",
      "but\n",
      "more\n",
      "importantly\n",
      ",\n",
      "that\n",
      "chunk\n",
      "information\n",
      "is\n",
      "separable\n",
      "from\n",
      "other\n",
      "types\n",
      "of\n",
      "information\n",
      "in\n",
      "the\n",
      "sentence\n",
      "embedding\n",
      ",\n",
      "as\n",
      "the\n",
      "patterns\n",
      "detecting\n",
      "it\n",
      "can\n",
      "be\n",
      "applied\n",
      "successfully\n",
      "for\n",
      "data\n",
      "with\n",
      "additional\n",
      "lexical\n",
      "variation.10\n",
      "Further\n",
      "confirmation\n",
      "of\n",
      "the\n",
      "fact\n",
      "that\n",
      "the\n",
      "sentence\n",
      "level\n",
      "learns\n",
      "to\n",
      "compress\n",
      "sentences\n",
      "into\n",
      "a\n",
      "latent\n",
      "that\n",
      "captures\n",
      "structural\n",
      "information\n",
      "comes\n",
      "from\n",
      "the\n",
      "error\n",
      "analysis\n",
      ",\n",
      "shown\n",
      "in\n",
      "the\n",
      "bottom\n",
      "panel\n",
      "of\n",
      "Lower\n",
      "rate\n",
      "of\n",
      "sequence\n",
      "errors\n",
      ",\n",
      "which\n",
      "are\n",
      "correct\n",
      "from\n",
      "the\n",
      "point\n",
      "of\n",
      "view\n",
      "of\n",
      "the\n",
      "targeted\n",
      "phenomenon\n",
      "as\n",
      "described\n",
      "in\n",
      "section\n",
      "3.2\n",
      "indicate\n",
      "that\n",
      "there\n",
      "is\n",
      "structure\n",
      "information\n",
      "in\n",
      "the\n",
      "compressed\n",
      "sentence\n",
      "latents\n",
      ".\n",
      "It\n",
      "is\n",
      "possible\n",
      "that\n",
      "the\n",
      "one\n",
      "-\n",
      "level\n",
      "VAE\n",
      "also\n",
      "detects\n",
      "10It\n",
      "might\n",
      "appear\n",
      "surprising\n",
      "that\n",
      "the\n",
      "two\n",
      "-\n",
      "level\n",
      "approach\n",
      "leads\n",
      "to\n",
      "lower\n",
      "performance\n",
      "on\n",
      "type\n",
      "III\n",
      "data\n",
      ",\n",
      "particularly\n",
      "when\n",
      "lexical\n",
      "variation\n",
      "had\n",
      "not\n",
      "been\n",
      "an\n",
      "issue\n",
      "for\n",
      "the\n",
      "sentence\n",
      "representation\n",
      "analysis\n",
      "Section\n",
      "4.1\n",
      ".\n",
      "The\n",
      "difference\n",
      "comes\n",
      "from\n",
      "the\n",
      "way\n",
      "the\n",
      "instances\n",
      "were\n",
      "formed\n",
      ",\n",
      "on\n",
      "the\n",
      "fly\n",
      ",\n",
      "for\n",
      "the\n",
      "two\n",
      "-\n",
      "level\n",
      "process\n",
      "the\n",
      "positive\n",
      "sentence\n",
      "to\n",
      "be\n",
      "reconstructed\n",
      "is\n",
      "the\n",
      "same\n",
      "as\n",
      "the\n",
      "input\n",
      ",\n",
      "instead\n",
      "of\n",
      "being\n",
      "a\n",
      "sentence\n",
      "that\n",
      "has\n",
      "the\n",
      "same\n",
      "structure\n",
      ",\n",
      "but\n",
      "different\n",
      "lexical\n",
      "material\n",
      ".\n",
      "This\n",
      "is\n",
      "because\n",
      "all\n",
      "sentences\n",
      "in\n",
      "the\n",
      "sequence\n",
      "have\n",
      "different\n",
      "structures\n",
      ".\n",
      "We\n",
      "think\n",
      "this\n",
      "weakens\n",
      "the\n",
      "indirect\n",
      "supervision\n",
      "signal\n",
      "as\n",
      "the\n",
      "correct\n",
      "answer\n",
      "is\n",
      "distinct\n",
      "from\n",
      "the\n",
      "other\n",
      "options\n",
      ".\n",
      "This\n",
      "is\n",
      "not\n",
      "the\n",
      "case\n",
      "for\n",
      "type\n",
      "I\n",
      "and\n",
      "II\n",
      "data\n",
      ",\n",
      "where\n",
      ",\n",
      "because\n",
      "of\n",
      "the\n",
      "very\n",
      "similar\n",
      "lexical\n",
      "material\n",
      ",\n",
      "the\n",
      "distinction\n",
      "between\n",
      "the\n",
      "correct\n",
      "and\n",
      "incorrect\n",
      "answers\n",
      "reduce\n",
      "to\n",
      "the\n",
      "structure\n",
      ".\n",
      "We\n",
      "plan\n",
      "to\n",
      "confirm\n",
      "this\n",
      "in\n",
      "future\n",
      "work\n",
      "using\n",
      "a\n",
      "pre\n",
      "-\n",
      "trained\n",
      "sentence\n",
      "-\n",
      "level\n",
      "VAE\n",
      ".\n",
      "7\n",
      "TSNE\n",
      "projection\n",
      "of\n",
      "latent\n",
      "representations\n",
      "from\n",
      "the\n",
      "latent\n",
      "layer\n",
      "of\n",
      "the\n",
      "sentence\n",
      "level\n",
      "for\n",
      "the\n",
      "sentences\n",
      "in\n",
      "BLM\n",
      "contexts\n",
      "in\n",
      "the\n",
      "training\n",
      "data\n",
      ",\n",
      "coloured\n",
      "by\n",
      "the\n",
      "pattern\n",
      "of\n",
      "semantic\n",
      "roles\n",
      ".\n",
      "Average\n",
      "F1\n",
      "score\n",
      "overruns\n",
      "But\n",
      "the\n",
      "fact\n",
      "that\n",
      "the\n",
      "one\n",
      "-\n",
      "level\n",
      "model\n",
      "makes\n",
      "more\n",
      "sequence\n",
      "-\n",
      "based\n",
      "errors\n",
      "indicates\n",
      "that\n",
      "modeling\n",
      "structural\n",
      "information\n",
      "separately\n",
      "is\n",
      "not\n",
      "only\n",
      "possible\n",
      ",\n",
      "but\n",
      "also\n",
      "beneficial\n",
      "for\n",
      "some\n",
      "tasks\n",
      ".\n",
      "The\n",
      "results\n",
      "on\n",
      "the\n",
      "verb\n",
      "alternation\n",
      "BLMs\n",
      "are\n",
      "shown\n",
      "in\n",
      "Figuresand\n",
      "7\n",
      ".\n",
      "In\n",
      "this\n",
      "problem\n",
      ",\n",
      "and\n",
      "unlike\n",
      "the\n",
      "verb\n",
      "-\n",
      "agreement\n",
      "BLM\n",
      "task\n",
      ",\n",
      "structurally\n",
      "similar\n",
      "chunks\n",
      "-\n",
      "NPs\n",
      ",\n",
      "PPs\n",
      "play\n",
      "different\n",
      "semantic\n",
      "roles\n",
      "in\n",
      "the\n",
      "verb\n",
      "alternation\n",
      "data\n",
      ",\n",
      "as\n",
      "shown\n",
      "in\n",
      "Other\n",
      "attributes\n",
      "of\n",
      "chunks\n",
      "that\n",
      "are\n",
      "relevant\n",
      "to\n",
      "the\n",
      "current\n",
      "problem\n",
      "in\n",
      "this\n",
      "case\n",
      ",\n",
      "semantic\n",
      "roles\n",
      "are\n",
      "separated\n",
      "from\n",
      "the\n",
      "sentence\n",
      "embedding\n",
      "whole\n",
      ".\n",
      "This\n",
      "is\n",
      "apparent\n",
      "not\n",
      "only\n",
      "through\n",
      "the\n",
      "F1\n",
      "results\n",
      "on\n",
      "the\n",
      "task\n",
      ",\n",
      "but\n",
      "also\n",
      ",\n",
      "and\n",
      "maybe\n",
      "more\n",
      "clearly\n",
      ",\n",
      "from\n",
      "the\n",
      "projection\n",
      "of\n",
      "the\n",
      "latent\n",
      "representations\n",
      "from\n",
      "the\n",
      "sentence\n",
      "level\n",
      ",\n",
      "where\n",
      "the\n",
      "separation\n",
      "of\n",
      "the\n",
      "different\n",
      "chunk\n",
      "syntactic\n",
      "and\n",
      "semantic\n",
      "patterns\n",
      "is\n",
      "clear\n",
      "for\n",
      "TSNE\n",
      "projection\n",
      "of\n",
      "latent\n",
      "representations\n",
      "from\n",
      "the\n",
      "latent\n",
      "layer\n",
      "of\n",
      "the\n",
      "sentence\n",
      "level\n",
      "for\n",
      "the\n",
      "sentences\n",
      "in\n",
      "BLM\n",
      "contexts\n",
      "in\n",
      "the\n",
      "training\n",
      "data\n",
      ",\n",
      "coloured\n",
      "by\n",
      "the\n",
      "pattern\n",
      "of\n",
      "semantic\n",
      "roles\n",
      ".\n",
      "Average\n",
      "F1\n",
      "score\n",
      "overruns\n",
      "For\n",
      "both\n",
      "data\n",
      "subsets\n",
      ",\n",
      "the\n",
      "closest\n",
      "representations\n",
      "are\n",
      "two\n",
      "that\n",
      "have\n",
      "the\n",
      "same\n",
      "syntactic\n",
      "pattern\n",
      "NP\n",
      "VerbPass\n",
      "PP\n",
      ",\n",
      "but\n",
      "semantically\n",
      "differ\n",
      "NP\n",
      "-\n",
      "Theme\n",
      "VerbPass\n",
      "PP\n",
      "-\n",
      "Agent\n",
      "vs.\n",
      "NP\n",
      "-\n",
      "Loc\n",
      "VerbPass\n",
      "PP\n",
      "-\n",
      "Agent\n",
      ".\n",
      "4.3\n",
      "Discussion\n",
      "We\n",
      "performed\n",
      "two\n",
      "types\n",
      "of\n",
      "experiments\n",
      "i\n",
      "using\n",
      "individual\n",
      "sentences\n",
      ",\n",
      "and\n",
      "an\n",
      "indirect\n",
      "supervision\n",
      "signal\n",
      "about\n",
      "the\n",
      "sentence\n",
      "structure\n",
      ",\n",
      "ii\n",
      "incorporating\n",
      "a\n",
      "sentence\n",
      "representation\n",
      "compression\n",
      "step\n",
      "in\n",
      "a\n",
      "task\n",
      "-\n",
      "specific\n",
      "setting\n",
      ".\n",
      "We\n",
      "used\n",
      "two\n",
      "tasks\n",
      ",\n",
      "one\n",
      "which\n",
      "relies\n",
      "on\n",
      "more\n",
      "structural\n",
      "information\n",
      "subject\n",
      "-\n",
      "verb\n",
      "agreement\n",
      ",\n",
      "and\n",
      "one\n",
      "that\n",
      "also\n",
      "relies\n",
      "on\n",
      "semantic\n",
      "information\n",
      "about\n",
      "the\n",
      "chunks\n",
      "verb\n",
      "alternation\n",
      ".\n",
      "We\n",
      "have\n",
      "investigated\n",
      "each\n",
      "set\n",
      "-\n",
      "up\n",
      "in\n",
      "terms\n",
      "of\n",
      "the\n",
      "results\n",
      "on\n",
      "the\n",
      "task\n",
      "as\n",
      "average\n",
      "F1\n",
      "scores\n",
      ",\n",
      "and\n",
      "through\n",
      "error\n",
      "analysis\n",
      "and\n",
      "in\n",
      "terms\n",
      "of\n",
      "internal\n",
      "representations\n",
      "on\n",
      "the\n",
      "latent\n",
      "layer\n",
      "of\n",
      "an\n",
      "encoder8\n",
      "decoder\n",
      "architecture\n",
      ".\n",
      "This\n",
      "dual\n",
      "analysis\n",
      "allows\n",
      "us\n",
      "to\n",
      "conclude\n",
      "not\n",
      "only\n",
      "that\n",
      "a\n",
      "task\n",
      "is\n",
      "solved\n",
      "correctly\n",
      ",\n",
      "but\n",
      "that\n",
      "it\n",
      "is\n",
      "solved\n",
      "using\n",
      "structural\n",
      ",\n",
      "morphological\n",
      "and\n",
      "semantic\n",
      "information\n",
      "from\n",
      "the\n",
      "sentence\n",
      ".\n",
      "We\n",
      "found\n",
      "that\n",
      "information\n",
      "about\n",
      "varying\n",
      "numbers\n",
      "of\n",
      "chunks\n",
      "noun\n",
      ",\n",
      "verb\n",
      "and\n",
      "prepositional\n",
      "phrases\n",
      "and\n",
      "their\n",
      "task\n",
      "-\n",
      "relevant\n",
      "attributes\n",
      ",\n",
      "whether\n",
      "morphological\n",
      "or\n",
      "semantic\n",
      ",\n",
      "can\n",
      "be\n",
      "detected\n",
      "in\n",
      "sentence\n",
      "embeddings\n",
      "from\n",
      "a\n",
      "pretrained\n",
      "transformer\n",
      "model\n",
      ".\n",
      "Conclusions\n",
      "Sentence\n",
      "embeddings\n",
      "obtained\n",
      "from\n",
      "transformer\n",
      "models\n",
      "are\n",
      "compact\n",
      "representations\n",
      ",\n",
      "compressing\n",
      "much\n",
      "knowledge\n",
      "morphological\n",
      ",\n",
      "grammatical\n",
      ",\n",
      "semantic\n",
      ",\n",
      "pragmatic\n",
      ",\n",
      "expressed\n",
      "in\n",
      "text\n",
      "fragments\n",
      "of\n",
      "various\n",
      "length\n",
      ",\n",
      "into\n",
      "a\n",
      "vector\n",
      "of\n",
      "real\n",
      "numbers\n",
      "of\n",
      "fixed\n",
      "length\n",
      ".\n",
      "If\n",
      "we\n",
      "view\n",
      "the\n",
      "sentence\n",
      "embedding\n",
      "as\n",
      "overlapping\n",
      "layers\n",
      "of\n",
      "information\n",
      ",\n",
      "in\n",
      "a\n",
      "manner\n",
      "similar\n",
      "to\n",
      "audio\n",
      "signals\n",
      "which\n",
      "consist\n",
      "of\n",
      "overlapping\n",
      "signals\n",
      "of\n",
      "different\n",
      "frequencies\n",
      ",\n",
      "we\n",
      "can\n",
      "distinguish\n",
      "specific\n",
      "information\n",
      "among\n",
      "these\n",
      "layers\n",
      ".\n",
      "In\n",
      "particular\n",
      ",\n",
      "we\n",
      "have\n",
      "shown\n",
      "that\n",
      "we\n",
      "can\n",
      "detect\n",
      "information\n",
      "about\n",
      "chunks\n",
      "nounverbprepositional\n",
      "phrases\n",
      "and\n",
      "their\n",
      "task\n",
      "-\n",
      "relevant\n",
      "attributes\n",
      "in\n",
      "these\n",
      "compact\n",
      "sentence\n",
      "representations\n",
      ".\n",
      "These\n",
      "building\n",
      "blocks\n",
      "can\n",
      "be\n",
      "further\n",
      "used\n",
      "in\n",
      "lexically\n",
      "-\n",
      "novel\n",
      "instances\n",
      "to\n",
      "solve\n",
      "tasks\n",
      "that\n",
      "require\n",
      "analytical\n",
      "reasoning\n",
      ",\n",
      "demonstrating\n",
      "that\n",
      "solutions\n",
      "to\n",
      "this\n",
      "task\n",
      "are\n",
      "achieved\n",
      "through\n",
      "abstract\n",
      "steps\n",
      "typical\n",
      "of\n",
      "fluid\n",
      "intelligence\n",
      ".\n",
      "Limitations\n",
      "We\n",
      "have\n",
      "performed\n",
      "experiments\n",
      "on\n",
      "datasets\n",
      "containing\n",
      "sentences\n",
      "with\n",
      "specific\n",
      "structure\n",
      "and\n",
      "properties\n",
      "to\n",
      "be\n",
      "able\n",
      "to\n",
      "determine\n",
      "whether\n",
      "the\n",
      "type\n",
      "of\n",
      "information\n",
      "we\n",
      "targeted\n",
      "can\n",
      "be\n",
      "detected\n",
      "in\n",
      "sentence\n",
      "embeddings\n",
      ".\n",
      "We\n",
      "applied\n",
      "our\n",
      "framework\n",
      "on\n",
      "a\n",
      "particular\n",
      "pretrained\n",
      "transformer\n",
      "model\n",
      "Electra\n",
      "which\n",
      "we\n",
      "chose\n",
      "because\n",
      "of\n",
      "the\n",
      "stronger\n",
      "influence\n",
      "of\n",
      "the\n",
      "full\n",
      "context\n",
      "on\n",
      "producing\n",
      "sentence\n",
      "embeddings\n",
      ".\n",
      "Different\n",
      "transformer\n",
      "models\n",
      "may\n",
      "produce\n",
      "different\n",
      "encoding\n",
      "patterns\n",
      "in\n",
      "the\n",
      "sentence\n",
      "embeddings\n",
      ".\n",
      "References\n",
      "Steven\n",
      "Abney\n",
      ".\n",
      "1992\n",
      ".\n",
      "Prosodic\n",
      "structure\n",
      ",\n",
      "performance\n",
      "structure\n",
      "and\n",
      "phrase\n",
      "structure\n",
      ".\n",
      "In\n",
      "Speech\n",
      "and\n",
      "Natural\n",
      "Language\n",
      "Proceedings\n",
      "of\n",
      "a\n",
      "Workshop\n",
      "Held\n",
      "at\n",
      "Harriman\n",
      ",\n",
      "New\n",
      "York\n",
      ",\n",
      "February\n",
      "23\n",
      "-\n",
      "26\n",
      ",\n",
      "1992\n",
      ".Aixiu\n",
      "An\n",
      ",\n",
      "Chunyang\n",
      "Jiang\n",
      ",\n",
      "Maria\n",
      "A.\n",
      "Rodriguez\n",
      ",\n",
      "Vivi\n",
      "Nastase\n",
      ",\n",
      "and\n",
      "Paola\n",
      "Merlo\n",
      ".\n",
      "2023\n",
      ".\n",
      "BLM\n",
      "-\n",
      "AgrF\n",
      "A\n",
      "new\n",
      "French\n",
      "benchmark\n",
      "to\n",
      "investigate\n",
      "generalization\n",
      "of\n",
      "agreement\n",
      "in\n",
      "neural\n",
      "networks\n",
      ".\n",
      "In\n",
      "Proceedings\n",
      "of\n",
      "the\n",
      "17th\n",
      "Conference\n",
      "of\n",
      "the\n",
      "European\n",
      "Chapter\n",
      "of\n",
      "the\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      ",\n",
      "pages\n",
      "1363\n",
      "1374\n",
      ",\n",
      "Dubrovnik\n",
      ",\n",
      "Croatia\n",
      ".\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      ".\n",
      "Yu\n",
      "Bao\n",
      ",\n",
      "Hao\n",
      "Zhou\n",
      ",\n",
      "Shujian\n",
      "Huang\n",
      ",\n",
      "Lei\n",
      "Li\n",
      ",\n",
      "Lili\n",
      "Mou\n",
      ",\n",
      "Olga\n",
      "Vechtomova\n",
      ",\n",
      "Xin\n",
      "-\n",
      "yu\n",
      "Dai\n",
      ",\n",
      "and\n",
      "Jiajun\n",
      "Chen\n",
      ".\n",
      "2019\n",
      ".\n",
      "Generating\n",
      "sentences\n",
      "from\n",
      "disentangled\n",
      "syntactic\n",
      "and\n",
      "semantic\n",
      "spaces\n",
      ".\n",
      "In\n",
      "Proceedings\n",
      "of\n",
      "the\n",
      "57th\n",
      "Annual\n",
      "Meeting\n",
      "of\n",
      "the\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      ",\n",
      "pages\n",
      "60086019\n",
      ",\n",
      "Florence\n",
      ",\n",
      "Italy\n",
      ".\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      ".\n",
      "Yonatan\n",
      "Belinkov\n",
      ".\n",
      "2022\n",
      ".\n",
      "Probing\n",
      "classifiers\n",
      "Promises\n",
      ",\n",
      "shortcomings\n",
      ",\n",
      "and\n",
      "advances\n",
      ".\n",
      "Computational\n",
      "Linguistics\n",
      ",\n",
      "481207219\n",
      ".\n",
      "Samuel\n",
      "R.\n",
      "Bowman\n",
      ",\n",
      "Luke\n",
      "Vilnis\n",
      ",\n",
      "Oriol\n",
      "Vinyals\n",
      ",\n",
      "Andrew\n",
      "Dai\n",
      ",\n",
      "Rafal\n",
      "Jozefowicz\n",
      ",\n",
      "and\n",
      "Samy\n",
      "Bengio\n",
      ".\n",
      "2016\n",
      ".\n",
      "Generating\n",
      "sentences\n",
      "from\n",
      "a\n",
      "continuous\n",
      "space\n",
      ".\n",
      "In\n",
      "Proceedings\n",
      "of\n",
      "the\n",
      "20th\n",
      "SIGNLL\n",
      "Conference\n",
      "on\n",
      "Computational\n",
      "Natural\n",
      "Language\n",
      "Learning\n",
      ",\n",
      "pages\n",
      "1021\n",
      ",\n",
      "Berlin\n",
      ",\n",
      "Germany\n",
      ".\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      ".\n",
      "Patricia\n",
      "ACarpenter\n",
      ",\n",
      "Marcel\n",
      "AJust\n",
      ",\n",
      "and\n",
      "Peter\n",
      "Shell\n",
      ".\n",
      "1990\n",
      ".\n",
      "What\n",
      "one\n",
      "intelligence\n",
      "test\n",
      "measures\n",
      "a\n",
      "theoretical\n",
      "account\n",
      "of\n",
      "the\n",
      "processing\n",
      "in\n",
      "the\n",
      "raven\n",
      "progressive\n",
      "matrices\n",
      "test\n",
      ".\n",
      "Psychological\n",
      "review\n",
      ",\n",
      "973404\n",
      ".\n",
      "Mingda\n",
      "Chen\n",
      ",\n",
      "Qingming\n",
      "Tang\n",
      ",\n",
      "Sam\n",
      "Wiseman\n",
      ",\n",
      "and\n",
      "Kevin\n",
      "Gimpel\n",
      ".\n",
      "2019\n",
      ".\n",
      "A\n",
      "multi\n",
      "-\n",
      "task\n",
      "approach\n",
      "for\n",
      "disentangling\n",
      "syntax\n",
      "and\n",
      "semantics\n",
      "in\n",
      "sentence\n",
      "representations\n",
      ".\n",
      "In\n",
      "Proceedings\n",
      "of\n",
      "the\n",
      "2019\n",
      "Conference\n",
      "of\n",
      "the\n",
      "North\n",
      "American\n",
      "Chapter\n",
      "of\n",
      "the\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      "Human\n",
      "Language\n",
      "Technologies\n",
      ",\n",
      "VolumeLong\n",
      "and\n",
      "Short\n",
      "Papers\n",
      ",\n",
      "pages\n",
      "24532464\n",
      ",\n",
      "Minneapolis\n",
      ",\n",
      "Minnesota\n",
      ".\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      ".\n",
      "Ethan\n",
      "A.\n",
      "Chi\n",
      ",\n",
      "John\n",
      "Hewitt\n",
      ",\n",
      "and\n",
      "Christopher\n",
      "D.\n",
      "Manning\n",
      ".\n",
      "2020\n",
      ".\n",
      "Finding\n",
      "universal\n",
      "grammatical\n",
      "relations\n",
      "in\n",
      "multilingual\n",
      "BERT\n",
      ".\n",
      "In\n",
      "Proceedings\n",
      "of\n",
      "the\n",
      "58th\n",
      "Annual\n",
      "Meeting\n",
      "of\n",
      "the\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      ",\n",
      "pages\n",
      "55645577\n",
      ",\n",
      "Online\n",
      ".\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      ".\n",
      "Kevin\n",
      "Clark\n",
      ",\n",
      "Urvashi\n",
      "Khandelwal\n",
      ",\n",
      "Omer\n",
      "Levy\n",
      ",\n",
      "and\n",
      "Christopher\n",
      "D.\n",
      "Manning\n",
      ".\n",
      "2019\n",
      ".\n",
      "What\n",
      "does\n",
      "BERT\n",
      "look\n",
      "at\n",
      "?\n",
      "an\n",
      "analysis\n",
      "of\n",
      "BERTs\n",
      "attention\n",
      ".\n",
      "In\n",
      "Proceedings\n",
      "of\n",
      "the\n",
      "2019\n",
      "ACL\n",
      "Workshop\n",
      "BlackboxNLP\n",
      "Analyzing\n",
      "and\n",
      "Interpreting\n",
      "Neural\n",
      "Networks\n",
      "for\n",
      "NLP\n",
      ",\n",
      "pages\n",
      "276286\n",
      ",\n",
      "Florence\n",
      ",\n",
      "Italy\n",
      ".\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      ".\n",
      "Kevin\n",
      "Clark\n",
      ",\n",
      "Minh\n",
      "-\n",
      "Thang\n",
      "Luong\n",
      ",\n",
      "Quoc\n",
      "V\n",
      ".\n",
      "Le\n",
      ",\n",
      "and\n",
      "Christopher\n",
      "D.\n",
      "Manning\n",
      ".\n",
      "2020\n",
      ".\n",
      "ELECTRA\n",
      "Pretraining\n",
      "text\n",
      "encoders\n",
      "as\n",
      "discriminators\n",
      "rather\n",
      "than\n",
      "generators\n",
      ".\n",
      "In\n",
      "ICLR\n",
      ".\n",
      "9\n",
      "Michael\n",
      "Collins\n",
      ".\n",
      "1997\n",
      ".\n",
      "Three\n",
      "generative\n",
      ",\n",
      "lexicalised\n",
      "models\n",
      "for\n",
      "statistical\n",
      "parsing\n",
      ".\n",
      "In\n",
      "35th\n",
      "Annual\n",
      "Meeting\n",
      "of\n",
      "the\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      "and\n",
      "8th\n",
      "Conference\n",
      "of\n",
      "the\n",
      "European\n",
      "Chapter\n",
      "of\n",
      "the\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      ",\n",
      "pages\n",
      "16\n",
      "23\n",
      ",\n",
      "Madrid\n",
      ",\n",
      "Spain\n",
      ".\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      ".\n",
      "Simone\n",
      "Conia\n",
      "and\n",
      "Roberto\n",
      "Navigli\n",
      ".\n",
      "2022\n",
      ".\n",
      "Probing\n",
      "for\n",
      "predicate\n",
      "argument\n",
      "structures\n",
      "in\n",
      "pretrained\n",
      "language\n",
      "models\n",
      ".\n",
      "In\n",
      "Proceedings\n",
      "of\n",
      "the\n",
      "60th\n",
      "Annual\n",
      "Meeting\n",
      "of\n",
      "the\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      "VolumeLong\n",
      "Papers\n",
      ",\n",
      "pages\n",
      "46224632\n",
      ",\n",
      "Dublin\n",
      ",\n",
      "Ireland\n",
      ".\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      ".\n",
      "Alexis\n",
      "Conneau\n",
      "and\n",
      "Douwe\n",
      "Kiela\n",
      ".\n",
      "2018\n",
      ".\n",
      "SentEval\n",
      "An\n",
      "evaluation\n",
      "toolkit\n",
      "for\n",
      "universal\n",
      "sentence\n",
      "representations\n",
      ".\n",
      "In\n",
      "Proceedings\n",
      "of\n",
      "the\n",
      "Eleventh\n",
      "International\n",
      "Conference\n",
      "on\n",
      "Language\n",
      "Resources\n",
      "and\n",
      "Evaluation\n",
      "LREC\n",
      "2018\n",
      ",\n",
      "Miyazaki\n",
      ",\n",
      "Japan\n",
      ".\n",
      "European\n",
      "Language\n",
      "Resources\n",
      "Association\n",
      "ELRA\n",
      ".\n",
      "Alexis\n",
      "Conneau\n",
      ",\n",
      "German\n",
      "Kruszewski\n",
      ",\n",
      "Guillaume\n",
      "Lample\n",
      ",\n",
      "Loc\n",
      "Barrault\n",
      ",\n",
      "and\n",
      "Marco\n",
      "Baroni\n",
      ".\n",
      "2018\n",
      ".\n",
      "What\n",
      "you\n",
      "can\n",
      "cram\n",
      "into\n",
      "a\n",
      "single\n",
      "!\n",
      "vector\n",
      "Probing\n",
      "sentence\n",
      "embeddings\n",
      "for\n",
      "linguistic\n",
      "properties\n",
      ".\n",
      "In\n",
      "Proceedings\n",
      "of\n",
      "the\n",
      "56th\n",
      "Annual\n",
      "Meeting\n",
      "of\n",
      "the\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      "VolumeLong\n",
      "Papers\n",
      ",\n",
      "pages\n",
      "21262136\n",
      ",\n",
      "Melbourne\n",
      ",\n",
      "Australia\n",
      ".\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      ".\n",
      "Wietse\n",
      "de\n",
      "Vries\n",
      ",\n",
      "Andreas\n",
      "van\n",
      "Cranenburgh\n",
      ",\n",
      "and\n",
      "Malvina\n",
      "Nissim\n",
      ".\n",
      "2020\n",
      ".\n",
      "What\n",
      "s\n",
      "so\n",
      "special\n",
      "about\n",
      "BERTs\n",
      "layers\n",
      "?\n",
      "a\n",
      "closer\n",
      "look\n",
      "at\n",
      "the\n",
      "NLP\n",
      "pipeline\n",
      "in\n",
      "monolingual\n",
      "and\n",
      "multilingual\n",
      "models\n",
      ".\n",
      "In\n",
      "Findings\n",
      "of\n",
      "the\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      "EMNLP\n",
      "2020\n",
      ",\n",
      "pages\n",
      "43394350\n",
      ",\n",
      "Online\n",
      ".\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      ".\n",
      "Jacob\n",
      "Devlin\n",
      ",\n",
      "Ming\n",
      "-\n",
      "Wei\n",
      "Chang\n",
      ",\n",
      "Kenton\n",
      "Lee\n",
      ",\n",
      "and\n",
      "Kristina\n",
      "Toutanova\n",
      ".\n",
      "2018\n",
      ".\n",
      "BERT\n",
      "pre\n",
      "-\n",
      "training\n",
      "of\n",
      "deep\n",
      "bidirectional\n",
      "transformers\n",
      "for\n",
      "language\n",
      "understanding\n",
      ".\n",
      "CoRR\n",
      ",\n",
      "abs1810.04805\n",
      ".\n",
      "Jacob\n",
      "Devlin\n",
      ",\n",
      "Ming\n",
      "-\n",
      "Wei\n",
      "Chang\n",
      ",\n",
      "Kenton\n",
      "Lee\n",
      ",\n",
      "and\n",
      "Kristina\n",
      "Toutanova\n",
      ".\n",
      "2019\n",
      ".\n",
      "BERT\n",
      "Pre\n",
      "-\n",
      "training\n",
      "of\n",
      "deep\n",
      "bidirectional\n",
      "transformers\n",
      "for\n",
      "language\n",
      "understanding\n",
      ".\n",
      "In\n",
      "Proceedings\n",
      "of\n",
      "the\n",
      "2019\n",
      "Conference\n",
      "of\n",
      "the\n",
      "North\n",
      "American\n",
      "Chapter\n",
      "of\n",
      "the\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      "Human\n",
      "Language\n",
      "Technologies\n",
      ",\n",
      "VolumeLong\n",
      "and\n",
      "Short\n",
      "Papers\n",
      ",\n",
      "pages\n",
      "41714186\n",
      ",\n",
      "Minneapolis\n",
      ",\n",
      "Minnesota\n",
      ".\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      ".\n",
      "Yanai\n",
      "Elazar\n",
      ",\n",
      "Shauli\n",
      "Ravfogel\n",
      ",\n",
      "Alon\n",
      "Jacovi\n",
      ",\n",
      "and\n",
      "Yoav\n",
      "Goldberg\n",
      ".\n",
      "2021\n",
      ".\n",
      "Amnesic\n",
      "probing\n",
      "Behavioral\n",
      "explanation\n",
      "with\n",
      "amnesic\n",
      "counterfactuals\n",
      ".\n",
      "Transactions\n",
      "of\n",
      "the\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      ",\n",
      "9160\n",
      "175\n",
      ".\n",
      "Julie\n",
      "Franck\n",
      ",\n",
      "Gabriella\n",
      "Vigliocco\n",
      ",\n",
      "and\n",
      "Janet\n",
      ".\n",
      "Nicol\n",
      ".\n",
      "2002\n",
      ".\n",
      "Subject\n",
      "-\n",
      "verb\n",
      "agreement\n",
      "errors\n",
      "in\n",
      "French\n",
      "and\n",
      "English\n",
      "The\n",
      "role\n",
      "of\n",
      "syntactic\n",
      "hierarchy\n",
      ".\n",
      "Language\n",
      "and\n",
      "Cognitive\n",
      "Processes\n",
      ",\n",
      "174371404.John\n",
      "Hewitt\n",
      "and\n",
      "Christopher\n",
      "D.\n",
      "Manning\n",
      ".\n",
      "2019\n",
      ".\n",
      "A\n",
      "structural\n",
      "probe\n",
      "for\n",
      "finding\n",
      "syntax\n",
      "in\n",
      "word\n",
      "representations\n",
      ".\n",
      "In\n",
      "Proceedings\n",
      "of\n",
      "the\n",
      "2019\n",
      "Conference\n",
      "of\n",
      "the\n",
      "North\n",
      "American\n",
      "Chapter\n",
      "of\n",
      "the\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      "Human\n",
      "Language\n",
      "Technologies\n",
      ",\n",
      "VolumeLong\n",
      "and\n",
      "Short\n",
      "Papers\n",
      ",\n",
      "pages\n",
      "41294138\n",
      ",\n",
      "Minneapolis\n",
      ",\n",
      "Minnesota\n",
      ".\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      ".\n",
      "Phu\n",
      "Mon\n",
      "Htut\n",
      ",\n",
      "Jason\n",
      "Phang\n",
      ",\n",
      "Shikha\n",
      "Bordia\n",
      ",\n",
      "and\n",
      "Samuel\n",
      "R.\n",
      "Bowman\n",
      ".\n",
      "2019\n",
      ".\n",
      "Do\n",
      "attention\n",
      "heads\n",
      "in\n",
      "bert\n",
      "track\n",
      "syntactic\n",
      "dependencies\n",
      "?\n",
      "Preprint\n",
      ",\n",
      "arXiv1911.12246\n",
      ".\n",
      "Daphne\n",
      "Ippolito\n",
      ",\n",
      "David\n",
      "Grangier\n",
      ",\n",
      "Douglas\n",
      "Eck\n",
      ",\n",
      "and\n",
      "Chris\n",
      "Callison\n",
      "-\n",
      "Burch\n",
      ".\n",
      "2020\n",
      ".\n",
      "Toward\n",
      "better\n",
      "storylines\n",
      "with\n",
      "sentence\n",
      "-\n",
      "level\n",
      "language\n",
      "models\n",
      ".\n",
      "In\n",
      "Proceedings\n",
      "of\n",
      "the\n",
      "58th\n",
      "Annual\n",
      "Meeting\n",
      "of\n",
      "the\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      ",\n",
      "pages\n",
      "74727478\n",
      ",\n",
      "Online\n",
      ".\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      ".\n",
      "Ganesh\n",
      "Jawahar\n",
      ",\n",
      "Benot\n",
      "Sagot\n",
      ",\n",
      "and\n",
      "Djam\n",
      "Seddah\n",
      ".\n",
      "2019\n",
      ".\n",
      "What\n",
      "does\n",
      "BERT\n",
      "learn\n",
      "about\n",
      "the\n",
      "structure\n",
      "of\n",
      "language\n",
      "?\n",
      "In\n",
      "Proceedings\n",
      "of\n",
      "the\n",
      "57th\n",
      "Annual\n",
      "Meeting\n",
      "of\n",
      "the\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      ",\n",
      "pages\n",
      "36513657\n",
      ",\n",
      "Florence\n",
      ",\n",
      "Italy\n",
      ".\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      ".\n",
      "Diederik\n",
      "PKingma\n",
      "and\n",
      "Max\n",
      "Welling\n",
      ".\n",
      "2013\n",
      ".\n",
      "Autoencoding\n",
      "variational\n",
      "bayes\n",
      ".\n",
      "arXiv\n",
      "preprint\n",
      "arXiv1312.6114\n",
      ".\n",
      "Zhenzhong\n",
      "Lan\n",
      ",\n",
      "Mingda\n",
      "Chen\n",
      ",\n",
      "Sebastian\n",
      "Goodman\n",
      ",\n",
      "Kevin\n",
      "Gimpel\n",
      ",\n",
      "Piyush\n",
      "Sharma\n",
      ",\n",
      "and\n",
      "Radu\n",
      "Soricut\n",
      ".\n",
      "2019\n",
      ".\n",
      "ALBERT\n",
      "A\n",
      "lite\n",
      "BERT\n",
      "for\n",
      "selfsupervised\n",
      "learning\n",
      "of\n",
      "language\n",
      "representations\n",
      ".\n",
      "CoRR\n",
      ",\n",
      "abs1909.11942\n",
      ".\n",
      "Yinhan\n",
      "Liu\n",
      ",\n",
      "Myle\n",
      "Ott\n",
      ",\n",
      "Naman\n",
      "Goyal\n",
      ",\n",
      "Jingfei\n",
      "Du\n",
      ",\n",
      "Mandar\n",
      "Joshi\n",
      ",\n",
      "Danqi\n",
      "Chen\n",
      ",\n",
      "Omer\n",
      "Levy\n",
      ",\n",
      "Mike\n",
      "Lewis\n",
      ",\n",
      "Luke\n",
      "Zettlemoyer\n",
      ",\n",
      "and\n",
      "Veselin\n",
      "Stoyanov\n",
      ".\n",
      "2019\n",
      ".\n",
      "Roberta\n",
      "A\n",
      "robustly\n",
      "optimized\n",
      "bert\n",
      "pretraining\n",
      "approach\n",
      ".\n",
      "arXiv\n",
      "preprint\n",
      "arXiv1907.11692\n",
      ".\n",
      "Ziyang\n",
      "Luo\n",
      ".\n",
      "2021\n",
      ".\n",
      "Have\n",
      "attention\n",
      "heads\n",
      "in\n",
      "BERT\n",
      "learned\n",
      "constituency\n",
      "grammar\n",
      "?\n",
      "In\n",
      "Proceedings\n",
      "of\n",
      "the\n",
      "16th\n",
      "Conference\n",
      "of\n",
      "the\n",
      "European\n",
      "Chapter\n",
      "of\n",
      "the\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      "Student\n",
      "Research\n",
      "Workshop\n",
      ",\n",
      "pages\n",
      "815\n",
      ",\n",
      "Online\n",
      ".\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      ".\n",
      "Giangiacomo\n",
      "Mercatali\n",
      "and\n",
      "Andr\n",
      "Freitas\n",
      ".\n",
      "2021\n",
      ".\n",
      "Disentangling\n",
      "generative\n",
      "factors\n",
      "in\n",
      "natural\n",
      "language\n",
      "with\n",
      "discrete\n",
      "variational\n",
      "autoencoders\n",
      ".\n",
      "In\n",
      "Findings\n",
      "of\n",
      "the\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      "EMNLP\n",
      "2021\n",
      ",\n",
      "pages\n",
      "35473556\n",
      ",\n",
      "Punta\n",
      "Cana\n",
      ",\n",
      "Dominican\n",
      "Republic\n",
      ".\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      ".\n",
      "Paola\n",
      "Merlo\n",
      ".\n",
      "2023\n",
      ".\n",
      "Blackbird\n",
      "language\n",
      "matrices\n",
      "BLM\n",
      ",\n",
      "a\n",
      "new\n",
      "task\n",
      "for\n",
      "rule\n",
      "-\n",
      "like\n",
      "generalization\n",
      "in\n",
      "neural\n",
      "networks\n",
      "Motivations\n",
      "and\n",
      "formal\n",
      "specifications\n",
      ".\n",
      "ArXiv\n",
      ",\n",
      "cs\n",
      ".\n",
      "CL\n",
      "2306.11444\n",
      ".\n",
      "Vivi\n",
      "Nastase\n",
      "and\n",
      "Paola\n",
      "Merlo\n",
      ".\n",
      "2023\n",
      ".\n",
      "Grammatical\n",
      "information\n",
      "in\n",
      "BERT\n",
      "sentence\n",
      "embeddings\n",
      "as\n",
      "two\n",
      "-\n",
      "dimensional\n",
      "arrays\n",
      ".\n",
      "In\n",
      "Proceedings\n",
      "of\n",
      "the\n",
      "10\n",
      " \n",
      "in\n",
      "large\n",
      "pre\n",
      "-\n",
      "trained\n",
      "language\n",
      "models\n",
      ".\n",
      "In\n",
      "Proceedings\n",
      "of\n",
      "the\n",
      "Fifth\n",
      "BlackboxNLP\n",
      "Workshop\n",
      "on\n",
      "Analyzing\n",
      "and\n",
      "Interpreting\n",
      "Neural\n",
      "Networks\n",
      "for\n",
      "NLP\n",
      ",\n",
      "pages\n",
      "142152\n",
      ",\n",
      "Abu\n",
      "Dhabi\n",
      ",\n",
      "United\n",
      "Arab\n",
      "Emirates\n",
      "Hybrid\n",
      ".\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      ".\n",
      "12\n",
      "A\n",
      "Appendix\n",
      "A.1\n",
      "Sentence\n",
      "data\n",
      "To\n",
      "build\n",
      "the\n",
      "sentence\n",
      "data\n",
      ",\n",
      "we\n",
      "use\n",
      "a\n",
      "seed\n",
      "file\n",
      "that\n",
      "was\n",
      "used\n",
      "to\n",
      "generate\n",
      "the\n",
      "subject\n",
      "-\n",
      "verb\n",
      "agreement\n",
      "data\n",
      ".\n",
      "A\n",
      "seed\n",
      ",\n",
      "consisting\n",
      "of\n",
      "noun\n",
      ",\n",
      "prepositional\n",
      "and\n",
      "verb\n",
      "phrases\n",
      "with\n",
      "different\n",
      "grammatical\n",
      "numbers\n",
      ",\n",
      "can\n",
      "be\n",
      "combined\n",
      "to\n",
      "build\n",
      "sentences\n",
      "consisting\n",
      "of\n",
      "different\n",
      "sequences\n",
      "of\n",
      "such\n",
      "chunks\n",
      ".\n",
      "We\n",
      "use\n",
      "French\n",
      "and\n",
      "English\n",
      "versions\n",
      "of\n",
      "the\n",
      "seed\n",
      "file\n",
      "to\n",
      "build\n",
      "the\n",
      "corresponding\n",
      "datasets\n",
      ".\n",
      "Subjsg\n",
      "Subjpl\n",
      "P1sg\n",
      "P1pl\n",
      "P2sg\n",
      "P2pl\n",
      "Vsg\n",
      "Vpl\n",
      "The\n",
      "computerThe\n",
      "computerswith\n",
      "the\n",
      "programwith\n",
      "the\n",
      "programsof\n",
      "the\n",
      "experimentof\n",
      "the\n",
      "experimentsis\n",
      "broken\n",
      "are\n",
      "broken\n",
      "Sent\n",
      ".\n",
      "with\n",
      "different\n",
      "chunks\n",
      "The\n",
      "computer\n",
      "is\n",
      "broken\n",
      ".\n",
      "np\n",
      "-\n",
      "s\n",
      "vp\n",
      "-\n",
      "s\n",
      "The\n",
      "computers\n",
      "are\n",
      "broken\n",
      ".\n",
      "np\n",
      "-\n",
      "p\n",
      "vp\n",
      "-\n",
      "p\n",
      "The\n",
      "computer\n",
      "with\n",
      "the\n",
      "program\n",
      "is\n",
      "broken.np\n",
      "-\n",
      "s\n",
      "pp1\n",
      "-\n",
      "s\n",
      "vp\n",
      "-\n",
      "s\n",
      "...\n",
      "...\n",
      "The\n",
      "computers\n",
      "with\n",
      "the\n",
      "programs\n",
      "of\n",
      "the\n",
      "experiments\n",
      "are\n",
      "broken.np\n",
      "-\n",
      "p\n",
      "pp1\n",
      "-\n",
      "p\n",
      "pp2\n",
      "-\n",
      "p\n",
      "vp\n",
      "-\n",
      "pa\n",
      "BLM\n",
      "instance\n",
      "Context\n",
      "The\n",
      "computer\n",
      "with\n",
      "the\n",
      "program\n",
      "is\n",
      "broken\n",
      ".\n",
      "The\n",
      "computers\n",
      "with\n",
      "the\n",
      "program\n",
      "are\n",
      "broken\n",
      ".\n",
      "The\n",
      "computer\n",
      "with\n",
      "the\n",
      "programs\n",
      "is\n",
      "broken\n",
      ".\n",
      "The\n",
      "computers\n",
      "with\n",
      "the\n",
      "programs\n",
      "are\n",
      "broken\n",
      ".\n",
      "The\n",
      "computer\n",
      "with\n",
      "the\n",
      "program\n",
      "of\n",
      "the\n",
      "experiment\n",
      "is\n",
      "broken\n",
      ".\n",
      "The\n",
      "computers\n",
      "with\n",
      "the\n",
      "program\n",
      "of\n",
      "the\n",
      "experiment\n",
      "are\n",
      "broken\n",
      ".\n",
      "The\n",
      "computer\n",
      "with\n",
      "the\n",
      "programs\n",
      "of\n",
      "the\n",
      "experiment\n",
      "is\n",
      "broken\n",
      ".\n",
      "Answer\n",
      "set\n",
      "The\n",
      "computers\n",
      "with\n",
      "the\n",
      "programs\n",
      "of\n",
      "the\n",
      "experiment\n",
      "are\n",
      "broken\n",
      ".\n",
      "The\n",
      "computers\n",
      "with\n",
      "the\n",
      "programs\n",
      "of\n",
      "the\n",
      "experiments\n",
      "are\n",
      "broken\n",
      ".\n",
      "The\n",
      "computers\n",
      "with\n",
      "the\n",
      "program\n",
      "of\n",
      "the\n",
      "experiment\n",
      "are\n",
      "broken\n",
      ".\n",
      "The\n",
      "computers\n",
      "with\n",
      "the\n",
      "program\n",
      "of\n",
      "the\n",
      "experiment\n",
      "is\n",
      "broken\n",
      ".\n",
      "...\n",
      "The\n",
      "algorithm\n",
      "to\n",
      "produce\n",
      "a\n",
      "dataset\n",
      "from\n",
      "the\n",
      "generated\n",
      "sentences\n",
      "is\n",
      "detailed\n",
      "in\n",
      "Data\n",
      "Nnegs\n",
      "forpatterns\n",
      "pdo\n",
      "forsiSpdo\n",
      "in\n",
      "si\n",
      "forsjSpdo\n",
      "outsj\n",
      "outsk\n",
      ",\n",
      "krange\n",
      "Nnegs\n",
      ",\n",
      "skSp\n",
      "Data\n",
      "Data\n",
      "in\n",
      ",\n",
      "out\n",
      ",\n",
      "out\n",
      "end\n",
      "for\n",
      "end\n",
      "for\n",
      "end\n",
      "for\n",
      "FigureData\n",
      "generation\n",
      "algorithm\n",
      "13\n",
      "A.2\n",
      "Example\n",
      "of\n",
      "data\n",
      "for\n",
      "the\n",
      "agreement\n",
      "BLM\n",
      "Example\n",
      "subject\n",
      "NPs\n",
      "from\n",
      "Franck\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2002\n",
      "Lordinateur\n",
      "avec\n",
      "le\n",
      "programme\n",
      "de\n",
      "lexperience\n",
      "The\n",
      "computer\n",
      "with\n",
      "the\n",
      "program\n",
      "of\n",
      "the\n",
      "experiments\n",
      "Manually\n",
      "expanded\n",
      "and\n",
      "completed\n",
      "sentences\n",
      "Lordinateur\n",
      "avec\n",
      "le\n",
      "programme\n",
      "de\n",
      "lexperience\n",
      "est\n",
      "en\n",
      "panne\n",
      ".\n",
      "The\n",
      "computer\n",
      "with\n",
      "the\n",
      "program\n",
      "of\n",
      "the\n",
      "experiments\n",
      "is\n",
      "down\n",
      ".\n",
      "Jean\n",
      "suppose\n",
      "que\n",
      "lordinateur\n",
      "avec\n",
      "le\n",
      "programme\n",
      "de\n",
      "lexperience\n",
      "est\n",
      "en\n",
      "panne\n",
      ".\n",
      "Jean\n",
      "thinks\n",
      "that\n",
      "the\n",
      "computer\n",
      "with\n",
      "the\n",
      "program\n",
      "of\n",
      "the\n",
      "experiments\n",
      "is\n",
      "down\n",
      ".\n",
      "Lordinateur\n",
      "avec\n",
      "le\n",
      "programme\n",
      "do\n",
      "nt\n",
      "Jean\n",
      "se\n",
      "servait\n",
      "est\n",
      "en\n",
      "panne\n",
      ".\n",
      "The\n",
      "computer\n",
      "with\n",
      "the\n",
      "program\n",
      "that\n",
      "John\n",
      "was\n",
      "using\n",
      "is\n",
      "down\n",
      ".\n",
      "A\n",
      "seed\n",
      "for\n",
      "language\n",
      "matrix\n",
      "generation\n",
      "Jean\n",
      "suppose\n",
      "que\n",
      "lordinateur\n",
      "avec\n",
      "le\n",
      "programme\n",
      "de\n",
      "lexperience\n",
      "do\n",
      "nt\n",
      "Jean\n",
      "se\n",
      "servait\n",
      "est\n",
      "en\n",
      "panne\n",
      "Jean\n",
      "thinks\n",
      "that\n",
      "the\n",
      "computer\n",
      "with\n",
      "the\n",
      "program\n",
      "of\n",
      "the\n",
      "experiment\n",
      "that\n",
      "John\n",
      "was\n",
      "using\n",
      "is\n",
      "down\n",
      "les\n",
      "ordinateurs\n",
      "avec\n",
      "les\n",
      "programmes\n",
      "sont\n",
      "en\n",
      "panne\n",
      "the\n",
      "computers\n",
      "with\n",
      "the\n",
      "programs\n",
      "are\n",
      "down\n",
      ",\n",
      "2002\n",
      ",\n",
      "manually\n",
      "completed\n",
      "and\n",
      "expanded\n",
      "sentences\n",
      "based\n",
      "on\n",
      "these\n",
      "examples\n",
      ",\n",
      "and\n",
      "seeds\n",
      "made\n",
      "based\n",
      "on\n",
      "these\n",
      "sentences\n",
      "for\n",
      "subject\n",
      "-\n",
      "verb\n",
      "agreement\n",
      "BLM\n",
      "dataset\n",
      "that\n",
      "contain\n",
      "all\n",
      "number\n",
      "variations\n",
      "for\n",
      "the\n",
      "nouns\n",
      "and\n",
      "the\n",
      "verb\n",
      ".\n",
      "Main\n",
      "clauseLordinateur\n",
      "avec\n",
      "le\n",
      "programme\n",
      "est\n",
      "en\n",
      "panne\n",
      ".\n",
      "Les\n",
      "ordinateurs\n",
      "avec\n",
      "le\n",
      "programme\n",
      "sont\n",
      "en\n",
      "panne\n",
      ".\n",
      "Lordinateur\n",
      "avec\n",
      "les\n",
      "programmes\n",
      "est\n",
      "en\n",
      "panne\n",
      ".\n",
      "Les\n",
      "ordinateurs\n",
      "avec\n",
      "les\n",
      "programmes\n",
      "sont\n",
      "en\n",
      "panne\n",
      ".\n",
      "Lordinateur\n",
      "avec\n",
      "le\n",
      "programme\n",
      "de\n",
      "lexprience\n",
      "est\n",
      "en\n",
      "panne\n",
      ".\n",
      "Les\n",
      "ordinateurs\n",
      "avec\n",
      "le\n",
      "programme\n",
      "de\n",
      "lexprience\n",
      "sont\n",
      "en\n",
      "panne\n",
      ".\n",
      "Lordinateur\n",
      "avec\n",
      "les\n",
      "programmes\n",
      "de\n",
      "lexprience\n",
      "est\n",
      "en\n",
      "panne\n",
      ".\n",
      "Les\n",
      "ordinateurs\n",
      "avec\n",
      "les\n",
      "programmes\n",
      "de\n",
      "lexprience\n",
      "sont\n",
      "en\n",
      "panne\n",
      ".\n",
      "Completive\n",
      "clauseJean\n",
      "suppose\n",
      "que\n",
      "lordinateur\n",
      "avec\n",
      "le\n",
      "programme\n",
      "est\n",
      "en\n",
      "panne\n",
      ".\n",
      "Jean\n",
      "suppose\n",
      "que\n",
      "les\n",
      "ordinateurs\n",
      "avec\n",
      "le\n",
      "programme\n",
      "sont\n",
      "en\n",
      "panne\n",
      ".\n",
      "Jean\n",
      "suppose\n",
      "que\n",
      "lordinateur\n",
      "avec\n",
      "les\n",
      "programmes\n",
      "est\n",
      "en\n",
      "panne\n",
      ".\n",
      "Jean\n",
      "suppose\n",
      "que\n",
      "les\n",
      "ordinateurs\n",
      "avec\n",
      "les\n",
      "programmes\n",
      "sont\n",
      "en\n",
      "panne\n",
      ".\n",
      "Jean\n",
      "suppose\n",
      "que\n",
      "lordinateur\n",
      "avec\n",
      "le\n",
      "programme\n",
      "de\n",
      "lexprience\n",
      "est\n",
      "en\n",
      "panne\n",
      ".\n",
      "Jean\n",
      "suppose\n",
      "que\n",
      "les\n",
      "ordinateurs\n",
      "avec\n",
      "le\n",
      "programme\n",
      "de\n",
      "lexprience\n",
      "sont\n",
      "en\n",
      "panne\n",
      ".\n",
      "Jean\n",
      "suppose\n",
      "que\n",
      "lordinateur\n",
      "avec\n",
      "les\n",
      "programmes\n",
      "de\n",
      "lexprience\n",
      "est\n",
      "en\n",
      "panne\n",
      ".\n",
      "Jean\n",
      "suppose\n",
      "que\n",
      "les\n",
      "ordinateurs\n",
      "avec\n",
      "les\n",
      "programmes\n",
      "de\n",
      "lexprience\n",
      "sont\n",
      "en\n",
      "panne\n",
      ".\n",
      "Relative\n",
      "clauseLordinateur\n",
      "avec\n",
      "le\n",
      "programme\n",
      "do\n",
      "nt\n",
      "Jean\n",
      "se\n",
      "servait\n",
      "est\n",
      "en\n",
      "panne\n",
      ".\n",
      "Les\n",
      "ordinateurs\n",
      "avec\n",
      "le\n",
      "programme\n",
      "do\n",
      "nt\n",
      "Jean\n",
      "se\n",
      "servait\n",
      "sont\n",
      "en\n",
      "panne\n",
      ".\n",
      "Lordinateur\n",
      "avec\n",
      "les\n",
      "programmes\n",
      "do\n",
      "nt\n",
      "Jean\n",
      "se\n",
      "servait\n",
      "est\n",
      "en\n",
      "panne\n",
      ".\n",
      "Les\n",
      "ordinateurs\n",
      "avec\n",
      "les\n",
      "programmes\n",
      "do\n",
      "nt\n",
      "Jean\n",
      "se\n",
      "servait\n",
      "sont\n",
      "en\n",
      "panne\n",
      ".\n",
      "Lordinateur\n",
      "avec\n",
      "le\n",
      "programme\n",
      "de\n",
      "lexprience\n",
      "do\n",
      "nt\n",
      "Jean\n",
      "se\n",
      "servait\n",
      "est\n",
      "en\n",
      "panne\n",
      ".\n",
      "Les\n",
      "ordinateurs\n",
      "avec\n",
      "le\n",
      "programme\n",
      "de\n",
      "lexprience\n",
      "do\n",
      "nt\n",
      "Jean\n",
      "se\n",
      "servait\n",
      "sont\n",
      "en\n",
      "panne\n",
      ".\n",
      "Lordinateur\n",
      "avec\n",
      "les\n",
      "programmes\n",
      "de\n",
      "lexprience\n",
      "do\n",
      "nt\n",
      "Jean\n",
      "se\n",
      "servait\n",
      "est\n",
      "en\n",
      "panne\n",
      ".\n",
      "Les\n",
      "ordinateurs\n",
      "avec\n",
      "les\n",
      "programmes\n",
      "de\n",
      "lexprience\n",
      "do\n",
      "nt\n",
      "Jean\n",
      "se\n",
      "servait\n",
      "sont\n",
      "en\n",
      "panne\n",
      ".\n",
      "Answer\n",
      "set\n",
      "for\n",
      "problem\n",
      "constructed\n",
      "from\n",
      "lines\n",
      "1\n",
      "-\n",
      "7\n",
      "of\n",
      "the\n",
      "main\n",
      "clause\n",
      "sequenceLordinateur\n",
      "avec\n",
      "le\n",
      "programme\n",
      "et\n",
      "lexperince\n",
      "est\n",
      "en\n",
      "panne\n",
      ".\n",
      "N2\n",
      "coord\n",
      "N3\n",
      "2Les\n",
      "ordinateurs\n",
      "avec\n",
      "les\n",
      "programmes\n",
      "de\n",
      "lexperince\n",
      "sont\n",
      "en\n",
      "panne\n",
      ".\n",
      "correctLordinateur\n",
      "avec\n",
      "le\n",
      "programme\n",
      "est\n",
      "en\n",
      "panne\n",
      ".\n",
      "wrong\n",
      "number\n",
      "of\n",
      "attractorsLordinateur\n",
      "avec\n",
      "le\n",
      "program\n",
      "de\n",
      "lexperince\n",
      "sont\n",
      "en\n",
      "panne\n",
      ".\n",
      "agreement\n",
      "errorLes\n",
      "ordinateurs\n",
      "avec\n",
      "les\n",
      "programmes\n",
      "de\n",
      "lexperince\n",
      "sont\n",
      "en\n",
      "panne\n",
      ".\n",
      "wrong\n",
      "nr\n",
      ".\n",
      "for\n",
      "1stattractor\n",
      "nounLes\n",
      "ordinateurs\n",
      "avec\n",
      "les\n",
      "programmes\n",
      "de\n",
      "les\n",
      "experinces\n",
      "sont\n",
      "en\n",
      "panne\n",
      ".\n",
      "wrong\n",
      "nr\n",
      ".\n",
      "for\n",
      "2ndattractor\n",
      "noun\n",
      "And\n",
      "candidate\n",
      "answer\n",
      "set\n",
      "for\n",
      "a\n",
      "problem\n",
      "constructed\n",
      "from\n",
      "lines\n",
      "1\n",
      "-\n",
      "7\n",
      "of\n",
      "the\n",
      "main\n",
      "clause\n",
      "sequence\n",
      ".\n",
      "14\n",
      "A.3\n",
      "Example\n",
      "of\n",
      "data\n",
      "for\n",
      "the\n",
      "verb\n",
      "alternation\n",
      "BLM\n",
      "TYPE\n",
      "IEXAMPLE\n",
      "OF\n",
      "CONTEXT\n",
      "The\n",
      "buyer\n",
      "can\n",
      "load\n",
      "the\n",
      "tools\n",
      "in\n",
      "bags\n",
      ".\n",
      "The\n",
      "tools\n",
      "were\n",
      "loaded\n",
      "by\n",
      "the\n",
      "buyer\n",
      "The\n",
      "tools\n",
      "were\n",
      "loaded\n",
      "in\n",
      "bags\n",
      "by\n",
      "the\n",
      "buyer\n",
      "The\n",
      "tools\n",
      "were\n",
      "loaded\n",
      "in\n",
      "bags\n",
      "Bags\n",
      "were\n",
      "loaded\n",
      "by\n",
      "the\n",
      "buyer\n",
      "Bags\n",
      "were\n",
      "loaded\n",
      "with\n",
      "the\n",
      "tools\n",
      "by\n",
      "the\n",
      "buyer\n",
      "Bags\n",
      "were\n",
      "loaded\n",
      "with\n",
      "the\n",
      "tools\n",
      "?\n",
      "?\n",
      "?\n",
      "EXAMPLE\n",
      "OF\n",
      "ANSWERS\n",
      "The\n",
      "buyer\n",
      "can\n",
      "load\n",
      "bags\n",
      "with\n",
      "the\n",
      "tools\n",
      "The\n",
      "buyer\n",
      "was\n",
      "loaded\n",
      "bags\n",
      "with\n",
      "the\n",
      "tools\n",
      "The\n",
      "buyer\n",
      "can\n",
      "load\n",
      "bags\n",
      "the\n",
      "tools\n",
      "The\n",
      "buyer\n",
      "can\n",
      "load\n",
      "in\n",
      "bags\n",
      "with\n",
      "the\n",
      "tools\n",
      "The\n",
      "buyer\n",
      "can\n",
      "load\n",
      "bags\n",
      "on\n",
      "sale\n",
      "The\n",
      "buyer\n",
      "can\n",
      "load\n",
      "bags\n",
      "under\n",
      "the\n",
      "tools\n",
      "Bags\n",
      "can\n",
      "load\n",
      "the\n",
      "buyer\n",
      "with\n",
      "the\n",
      "tools\n",
      "The\n",
      "tools\n",
      "can\n",
      "load\n",
      "the\n",
      "buyer\n",
      "in\n",
      "bags\n",
      "Bags\n",
      "can\n",
      "load\n",
      "the\n",
      "tools\n",
      "in\n",
      "the\n",
      "buyer\n",
      "A.4\n",
      "Experimental\n",
      "details\n",
      "All\n",
      "systems\n",
      "used\n",
      "a\n",
      "learning\n",
      "rate\n",
      "of\n",
      "0.001\n",
      "and\n",
      "Adam\n",
      "optimizer\n",
      ",\n",
      "and\n",
      "batch\n",
      "size\n",
      "100\n",
      ".\n",
      "The\n",
      "system\n",
      "was\n",
      "trained\n",
      "for\n",
      "300\n",
      "epochs\n",
      "for\n",
      "all\n",
      "experiments\n",
      ".\n",
      "The\n",
      "experiments\n",
      "were\n",
      "run\n",
      "on\n",
      "an\n",
      "HP\n",
      "PAIR\n",
      "Workstation\n",
      "Z4\n",
      "G4\n",
      "MT\n",
      ",\n",
      "with\n",
      "an\n",
      "Intel\n",
      "Xeon\n",
      "W-2255\n",
      "processor\n",
      ",\n",
      "64\n",
      "G\n",
      "RAM\n",
      ",\n",
      "and\n",
      "a\n",
      "MSI\n",
      "GeForce\n",
      "RTX\n",
      "3090\n",
      "VENTUS\n",
      "3X\n",
      "OC\n",
      "24\n",
      "G\n",
      "GDDR6X\n",
      "GPU\n",
      ".\n",
      "Thesentence\n",
      "-\n",
      "level\n",
      "encoder\n",
      "decoder\n",
      "has\n",
      "106\n",
      "603\n",
      "parameters\n",
      ".\n",
      "It\n",
      "consists\n",
      "of\n",
      "an\n",
      "encoder\n",
      "with\n",
      "a\n",
      "CNN\n",
      "layer\n",
      "followed\n",
      "by\n",
      "a\n",
      "FFNN\n",
      "layer\n",
      ".\n",
      "The\n",
      "CNN\n",
      "input\n",
      "has\n",
      "shape\n",
      "32x24\n",
      ".\n",
      "We\n",
      "use\n",
      "a\n",
      "kernel\n",
      "size\n",
      "15x15\n",
      "with\n",
      "stride\n",
      "1x1\n",
      ",\n",
      "and\n",
      "40\n",
      "channels\n",
      ".\n",
      "The\n",
      "linearized\n",
      "CNN\n",
      "output\n",
      "has\n",
      "240\n",
      "units\n",
      ",\n",
      "which\n",
      "the\n",
      "FFNN\n",
      "compresses\n",
      "into\n",
      "the\n",
      "latent\n",
      "layer\n",
      "of\n",
      "size\n",
      "55\n",
      "meanstd\n",
      ".\n",
      "The\n",
      "decoder\n",
      "is\n",
      "a\n",
      "mirror\n",
      "of\n",
      "the\n",
      "encoder\n",
      ",\n",
      "which\n",
      "expands\n",
      "a\n",
      "sampled\n",
      "latent\n",
      "of\n",
      "sizeinto\n",
      "a\n",
      "32x24\n",
      "representation\n",
      ".\n",
      "The\n",
      "two\n",
      "-\n",
      "level\n",
      "system\n",
      "consists\n",
      "of\n",
      "the\n",
      "sentence\n",
      "level\n",
      "encoder\n",
      "-\n",
      "decoder\n",
      "described\n",
      "above\n",
      ",\n",
      "and\n",
      "a\n",
      "taskspecific\n",
      "layer\n",
      ".\n",
      "The\n",
      "input\n",
      "to\n",
      "the\n",
      "task\n",
      "layer\n",
      "is\n",
      "a\n",
      "7x5\n",
      "input\n",
      "sequence\n",
      "ofsentences\n",
      ",\n",
      "whose\n",
      "representation\n",
      "we\n",
      "obtain\n",
      "from\n",
      "the\n",
      "latent\n",
      "of\n",
      "the\n",
      "sentence\n",
      "level\n",
      ",\n",
      "which\n",
      "is\n",
      "compressed\n",
      "using\n",
      "a\n",
      "CNN\n",
      "with\n",
      "kernel\n",
      "4x4\n",
      "and\n",
      "stride\n",
      "1x1\n",
      "and\n",
      "32\n",
      "channels\n",
      "into\n",
      "...\n",
      "units\n",
      ",\n",
      "which\n",
      "are\n",
      "compressed\n",
      "using\n",
      "a\n",
      "FFNN\n",
      "layer\n",
      "into\n",
      "a\n",
      "latent\n",
      "layer\n",
      "of\n",
      "size\n",
      "55\n",
      "meanstd\n",
      ".\n",
      "The\n",
      "decoder\n",
      "consists\n",
      "of\n",
      "a\n",
      "FFNN\n",
      "which\n",
      "expands\n",
      "the\n",
      "sampled\n",
      "latent\n",
      "of\n",
      "sizeinto\n",
      "7200\n",
      "units\n",
      ",\n",
      "which\n",
      "are\n",
      "then\n",
      "processed\n",
      "through\n",
      "a\n",
      "CNN\n",
      "with\n",
      "kernel\n",
      "size\n",
      "15x15\n",
      "and\n",
      "stride\n",
      "1x1\n",
      ",\n",
      "and\n",
      "produces\n",
      "a\n",
      "sentence\n",
      "embedding\n",
      "of\n",
      "size\n",
      "32x24\n",
      ".\n",
      "The\n",
      "two\n",
      "level\n",
      "system\n",
      "has\n",
      "178\n",
      "126\n",
      "parameters\n",
      ".\n",
      "15\n",
      "A.5\n",
      "Sentence\n",
      "-\n",
      "level\n",
      "analysis\n",
      "A.5.1\n",
      "Sample\n",
      "confusion\n",
      "matrices\n",
      "for\n",
      "altered\n",
      "latent\n",
      "values\n",
      "A.5.2\n",
      "Sentence\n",
      "-\n",
      "level\n",
      "analysis\n",
      "for\n",
      "English\n",
      "data\n",
      "16\n",
      "A.6\n",
      "Detailed\n",
      "task\n",
      "results\n",
      "TRAIN\n",
      "ON\n",
      "TEST\n",
      "ON\n",
      "VAELEVEL\n",
      "VAE\n",
      "BLM\n",
      "agreement\n",
      "typeI\n",
      "typeI\n",
      "0.9290.935\n",
      "0.0049\n",
      "typeI\n",
      "typeII\n",
      "0.8990.908\n",
      "0.0059\n",
      "typeI\n",
      "typeIII\n",
      "0.6620.871\n",
      "0.0092\n",
      "typeII\n",
      "typeI\n",
      "0.948\n",
      "e-10\n",
      "0.974\n",
      "0.0049\n",
      "typeII\n",
      "typeII\n",
      "0.879\n",
      "e-10\n",
      "0.904\n",
      "0.0021\n",
      "typeII\n",
      "typeIII\n",
      "0.7130.891\n",
      "0.0015\n",
      "typeIII\n",
      "typeI\n",
      "0.851\n",
      "0.037\n",
      "0.611\n",
      "0.1268\n",
      "typeIII\n",
      "typeII\n",
      "0.815\n",
      "0.0308\n",
      "0.620\n",
      "0.1304\n",
      "typeIII\n",
      "typeIII\n",
      "0.779\n",
      "0.0285\n",
      "0.602\n",
      "0.1195\n",
      "BLM\n",
      "verb\n",
      "alternation\n",
      "grouptypeI\n",
      "typeI\n",
      "0.9890.995\n",
      "e-10\n",
      "typeI\n",
      "typeII\n",
      "0.9070.912\n",
      "0.0141\n",
      "typeI\n",
      "typeIII\n",
      "0.8090.804\n",
      "0.0167\n",
      "typeII\n",
      "typeI\n",
      "0.9890.996\n",
      "0.0013\n",
      "typeII\n",
      "typeII\n",
      "0.979\n",
      "e-10\n",
      "0.984\n",
      "0.0016\n",
      "typeII\n",
      "typeIII\n",
      "0.9150.928\n",
      "0.0178\n",
      "typeIII\n",
      "typeI\n",
      "0.9970.999\n",
      "0.0013\n",
      "typeIII\n",
      "typeII\n",
      "0.9770.986\n",
      "0.0027\n",
      "typeIII\n",
      "typeIII\n",
      "0.980.989\n",
      "0.0003\n",
      "BLM\n",
      "verb\n",
      "alternation\n",
      "grouptypeI\n",
      "typeI\n",
      "0.9920.987\n",
      "0.0033\n",
      "typeI\n",
      "typeII\n",
      "0.9110.931\n",
      "0.0065\n",
      "typeI\n",
      "typeIII\n",
      "0.8470.869\n",
      "0.0102\n",
      "typeII\n",
      "typeI\n",
      "0.9970.993\n",
      "0.0025\n",
      "typeII\n",
      "typeII\n",
      "0.978\n",
      "e-10\n",
      "0.978\n",
      "0.0017\n",
      "typeII\n",
      "typeIII\n",
      "0.9230.956\n",
      "0.0023\n",
      "typeIII\n",
      "typeI\n",
      "0.979\n",
      "e-10\n",
      "0.981\n",
      "0.0022\n",
      "typeIII\n",
      "typeII\n",
      "0.9720.975\n",
      "0.0005\n",
      "typeIII\n",
      "typeIII\n",
      "0.9670.977\n",
      "0.0022\n",
      "The\n",
      "highest\n",
      "value\n",
      "for\n",
      "each\n",
      "traintest\n",
      "combination\n",
      "highlighted\n",
      "in\n",
      "bold\n",
      ".\n",
      "17\n",
      "A.7\n",
      "Detailed\n",
      "error\n",
      "results\n",
      "N1alter\n",
      "and\n",
      "N2alter\n",
      "are\n",
      "sequence\n",
      "errors\n",
      ".\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "for i in docs:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number_pattern = [{\"TEXT\": {\"REGEX\": r\"^\\d.\"}}]\n",
    "foot_note_pattern = [{\"TEXT\": {\"REGEX\": r\"^[\\d*]+\\w.*$\"}}]\n",
    "figure_pattern = [{\"TEXT\": {\"REGEX\": r\"(?i)\\btable\\s+\\d+[:.]*\\s*.*$\"}}] #'\\b(Table|TABLE)\\s*\\d+[\\s\\S]*?\\.\n",
    "email_pattern = [{\"TEXT\": {\"REGEX\": r\"^[\\w\\.-]+[a-zA-Z0-9\\.-]+\\.[a-zA-Z]{2,}$\"}}]\n",
    "url_pattern = [{\"TEXT\": {\"REGEX\": r\"https?[\\w\\.-]+\\.[\\w\\.-]+\"}}]\n",
    "\n",
    "matcher =  Matcher(nlp.vocab)\n",
    "#matcher.add(\"NUMBER_PATTERN\", [number_pattern])\n",
    "matcher.add(\"URL_PATTERN\",[url_pattern])\n",
    "matcher.add(\"EMAIL_PATTERN\",[email_pattern])\n",
    "matcher.add(\"FIGURE_PATTERN\", [figure_pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched pattern: vivi.a.nastasegmail.com\n",
      "27 28\n",
      "Matched pattern: Paola.Merlounige.ch\n",
      "29 30\n",
      "Matched pattern: Geneva.principal\n",
      "338 339\n",
      "Matched pattern: 2023.Data\n",
      "1890 1891\n",
      "Matched pattern: 174371404.John\n",
      "5783 5784\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "matches = matcher(docs)\n",
    "for match_id, start, end in matches:\n",
    "    span = docs[start:end]\n",
    "    print(f\"Matched pattern: {span.text}\")\n",
    "    print(start,end)\n",
    "print(len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sent.text.strip() for sent in docs.sents]\n",
    "valid_sentences = [sentences for sentence in sentences if len(sentence) > 15] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_indexes = []\n",
    "dot_pattern = [{\"TEXT\": \".\"}]\n",
    "match1 = Matcher(nlp.vocab)\n",
    "match1.add(\"DOT_PATTERN\",[dot_pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "Matched pattern: .\n",
      "[45, 85, 145, 173, 200, 209, 212, 228, 271, 274, 315, 372, 421, 424, 445, 494, 508, 516, 562, 669, 719, 739, 748, 764, 799, 802, 836, 848, 851, 867, 875, 895, 901, 910, 930, 936, 939, 953, 973, 976, 1002, 1005, 1045, 1071, 1078, 1084, 1120, 1126, 1136, 1139, 1144, 1181, 1203, 1215, 1226, 1229, 1264, 1296, 1299, 1318, 1321, 1325, 1344, 1357, 1372, 1396, 1400, 1406, 1415, 1421, 1436, 1473, 1510, 1888, 1927, 1982, 2003, 2021, 2055, 2098, 2155, 2183, 2201, 2225, 2246, 2304, 2329, 2371, 2400, 2431, 2437, 2453, 2456, 2477, 2486, 2522, 2542, 2552, 2565, 2616, 2627, 2645, 2655, 2673, 2694, 2702, 2742, 2795, 2819, 2824, 2858, 2884, 2923, 2944, 3005, 3031, 3050, 3066, 3077, 3110, 3173, 3187, 3269, 3276, 3334, 3366, 3397, 3435, 3488, 3535, 3553, 3623, 3631, 3634, 3666, 3699, 3710, 3733, 3785, 3819, 3842, 3865, 3891, 3920, 3980, 4002, 4005, 4030, 4070, 4080, 4109, 4116, 4150, 4162, 4216, 4240, 4284, 4297, 4355, 4382, 4388, 4434, 4463, 4473, 4527, 4543, 4586, 4643, 4677, 4799, 4846, 4896, 4908, 4927, 4962, 4980, 5013, 5050, 5063, 5121, 5200, 5243, 5280, 5311, 5351, 5383, 5422, 5462, 5499, 5526, 5562, 5595, 5623, 5636, 5640, 5642, 5651, 5690, 5692, 5708, 5733, 5738, 5765, 5767, 5776, 5797, 5802, 5805, 5807, 5816, 5821, 5841, 5843, 5850, 5870, 5875, 5885, 5887, 5905, 5910, 5923, 5925, 5939, 5972, 5977, 5989, 5991, 5999, 6018, 6023, 6037, 6039, 6051, 6074, 6079, 6090, 6097, 6099, 6109, 6112, 6116, 6118, 6127, 6159, 6164, 6170, 6172, 6182, 6205, 6210, 6216, 6218, 6227, 6246, 6252, 6268, 6270, 6286, 6309, 6314, 6326, 6328, 6349, 6365, 6370, 6385, 6387, 6399, 6403, 6418, 6420, 6432, 6465, 6470, 6483, 6485, 6493, 6504, 6513, 6515, 6517, 6532, 6544, 6546, 6556, 6589, 6594, 6609, 6611, 6624, 6639, 6641, 6651, 6670, 6675, 6685, 6687, 6718, 6723, 6729, 6731, 6735, 6739, 6758, 6760, 6771, 6775, 6806, 6808, 6816, 6820, 6823, 6825, 6859, 6864, 6870, 6872, 6883, 6903, 6908, 6911, 6913, 6934, 6938, 6941, 6947, 6949, 6961, 6975, 7002, 7007, 7036, 7066, 7087, 7103, 7127, 7135, 7146, 7201, 7209, 7217, 7225, 7236, 7247, 7258, 7271, 7282, 7293, 7304, 7331, 7392, 7423, 7434, 7447, 7461, 7474, 7486, 7547, 7586, 7595, 7604, 7612, 7621, 7631, 7642, 7652, 7663, 7675, 7687, 7698, 7710, 7723, 7737, 7750, 7764, 7778, 7792, 7805, 7819, 7834, 7850, 7865, 7881, 7905, 7919, 7927, 7940, 7952, 7955, 7969, 7972, 7993, 8012, 8035, 8175, 8186, 8224, 8234, 8249, 8256, 8270, 8291, 8311, 8332, 8392, 8432, 8441, 8622, 8633, 8645]\n"
     ]
    }
   ],
   "source": [
    "m = match1(docs)\n",
    "for match_id, start, end in m:\n",
    "    span = docs[start:end]\n",
    "    dot_indexes.append(start)\n",
    "    print(f\"Matched pattern: {span.text}\")\n",
    "print(dot_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched pattern: 1Idiap\n",
      "2 3\n",
      "Paola Merlo1,2 1Idiap Research Institute, Martigny, Switzerland 2University of Geneva, Swizerland vivi.a.nastasegmail.com,\n",
      "Matched pattern: 2University\n",
      "9 10\n",
      "Paola Merlo1,2 1Idiap Research Institute, Martigny, Switzerland 2University of Geneva, Swizerland vivi.a.nastasegmail.com,\n",
      "Matched pattern: 2020\n",
      "28 29\n",
      "Sentence embeddings usually fine-tuned have proven useful for a variety of high-level language processing tasks e.g. the GLUE tasks Clark et al., 2020, story continuation Ippolito et al., 2020.\n",
      "Matched pattern: 2020\n",
      "37 38\n",
      "Sentence embeddings usually fine-tuned have proven useful for a variety of high-level language processing tasks e.g. the GLUE tasks Clark et al., 2020, story continuation Ippolito et al., 2020.\n",
      "Matched pattern: 2018\n",
      "44 45\n",
      "Sentence embeddings built using a BiLSTM model do seem to encode a range of information, from shallow e.g. sentence length, word order to syntactic e.g. tree depth, top constituent and semantic e.g. tense, semantic mismatches Conneau et al., 2018.\n",
      "Matched pattern: 2023c\n",
      "55 56\n",
      "An investigation of the dimensions of BERT sentence embeddings using The work was done while the authors were at the University of Geneva.principal component analysis indicated that there is much correlation and redundancy, and that they encode more shallow information length, rather than morphological, syntactic or semantic features Nikolaev and Pad, 2023c.\n",
      "Matched pattern: 2020\n",
      "50 51\n",
      "Moreover, analysis of information propagation through the model layers, and analysis of the sentence embeddings seem to show that much specialized information e.g. POS, syntactic structure while quite apparent at lower levels, gets lost towards the highest levels of the models Rogers et al., 2020.\n",
      "Matched pattern: 2Throughout\n",
      "82 83\n",
      "Understanding what kind of information the sentence embeddings encode, and how, has multiple benefits it connects internal changes in the model parameters and structure with changes in its outputs it contributes to verifying the robustness of models and whether or not they rely on shallow or accidental regularities in the data it narrows down the field of search when a language model produces wrong outputs, and it helps maximize the use of training data for developing more robust models 2Throughout this paper, by \"layer\" we mean \"a stratum of information\", not the layers of a transformer architecture.\n",
      "Matched pattern: 2020\n",
      "10 11\n",
      "Tracing information through a transformer Rogers et al., 2020 have shown that from the unstructured textual input, BERT Devlin et al., 2019 is able to infer POS, structural, entity-related, syntactic and semantic information at successively higher layers of the architecture, mirroring the classical NLP pipeline Tenney et al., 2019a.\n",
      "Matched pattern: 2019\n",
      "26 27\n",
      "Tracing information through a transformer Rogers et al., 2020 have shown that from the unstructured textual input, BERT Devlin et al., 2019 is able to infer POS, structural, entity-related, syntactic and semantic information at successively higher layers of the architecture, mirroring the classical NLP pipeline Tenney et al., 2019a.\n",
      "Matched pattern: 2019a\n",
      "61 62\n",
      "Tracing information through a transformer Rogers et al., 2020 have shown that from the unstructured textual input, BERT Devlin et al., 2019 is able to infer POS, structural, entity-related, syntactic and semantic information at successively higher layers of the architecture, mirroring the classical NLP pipeline Tenney et al., 2019a.\n",
      "Matched pattern: 2020\n",
      "35 36\n",
      "Further studies have shown that the information is not sharply separated, information from higher level can influence information at lower levels, such as POS in multilingual models de Vries et al., 2020, or subject-verb agreement Jawahar et al., 2019.\n",
      "Matched pattern: 2019\n",
      "47 48\n",
      "Further studies have shown that the information is not sharply separated, information from higher level can influence information at lower levels, such as POS in multilingual models de Vries et al., 2020, or subject-verb agreement Jawahar et al., 2019.\n",
      "Matched pattern: 2022\n",
      "5 6\n",
      "Niu et al., 2022 Nikolaev and Pad, 2023c.\n",
      "Matched pattern: 2023c\n",
      "10 11\n",
      "Niu et al., 2022 Nikolaev and Pad, 2023c.\n",
      "Matched pattern: 2020\n",
      "21 22\n",
      "Attention is part of the process, as it helps encode various types of linguistic information Rogers et al., 2020\n",
      "Matched pattern: 2019\n",
      "5 6\n",
      "Clark et al., 2019, syntactic dependencies Htut et al., 2019, grammatical structure Luo, 2021, and can contribute towards semantic role labeling Tan et al., 2018\n",
      "Matched pattern: 2019\n",
      "14 15\n",
      "Clark et al., 2019, syntactic dependencies Htut et al., 2019, grammatical structure Luo, 2021, and can contribute towards semantic role labeling Tan et al., 2018\n",
      "Matched pattern: 2021\n",
      "20 21\n",
      "Clark et al., 2019, syntactic dependencies Htut et al., 2019, grammatical structure Luo, 2021, and can contribute towards semantic role labeling Tan et al., 2018\n",
      "Matched pattern: 2018\n",
      "34 35\n",
      "Clark et al., 2019, syntactic dependencies Htut et al., 2019, grammatical structure Luo, 2021, and can contribute towards semantic role labeling Tan et al., 2018\n",
      "Matched pattern: 2018\n",
      "5 6\n",
      "Strubell et al., 2018.\n",
      "Matched pattern: 2019b\n",
      "15 16\n",
      "Word embeddings were shown to encode sentence-level information Tenney et al., 2019b, including syntactic structure Hewitt and Manning, 2019, even in multilingual models Chi et al., 2020.\n",
      "Matched pattern: 2019\n",
      "24 25\n",
      "Word embeddings were shown to encode sentence-level information Tenney et al., 2019b, including syntactic structure Hewitt and Manning, 2019, even in multilingual models Chi et al., 2020.\n",
      "Matched pattern: 2020\n",
      "35 36\n",
      "Word embeddings were shown to encode sentence-level information Tenney et al., 2019b, including syntactic structure Hewitt and Manning, 2019, even in multilingual models Chi et al., 2020.\n",
      "Matched pattern: 2022\n",
      "13 14\n",
      "Predicate embeddings contain information about its semantic roles structure Conia and Navigli, 2022, embeddings of nouns encode subjecthood and objecthood Papadimitriou et al., 2021.\n",
      "Matched pattern: 2021\n",
      "27 28\n",
      "Predicate embeddings contain information about its semantic roles structure Conia and Navigli, 2022, embeddings of nouns encode subjecthood and objecthood Papadimitriou et al., 2021.\n",
      "Matched pattern: 2023a\n",
      "16 17\n",
      "The averaged token embeddings are more commonly used as sentence embeddings e.g. Nikolaev and Pad, 2023a, or the special token CLS s embeddings are fine-tuned for specific tasks such as story continuation Ippolito et al., 2020, sentence similarity Reimers and Gurevych, 2019, alignment to semantic features 3We will share the code and sentence data upon acceptance.\n",
      "Matched pattern: 2020\n",
      "41 42\n",
      "The averaged token embeddings are more commonly used as sentence embeddings e.g. Nikolaev and Pad, 2023a, or the special token CLS s embeddings are fine-tuned for specific tasks such as story continuation Ippolito et al., 2020, sentence similarity Reimers and Gurevych, 2019, alignment to semantic features 3We will share the code and sentence data upon acceptance.\n",
      "Matched pattern: 2019\n",
      "49 50\n",
      "The averaged token embeddings are more commonly used as sentence embeddings e.g. Nikolaev and Pad, 2023a, or the special token CLS s embeddings are fine-tuned for specific tasks such as story continuation Ippolito et al., 2020, sentence similarity Reimers and Gurevych, 2019, alignment to semantic features 3We will share the code and sentence data upon acceptance.\n",
      "Matched pattern: 3We\n",
      "55 56\n",
      "The averaged token embeddings are more commonly used as sentence embeddings e.g. Nikolaev and Pad, 2023a, or the special token CLS s embeddings are fine-tuned for specific tasks such as story continuation Ippolito et al., 2020, sentence similarity Reimers and Gurevych, 2019, alignment to semantic features 3We will share the code and sentence data upon acceptance.\n",
      "Matched pattern: 2022\n",
      "4 5\n",
      "Opitz and Frank, 2022.\n",
      "Matched pattern: 2018\n",
      "37 38\n",
      "This token averaging is justifiable as the learning signal for transformer models is stronger at the token level, with a much weaker objective at the sentence level e.g. next sentence prediction Devlin et al., 2018 Liu et al., 2019, sentence order prediction Lan et al., 2019.\n",
      "Matched pattern: 2019\n",
      "43 44\n",
      "This token averaging is justifiable as the learning signal for transformer models is stronger at the token level, with a much weaker objective at the sentence level e.g. next sentence prediction Devlin et al., 2018 Liu et al., 2019, sentence order prediction Lan et al., 2019.\n",
      "Matched pattern: 2019\n",
      "53 54\n",
      "This token averaging is justifiable as the learning signal for transformer models is stronger at the token level, with a much weaker objective at the sentence level e.g. next sentence prediction Devlin et al., 2018 Liu et al., 2019, sentence order prediction Lan et al., 2019.\n",
      "Matched pattern: 2020\n",
      "6 7\n",
      "Electra Clark et al., 2020 does not either, but it relies on replaced token detection, which uses the sentence context to determine whether a number of tokens in the given sentence were replaced by a generator sample.\n",
      "Matched pattern: 2018\n",
      "23 24\n",
      "This training regime leads to sentence embeddings that perform well on the General Language Understanding Evaluation GLUE benchmark Wang et al., 2018 and Stanford Question Answering SQuAD dataset Rajpurkar et al., 2016, or detecting verb classes Yi et al., 2022.\n",
      "Matched pattern: 2016\n",
      "35 36\n",
      "This training regime leads to sentence embeddings that perform well on the General Language Understanding Evaluation GLUE benchmark Wang et al., 2018 and Stanford Question Answering SQuAD dataset Rajpurkar et al., 2016, or detecting verb classes Yi et al., 2022.\n",
      "Matched pattern: 2022\n",
      "46 47\n",
      "This training regime leads to sentence embeddings that perform well on the General Language Understanding Evaluation GLUE benchmark Wang et al., 2018 and Stanford Question Answering SQuAD dataset Rajpurkar et al., 2016, or detecting verb classes Yi et al., 2022.\n",
      "Matched pattern: 2023c\n",
      "13 14\n",
      "Raw sentence embeddings also seemed to capture shallower information Nikolaev and Pad, 2023c, but Nastase and Merlo 2023 show that raw sentence embeddings have internal structure that can encode grammatical sentence properties.\n",
      "Matched pattern: 2023\n",
      "19 20\n",
      "Raw sentence embeddings also seemed to capture shallower information Nikolaev and Pad, 2023c, but Nastase and Merlo 2023 show that raw sentence embeddings have internal structure that can encode grammatical sentence properties.\n",
      "Matched pattern: 2022\n",
      "15 16\n",
      "Probing models Analysis of BERTs inner workings has been done using probing classifiers Belinkov, 2022, or through clustering based on the representations at the different levels Jawahar et al., 2019.\n",
      "Matched pattern: 2019\n",
      "33 34\n",
      "Probing models Analysis of BERTs inner workings has been done using probing classifiers Belinkov, 2022, or through clustering based on the representations at the different levels Jawahar et al., 2019.\n",
      "Matched pattern: 2018\n",
      "20 21\n",
      "Probing has also been used to investigate the representations obtained from a pretrained transformer model Conneau et al., 2018.\n",
      "Matched pattern: 2021\n",
      "4 5\n",
      "Elazar et al. 2021 propose amnesic probing to test both whether some information is encoded, and whether it is used.\n",
      "Matched pattern: 2013\n",
      "8 9\n",
      "VAE-based methods Kingma and Welling, 2013 Bowman et al., 2016 have been used to detect or separate specific information from input representations.\n",
      "Matched pattern: 2016\n",
      "14 15\n",
      "VAE-based methods Kingma and Welling, 2013 Bowman et al., 2016 have been used to detect or separate specific information from input representations.\n",
      "Matched pattern: 2021\n",
      "3 4\n",
      "Mercatali and Freitas 2021 capture discrete properties of sentences encoded with an LSTM e.g. number and aspect of verbs on the latent layer.\n",
      "Matched pattern: 2019\n",
      "4 5\n",
      "Bao et al. 2019 and Chen et al. 2019 learn to disentangle syntactic and semantic information.\n",
      "Matched pattern: 2019\n",
      "10 11\n",
      "Bao et al. 2019 and Chen et al. 2019 learn to disentangle syntactic and semantic information.\n",
      "Matched pattern: 2023\n",
      "6 7\n",
      "Silva De Carvalho et al. 2023 learn to disentangle the semantic roles in natural language definitions from word embeddings.\n",
      "Matched pattern: 2023\n",
      "38 39\n",
      "Data For probing transfomers embeddings and behaviour, most approaches use datasets built by selecting, or constructing, sentences that exhibit specific structure and properties definition sentences with annotated roles Silva De Carvalho et al., 2023, sentences built according to a given template Nikolaev and Pad, 2023b, sentences with specific structures for investigating different tasks, in particular SentEval Conneau and Kiela, 2018 Jawahar et al., 2019, example sentences from 2 BLM agreement problem CONTEXT TEMPLATE NP-sg PP1-sg VP-sg NP-pl PP1-sg VP-pl NP-sg PP1-pl VP-sg NP-pl PP1-pl VP-pl NP-sg PP1-sg PP2-sg VP-sg NP-pl PP1-sg PP2-sg VP-pl NP-sg PP1-pl PP2-sg VP-sg ANSWER SET NP-sg PP1-sg et NP2 VP-sg Coord NP-pl PP1-pl NP2-sg VP-pl correct NP-sg PP1-sg VP-sg WNA NP-pl PP1-pl NP2-pl VP-sg AEV NP-pl PP1-sg NP2-pl VP-sg AEN1 NP-pl PP1-pl NP2-sg VP-sg AEN2 NP-pl PP1-sg PP1-sg VP-pl WN1 NP-pl PP1-pl PP2-pl VP-pl WN2BLM verb alternation problem CONTEXT TEMPLATE NP-Agent Verb NP-Loc PP-Theme NP-Theme VerbPass PP-Agent NP-Theme VerbPass PP-Loc PP-Agent NP-Theme VerbPass PP-Loc NP-Loc VerbPass PP-Agent NP-Loc VerbPass PP-Theme PP-Agent NP-Loc VerbPass PP-Theme ANSWER SET NP-Agent Verb NP-Theme PP-Loc CORRECT NP-Agent VerbPass NP-Theme PP-Loc AGENT ACT NP-Agent Verb NP-Theme NP-Loc ALT1 NP-Agent Verb PP-Theme PP-Loc ALT2 NP-Agent Verb NP-Theme PP-Loc NOEMB NP-Agent Verb NP-Theme PP-Loc LEXPREP NP-Theme Verb NP-Agent PP-Loc SSM1 NP-Loc Verb NP-Agent PP-Theme SSM2 NP-Theme Verb NP-Loc PP-Agent AASSM FrameNet Conia and Navigli, 2022, a dataset with multi-level structure inspired by the Raven Progressive Matrices visual intelligence tests An et al.,\n",
      "Matched pattern: 2023b\n",
      "51 52\n",
      "Data For probing transfomers embeddings and behaviour, most approaches use datasets built by selecting, or constructing, sentences that exhibit specific structure and properties definition sentences with annotated roles Silva De Carvalho et al., 2023, sentences built according to a given template Nikolaev and Pad, 2023b, sentences with specific structures for investigating different tasks, in particular SentEval Conneau and Kiela, 2018 Jawahar et al., 2019, example sentences from 2 BLM agreement problem CONTEXT TEMPLATE NP-sg PP1-sg VP-sg NP-pl PP1-sg VP-pl NP-sg PP1-pl VP-sg NP-pl PP1-pl VP-pl NP-sg PP1-sg PP2-sg VP-sg NP-pl PP1-sg PP2-sg VP-pl NP-sg PP1-pl PP2-sg VP-sg ANSWER SET NP-sg PP1-sg et NP2 VP-sg Coord NP-pl PP1-pl NP2-sg VP-pl correct NP-sg PP1-sg VP-sg WNA NP-pl PP1-pl NP2-pl VP-sg AEV NP-pl PP1-sg NP2-pl VP-sg AEN1 NP-pl PP1-pl NP2-sg VP-sg AEN2 NP-pl PP1-sg PP1-sg VP-pl WN1 NP-pl PP1-pl PP2-pl VP-pl WN2BLM verb alternation problem CONTEXT TEMPLATE NP-Agent Verb NP-Loc PP-Theme NP-Theme VerbPass PP-Agent NP-Theme VerbPass PP-Loc PP-Agent NP-Theme VerbPass PP-Loc NP-Loc VerbPass PP-Agent NP-Loc VerbPass PP-Theme PP-Agent NP-Loc VerbPass PP-Theme ANSWER SET NP-Agent Verb NP-Theme PP-Loc CORRECT NP-Agent VerbPass NP-Theme PP-Loc AGENT ACT NP-Agent Verb NP-Theme NP-Loc ALT1 NP-Agent Verb PP-Theme PP-Loc ALT2 NP-Agent Verb NP-Theme PP-Loc NOEMB NP-Agent Verb NP-Theme PP-Loc LEXPREP NP-Theme Verb NP-Agent PP-Loc SSM1 NP-Loc Verb NP-Agent PP-Theme SSM2 NP-Theme Verb NP-Loc PP-Agent AASSM FrameNet Conia and Navigli, 2022, a dataset with multi-level structure inspired by the Raven Progressive Matrices visual intelligence tests An et al.,\n",
      "Matched pattern: 2018\n",
      "69 70\n",
      "Data For probing transfomers embeddings and behaviour, most approaches use datasets built by selecting, or constructing, sentences that exhibit specific structure and properties definition sentences with annotated roles Silva De Carvalho et al., 2023, sentences built according to a given template Nikolaev and Pad, 2023b, sentences with specific structures for investigating different tasks, in particular SentEval Conneau and Kiela, 2018 Jawahar et al., 2019, example sentences from 2 BLM agreement problem CONTEXT TEMPLATE NP-sg PP1-sg VP-sg NP-pl PP1-sg VP-pl NP-sg PP1-pl VP-sg NP-pl PP1-pl VP-pl NP-sg PP1-sg PP2-sg VP-sg NP-pl PP1-sg PP2-sg VP-pl NP-sg PP1-pl PP2-sg VP-sg ANSWER SET NP-sg PP1-sg et NP2 VP-sg Coord NP-pl PP1-pl NP2-sg VP-pl correct NP-sg PP1-sg VP-sg WNA NP-pl PP1-pl NP2-pl VP-sg AEV NP-pl PP1-sg NP2-pl VP-sg AEN1 NP-pl PP1-pl NP2-sg VP-sg AEN2 NP-pl PP1-sg PP1-sg VP-pl WN1 NP-pl PP1-pl PP2-pl VP-pl WN2BLM verb alternation problem CONTEXT TEMPLATE NP-Agent Verb NP-Loc PP-Theme NP-Theme VerbPass PP-Agent NP-Theme VerbPass PP-Loc PP-Agent NP-Theme VerbPass PP-Loc NP-Loc VerbPass PP-Agent NP-Loc VerbPass PP-Theme PP-Agent NP-Loc VerbPass PP-Theme ANSWER SET NP-Agent Verb NP-Theme PP-Loc CORRECT NP-Agent VerbPass NP-Theme PP-Loc AGENT ACT NP-Agent Verb NP-Theme NP-Loc ALT1 NP-Agent Verb PP-Theme PP-Loc ALT2 NP-Agent Verb NP-Theme PP-Loc NOEMB NP-Agent Verb NP-Theme PP-Loc LEXPREP NP-Theme Verb NP-Agent PP-Loc SSM1 NP-Loc Verb NP-Agent PP-Theme SSM2 NP-Theme Verb NP-Loc PP-Agent AASSM FrameNet Conia and Navigli, 2022, a dataset with multi-level structure inspired by the Raven Progressive Matrices visual intelligence tests An et al.,\n",
      "Matched pattern: 2019\n",
      "75 76\n",
      "Data For probing transfomers embeddings and behaviour, most approaches use datasets built by selecting, or constructing, sentences that exhibit specific structure and properties definition sentences with annotated roles Silva De Carvalho et al., 2023, sentences built according to a given template Nikolaev and Pad, 2023b, sentences with specific structures for investigating different tasks, in particular SentEval Conneau and Kiela, 2018 Jawahar et al., 2019, example sentences from 2 BLM agreement problem CONTEXT TEMPLATE NP-sg PP1-sg VP-sg NP-pl PP1-sg VP-pl NP-sg PP1-pl VP-sg NP-pl PP1-pl VP-pl NP-sg PP1-sg PP2-sg VP-sg NP-pl PP1-sg PP2-sg VP-pl NP-sg PP1-pl PP2-sg VP-sg ANSWER SET NP-sg PP1-sg et NP2 VP-sg Coord NP-pl PP1-pl NP2-sg VP-pl correct NP-sg PP1-sg VP-sg WNA NP-pl PP1-pl NP2-pl VP-sg AEV NP-pl PP1-sg NP2-pl VP-sg AEN1 NP-pl PP1-pl NP2-sg VP-sg AEN2 NP-pl PP1-sg PP1-sg VP-pl WN1 NP-pl PP1-pl PP2-pl VP-pl WN2BLM verb alternation problem CONTEXT TEMPLATE NP-Agent Verb NP-Loc PP-Theme NP-Theme VerbPass PP-Agent NP-Theme VerbPass PP-Loc PP-Agent NP-Theme VerbPass PP-Loc NP-Loc VerbPass PP-Agent NP-Loc VerbPass PP-Theme PP-Agent NP-Loc VerbPass PP-Theme ANSWER SET NP-Agent Verb NP-Theme PP-Loc CORRECT NP-Agent VerbPass NP-Theme PP-Loc AGENT ACT NP-Agent Verb NP-Theme NP-Loc ALT1 NP-Agent Verb PP-Theme PP-Loc ALT2 NP-Agent Verb NP-Theme PP-Loc NOEMB NP-Agent Verb NP-Theme PP-Loc LEXPREP NP-Theme Verb NP-Agent PP-Loc SSM1 NP-Loc Verb NP-Agent PP-Theme SSM2 NP-Theme Verb NP-Loc PP-Agent AASSM FrameNet Conia and Navigli, 2022, a dataset with multi-level structure inspired by the Raven Progressive Matrices visual intelligence tests An et al.,\n",
      "Matched pattern: 2022\n",
      "430 431\n",
      "Data For probing transfomers embeddings and behaviour, most approaches use datasets built by selecting, or constructing, sentences that exhibit specific structure and properties definition sentences with annotated roles Silva De Carvalho et al., 2023, sentences built according to a given template Nikolaev and Pad, 2023b, sentences with specific structures for investigating different tasks, in particular SentEval Conneau and Kiela, 2018 Jawahar et al., 2019, example sentences from 2 BLM agreement problem CONTEXT TEMPLATE NP-sg PP1-sg VP-sg NP-pl PP1-sg VP-pl NP-sg PP1-pl VP-sg NP-pl PP1-pl VP-pl NP-sg PP1-sg PP2-sg VP-sg NP-pl PP1-sg PP2-sg VP-pl NP-sg PP1-pl PP2-sg VP-sg ANSWER SET NP-sg PP1-sg et NP2 VP-sg Coord NP-pl PP1-pl NP2-sg VP-pl correct NP-sg PP1-sg VP-sg WNA NP-pl PP1-pl NP2-pl VP-sg AEV NP-pl PP1-sg NP2-pl VP-sg AEN1 NP-pl PP1-pl NP2-sg VP-sg AEN2 NP-pl PP1-sg PP1-sg VP-pl WN1 NP-pl PP1-pl PP2-pl VP-pl WN2BLM verb alternation problem CONTEXT TEMPLATE NP-Agent Verb NP-Loc PP-Theme NP-Theme VerbPass PP-Agent NP-Theme VerbPass PP-Loc PP-Agent NP-Theme VerbPass PP-Loc NP-Loc VerbPass PP-Agent NP-Loc VerbPass PP-Theme PP-Agent NP-Loc VerbPass PP-Theme ANSWER SET NP-Agent Verb NP-Theme PP-Loc CORRECT NP-Agent VerbPass NP-Theme PP-Loc AGENT ACT NP-Agent Verb NP-Theme NP-Loc ALT1 NP-Agent Verb PP-Theme PP-Loc ALT2 NP-Agent Verb NP-Theme PP-Loc NOEMB NP-Agent Verb NP-Theme PP-Loc LEXPREP NP-Theme Verb NP-Agent PP-Loc SSM1 NP-Loc Verb NP-Agent PP-Theme SSM2 NP-Theme Verb NP-Loc PP-Agent AASSM FrameNet Conia and Navigli, 2022, a dataset with multi-level structure inspired by the Raven Progressive Matrices visual intelligence tests An et al.,\n",
      "Matched pattern: 2023.Data\n",
      "0 1\n",
      "2023.Data Our main object of investigation are chunks, sequence of adjacent words that segment a sentence as defined initially in Abney, 1992, Collins, 1997 and then Tjong Kim Sang and Buchholz, 2000.\n",
      "Matched pattern: 1992\n",
      "23 24\n",
      "2023.Data Our main object of investigation are chunks, sequence of adjacent words that segment a sentence as defined initially in Abney, 1992, Collins, 1997 and then Tjong Kim Sang and Buchholz, 2000.\n",
      "Matched pattern: 1997\n",
      "27 28\n",
      "2023.Data Our main object of investigation are chunks, sequence of adjacent words that segment a sentence as defined initially in Abney, 1992, Collins, 1997 and then Tjong Kim Sang and Buchholz, 2000.\n",
      "Matched pattern: 2000\n",
      "36 37\n",
      "2023.Data Our main object of investigation are chunks, sequence of adjacent words that segment a sentence as defined initially in Abney, 1992, Collins, 1997 and then Tjong Kim Sang and Buchholz, 2000.\n",
      "Matched pattern: 3.1\n",
      "29 30\n",
      "To investigate whether chunks and their properties are identifiable in sentence embeddings, we use two types of data i sentences with known chunk pattern, described in Section 3.1 ii two datasets with multi-level structure built for linguistic intelligence tests for language models Merlo, 2023, described in Section 3.2.\n",
      "Matched pattern: 2023\n",
      "48 49\n",
      "To investigate whether chunks and their properties are identifiable in sentence embeddings, we use two types of data i sentences with known chunk pattern, described in Section 3.1 ii two datasets with multi-level structure built for linguistic intelligence tests for language models Merlo, 2023, described in Section 3.2.\n",
      "Matched pattern: 3.2\n",
      "53 54\n",
      "To investigate whether chunks and their properties are identifiable in sentence embeddings, we use two types of data i sentences with known chunk pattern, described in Section 3.1 ii two datasets with multi-level structure built for linguistic intelligence tests for language models Merlo, 2023, described in Section 3.2.\n",
      "Matched pattern: 3.1\n",
      "0 1\n",
      "3.1 Sentences Sentences are built from a seed file containing noun, verb and prepositional phrases, including singularplural variations.\n",
      "Matched pattern: 1pp2\n",
      "15 16\n",
      "From these chunks, we built sentences with all grammatically correct combinations of np pp 1pp2 vp4.\n",
      "Matched pattern: 14\n",
      "6 7\n",
      "For each chunk pattern pof the 14 possibilities for instance, p \"np-s pp1-s vp-s\", all corresponding sentences are collected into a set Sp.\n",
      "Matched pattern: 4We\n",
      "4 5\n",
      "outareNnegs incorrect outputs, 4We use BNF notation pp\n",
      "Matched pattern: 1and\n",
      "0 1\n",
      "1and pp 2may be included or not, pp 2may be included only if pp1 is includedrandomly chosen from the sentences that have a chunk pattern different from s. The algorithm for building the data and a sample line and generated sentences are shown in appendix A.1.\n",
      "Matched pattern: 2may\n",
      "2 3\n",
      "1and pp 2may be included or not, pp 2may be included only if pp1 is includedrandomly chosen from the sentences that have a chunk pattern different from s. The algorithm for building the data and a sample line and generated sentences are shown in appendix A.1.\n",
      "Matched pattern: 2may\n",
      "9 10\n",
      "1and pp 2may be included or not, pp 2may be included only if pp1 is includedrandomly chosen from the sentences that have a chunk pattern different from s. The algorithm for building the data and a sample line and generated sentences are shown in appendix A.1.\n",
      "Matched pattern: 4000\n",
      "19 20\n",
      "From the generated instances, we sample uniformly, based on the pattern of the input sentence, approximately 4000 instances, randomly split 8020 into traintest.\n",
      "Matched pattern: 8020\n",
      "24 25\n",
      "From the generated instances, we sample uniformly, based on the pattern of the input sentence, approximately 4000 instances, randomly split 8020 into traintest.\n",
      "Matched pattern: 8020\n",
      "6 7\n",
      "The train part is further split 8020 into traindev, resulting in a 2576630798 split for traindevtest.\n",
      "Matched pattern: 2576630798\n",
      "13 14\n",
      "The train part is further split 8020 into traindev, resulting in a 2576630798 split for traindevtest.\n",
      "Matched pattern: 3.2\n",
      "0 1\n",
      "3.2 Blackbird Language Matrices Blackbird Language Matrices BLMs Merlo, 2023 are language versions of the visual Raven Progressive Matrices.\n",
      "Matched pattern: 2023\n",
      "10 11\n",
      "3.2 Blackbird Language Matrices Blackbird Language Matrices BLMs Merlo, 2023 are language versions of the visual Raven Progressive Matrices.\n",
      "Matched pattern: 2000252\n",
      "10 11\n",
      "Verb alternations ALT-ATL ATL-ALT Type I 2000252 2000375 2000375 Type II 20004866 20001500 20001500 Type III 20004869 20001500 20001500 TableTrainTest statistics for the two BLM problems.\n",
      "Matched pattern: 2000375\n",
      "11 12\n",
      "Verb alternations ALT-ATL ATL-ALT Type I 2000252 2000375 2000375 Type II 20004866 20001500 20001500 Type III 20004869 20001500 20001500 TableTrainTest statistics for the two BLM problems.\n",
      "Matched pattern: 2000375\n",
      "12 13\n",
      "Verb alternations ALT-ATL ATL-ALT Type I 2000252 2000375 2000375 Type II 20004866 20001500 20001500 Type III 20004869 20001500 20001500 TableTrainTest statistics for the two BLM problems.\n",
      "Matched pattern: 20004866\n",
      "15 16\n",
      "Verb alternations ALT-ATL ATL-ALT Type I 2000252 2000375 2000375 Type II 20004866 20001500 20001500 Type III 20004869 20001500 20001500 TableTrainTest statistics for the two BLM problems.\n",
      "Matched pattern: 20001500\n",
      "16 17\n",
      "Verb alternations ALT-ATL ATL-ALT Type I 2000252 2000375 2000375 Type II 20004866 20001500 20001500 Type III 20004869 20001500 20001500 TableTrainTest statistics for the two BLM problems.\n",
      "Matched pattern: 20001500\n",
      "17 18\n",
      "Verb alternations ALT-ATL ATL-ALT Type I 2000252 2000375 2000375 Type II 20004866 20001500 20001500 Type III 20004869 20001500 20001500 TableTrainTest statistics for the two BLM problems.\n",
      "Matched pattern: 20004869\n",
      "20 21\n",
      "Verb alternations ALT-ATL ATL-ALT Type I 2000252 2000375 2000375 Type II 20004866 20001500 20001500 Type III 20004869 20001500 20001500 TableTrainTest statistics for the two BLM problems.\n",
      "Matched pattern: 20001500\n",
      "21 22\n",
      "Verb alternations ALT-ATL ATL-ALT Type I 2000252 2000375 2000375 Type II 20004866 20001500 20001500 Type III 20004869 20001500 20001500 TableTrainTest statistics for the two BLM problems.\n",
      "Matched pattern: 20001500\n",
      "22 23\n",
      "Verb alternations ALT-ATL ATL-ALT Type I 2000252 2000375 2000375 Type II 20004866 20001500 20001500 Type III 20004869 20001500 20001500 TableTrainTest statistics for the two BLM problems.\n",
      "Matched pattern: 2023\n",
      "7 8\n",
      "in French An et al., 2023, and ii BLM-slE verb alternations in English Samo et al., 2023.\n",
      "Matched pattern: 2023\n",
      "23 24\n",
      "in French An et al., 2023, and ii BLM-slE verb alternations in English Samo et al., 2023.\n",
      "Matched pattern: 2023\n",
      "14 15\n",
      "We built a variation of the BLM-AgrF An et al., 2023 that separates clearly sequence-based errors WN1 and WN2 in the agreement scheme presented in We include erroneous answers that have correct agreement, but do not respect the pattern of the sequence, to be able to contrast linguistic errors from errors in identifying sentence parts.\n",
      "Matched pattern: 9010\n",
      "4 5\n",
      "After splitting each subset 9010 into traintest subsets, we randomly sample 2000 instances as train data.\n",
      "Matched pattern: 2000\n",
      "12 13\n",
      "After splitting each subset 9010 into traintest subsets, we randomly sample 2000 instances as train data.\n",
      "Matched pattern: 20\n",
      "0 1\n",
      "20 of the train data is used for development.\n",
      "Matched pattern: 4.1\n",
      "37 38\n",
      "First, using sentences and a VAE-based system, we test whether we can compress sentences into a smaller representation on the latent layer that captures information about the chunk structure of the sentence Section 4.1 below.\n",
      "Matched pattern: 4.2\n",
      "50 51\n",
      "Second, to see if the chunks thus identified are being used in a separate task, we combine the compression of the sentence representation with the BLM problems, where a crucial part of the solution lies in identifying the structures ofsentences and their sequence in the input Section 4.2 below.\n",
      "Matched pattern: 2020\n",
      "25 26\n",
      "As sentence representations, we use the embeddings of the s character read from the last layer of the Electra Clark et al., 2020 pretrained model5.\n",
      "Matched pattern: 4.1\n",
      "0 1\n",
      "4.1 Parts in sentences We test whether sentence embeddings contain information about the chunk structure of the corresponding sentences by compressing them into a lower dimensional representation in a VAE-like system.\n",
      "Matched pattern: 4.1.1\n",
      "0 1\n",
      "4.1.1 Experimental set-up The architecture of the sentence-level VAE is similar to a previously proposed system Nastase and Merlo, 2023.\n",
      "Matched pattern: 2023\n",
      "24 25\n",
      "4.1.1 Experimental set-up The architecture of the sentence-level VAE is similar to a previously proposed system Nastase and Merlo, 2023.\n",
      "Matched pattern: 15x15\n",
      "9 10\n",
      "The encoder consists of a CNN layer with a 15x15 kernel, which is applied to a 32x24shaped sentence embedding,6followed by a linear layer that compresses the output of the CNN into a latent layer of size 5.\n",
      "Matched pattern: 32x24shaped\n",
      "17 18\n",
      "The encoder consists of a CNN layer with a 15x15 kernel, which is applied to a 32x24shaped sentence embedding,6followed by a linear layer that compresses the output of the CNN into a latent layer of size 5.\n",
      "Matched pattern: 32x24\n",
      "17 18\n",
      "The decoder is a mirrorimage of the encoder, and unpacks a sampled latent vector into a 32x24 sentence representation.\n",
      "Matched pattern: 5Electra\n",
      "13 14\n",
      "At prediction time, the sentence from theout outoptions that has the highest 5Electra pretrained model googleelectra-basediscriminator 6Nastase and Merlo 2023 show that task-relevant information is more easily accessible in transformer-based sentence embeddings reshaped as two-dimensional arrays, which indicates that patterns are encoded periodically, and are best detected with a 15x15 kernel.\n",
      "Matched pattern: 6Nastase\n",
      "19 20\n",
      "At prediction time, the sentence from theout outoptions that has the highest 5Electra pretrained model googleelectra-basediscriminator 6Nastase and Merlo 2023 show that task-relevant information is more easily accessible in transformer-based sentence embeddings reshaped as two-dimensional arrays, which indicates that patterns are encoded periodically, and are best detected with a 15x15 kernel.\n",
      "Matched pattern: 2023\n",
      "22 23\n",
      "At prediction time, the sentence from theout outoptions that has the highest 5Electra pretrained model googleelectra-basediscriminator 6Nastase and Merlo 2023 show that task-relevant information is more easily accessible in transformer-based sentence embeddings reshaped as two-dimensional arrays, which indicates that patterns are encoded periodically, and are best detected with a 15x15 kernel.\n",
      "Matched pattern: 15x15\n",
      "60 61\n",
      "At prediction time, the sentence from theout outoptions that has the highest 5Electra pretrained model googleelectra-basediscriminator 6Nastase and Merlo 2023 show that task-relevant information is more easily accessible in transformer-based sentence embeddings reshaped as two-dimensional arrays, which indicates that patterns are encoded periodically, and are best detected with a 15x15 kernel.\n",
      "Matched pattern: 4.1.2\n",
      "0 1\n",
      "4.1.2 Analysis To assess whether the correct patterns of chunks are detected in sentences, we analyze the results for the experiments described in the previous section in two ways i analyze the output of the system, in terms of average F1 score over three runs and confusion matrices ii analyze the latent layer, to determine whether chunk patterns are encoded in the latent vectors for instance, latent vectors cluster according to the pattern of their corresponding sentences.\n",
      "Matched pattern: 0.9992\n",
      "43 44\n",
      "If we consider the multiple choice task as a binary task Has the system built a sentence representation that is closest to the correct answer?, the system achieves an average positive class F1 score and standard deviation over three runs of 0.9992 0.01 for the French dataset, and 0.997 0.0035 for the English dataset.\n",
      "Matched pattern: 0.01\n",
      "44 45\n",
      "If we consider the multiple choice task as a binary task Has the system built a sentence representation that is closest to the correct answer?, the system achieves an average positive class F1 score and standard deviation over three runs of 0.9992 0.01 for the French dataset, and 0.997 0.0035 for the English dataset.\n",
      "Matched pattern: 0.997\n",
      "51 52\n",
      "If we consider the multiple choice task as a binary task Has the system built a sentence representation that is closest to the correct answer?, the system achieves an average positive class F1 score and standard deviation over three runs of 0.9992 0.01 for the French dataset, and 0.997 0.0035 for the English dataset.\n",
      "Matched pattern: 0.0035\n",
      "52 53\n",
      "If we consider the multiple choice task as a binary task Has the system built a sentence representation that is closest to the correct answer?, the system achieves an average positive class F1 score and standard deviation over three runs of 0.9992 0.01 for the French dataset, and 0.997 0.0035 for the English dataset.\n",
      "Matched pattern: 4.1.3\n",
      "8 9\n",
      "Changes to latent unitsandlead to the matricesandin the 4.1.3 Electra vs. BERT and RoBERTa, and the price of fine-tuning We have chosen Electra for the investigations presented here, because with its use of the full context of the sentence to determine whether tokens in the input have been replaced by a generator model, it provides a stronger supervision signal than BERT Devlin et al., 2019 and RoBERTa Liu et al., 2019.\n",
      "Matched pattern: 2019\n",
      "71 72\n",
      "Changes to latent unitsandlead to the matricesandin the 4.1.3 Electra vs. BERT and RoBERTa, and the price of fine-tuning We have chosen Electra for the investigations presented here, because with its use of the full context of the sentence to determine whether tokens in the input have been replaced by a generator model, it provides a stronger supervision signal than BERT Devlin et al., 2019 and RoBERTa Liu et al., 2019.\n",
      "Matched pattern: 2019\n",
      "79 80\n",
      "Changes to latent unitsandlead to the matricesandin the 4.1.3 Electra vs. BERT and RoBERTa, and the price of fine-tuning We have chosen Electra for the investigations presented here, because with its use of the full context of the sentence to determine whether tokens in the input have been replaced by a generator model, it provides a stronger supervision signal than BERT Devlin et al., 2019 and RoBERTa Liu et al., 2019.\n",
      "Matched pattern: 0.91\n",
      "24 25\n",
      "In terms of F1 score on the task of reconstructing a sentence with the same chunk structure, BERT has a mean overruns of 0.91 std0.0346, while RoBERTA has 0.8926 std0.0166.\n",
      "Matched pattern: 0.8926\n",
      "30 31\n",
      "In terms of F1 score on the task of reconstructing a sentence with the same chunk structure, BERT has a mean overruns of 0.91 std0.0346, while RoBERTA has 0.8926 std0.0166.\n",
      "Matched pattern: 0.43\n",
      "16 17\n",
      "We tested two sentence transformer models LaBSE and MPNet9, and obtained an F1 mean of 0.43 std0.0336 and 0.669 std0.0407 respectively.\n",
      "Matched pattern: 0.669\n",
      "19 20\n",
      "We tested two sentence transformer models LaBSE and MPNet9, and obtained an F1 mean of 0.43 std0.0336 and 0.669 std0.0407 respectively.\n",
      "Matched pattern: 768\n",
      "44 45\n",
      "We chose LaBSE and MPNet because they are two differently tuned models LaBSE is trained with bilingual sentence pairs with high results on a cross-language sentence retrieval task, MPNet is optimized for sentence similarity and their representations have the same dimensionality 768 as the transformer models we used.\n",
      "Matched pattern: 4.2\n",
      "0 1\n",
      "4.2 Parts in sentences for BLM tasks The first experiment shows that compressing sentence representations results in latent vectors containing chunk information.\n",
      "Matched pattern: 4.2.1\n",
      "0 1\n",
      "4.2.1 Experimental set-up\n",
      "Matched pattern: 1990\n",
      "23 24\n",
      "It has also been shown that being able to solve RPMs requires being able to handle item novelty Carpenter et al., 1990.\n",
      "Matched pattern: 7httpshuggingface.cogoogle\n",
      "2 3\n",
      "We model 7httpshuggingface.cogoogle-bert bert-base-multilingual-cased 8httpshuggingface.coFacebookAI xlm-roberta-base 9httpshuggingface.co sentence-transformersLaBSE,httpshuggingface.\n",
      "Matched pattern: 8httpshuggingface.coFacebookAI\n",
      "12 13\n",
      "We model 7httpshuggingface.cogoogle-bert bert-base-multilingual-cased 8httpshuggingface.coFacebookAI xlm-roberta-base 9httpshuggingface.co sentence-transformersLaBSE,httpshuggingface.\n",
      "Matched pattern: 9httpshuggingface.co\n",
      "18 19\n",
      "We model 7httpshuggingface.cogoogle-bert bert-base-multilingual-cased 8httpshuggingface.coFacebookAI xlm-roberta-base 9httpshuggingface.co sentence-transformersLaBSE,httpshuggingface.\n",
      "Matched pattern: 1,7\n",
      "15 16\n",
      "An instance for a BLM problem consists of an ordered sequence Sof sentences, Ssii 1,7 as input, and an answer set Awith one correct answerac, and several incorrect answers aerr.\n",
      "Matched pattern: 4.1\n",
      "27 28\n",
      "Themaxmargin and the scoring of the reconstructed sentence at the sentence level, and the constructed answer at the task level are computed as described in Section 4.1.\n",
      "Matched pattern: 4.2.2\n",
      "0 1\n",
      "4.2.2 Analysis\n",
      "Matched pattern: 4.1\n",
      "34 35\n",
      "First, from the latent representation analysis, we note that while the sentence representations on the latent layer are not as crisply separated by their chunk pattern as for the experiment in Section 4.1, there is a clear separation in terms of the grammatical number of the subject and the verb.\n",
      "Matched pattern: 3.2\n",
      "109 110\n",
      "This result not only indicates that structure information is more easily detectable when lexical variation is less of a factor, but more importantly, that chunk information is separable from other types of information in the sentence embedding, as the patterns detecting it can be applied successfully for data with additional lexical variation.10 Further confirmation of the fact that the sentence level learns to compress sentences into a latent that captures structural information comes from the error analysis, shown in the bottom panel of Lower rate of sequence errors, which are correct from the point of view of the targeted phenomenon as described in section 3.2 indicate that there is structure information in the compressed sentence latents.\n",
      "Matched pattern: 10It\n",
      "11 12\n",
      "It is possible that the one-level VAE also detects 10It might appear surprising that the two-level approach leads to lower performance on type III data, particularly when lexical variation had not been an issue for the sentence representation analysis Section 4.1.\n",
      "Matched pattern: 4.1\n",
      "45 46\n",
      "It is possible that the one-level VAE also detects 10It might appear surprising that the two-level approach leads to lower performance on type III data, particularly when lexical variation had not been an issue for the sentence representation analysis Section 4.1.\n",
      "Matched pattern: 4.3\n",
      "0 1\n",
      "4.3 Discussion We performed two types of experiments i using individual sentences, and an indirect supervision signal about the sentence structure, ii incorporating a sentence representation compression step in a task-specific setting.\n",
      "Matched pattern: 1992\n",
      "4 5\n",
      "References Steven Abney. 1992.\n",
      "Matched pattern: 23\n",
      "17 18\n",
      "In Speech and Natural Language Proceedings of a Workshop Held at Harriman, New York, February 23-26, 1992 .Aixiu\n",
      "Matched pattern: 26\n",
      "19 20\n",
      "In Speech and Natural Language Proceedings of a Workshop Held at Harriman, New York, February 23-26, 1992 .Aixiu\n",
      "Matched pattern: 1992\n",
      "21 22\n",
      "In Speech and Natural Language Proceedings of a Workshop Held at Harriman, New York, February 23-26, 1992 .Aixiu\n",
      "Matched pattern: 2023\n",
      "16 17\n",
      "An, Chunyang Jiang, Maria A. Rodriguez, Vivi Nastase, and Paola Merlo. 2023.\n",
      "Matched pattern: 17th\n",
      "4 5\n",
      "In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics , pages 1363 1374, Dubrovnik, Croatia.\n",
      "Matched pattern: 1363\n",
      "18 19\n",
      "In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics , pages 1363 1374, Dubrovnik, Croatia.\n",
      "Matched pattern: 1374\n",
      "19 20\n",
      "In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics , pages 1363 1374, Dubrovnik, Croatia.\n",
      "Matched pattern: 2019\n",
      "27 28\n",
      "Yu Bao, Hao Zhou, Shujian Huang, Lei Li, Lili Mou, Olga Vechtomova, Xin-yu Dai, and Jiajun Chen. 2019.\n",
      "Matched pattern: 57th\n",
      "4 5\n",
      "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 60086019, Florence, Italy.\n",
      "Matched pattern: 60086019\n",
      "15 16\n",
      "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 60086019, Florence, Italy.\n",
      "Matched pattern: 2022\n",
      "3 4\n",
      "Yonatan Belinkov. 2022.\n",
      "Matched pattern: 481207219\n",
      "3 4\n",
      "Computational Linguistics, 481207219.\n",
      "Matched pattern: 2016\n",
      "0 1\n",
      "2016.\n",
      "Matched pattern: 20th\n",
      "4 5\n",
      "In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning , pages 1021, Berlin, Germany.\n",
      "Matched pattern: 1021\n",
      "14 15\n",
      "In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning , pages 1021, Berlin, Germany.\n",
      "Matched pattern: 1990\n",
      "0 1\n",
      "1990.\n",
      "Matched pattern: 973404\n",
      "3 4\n",
      "Psychological review , 973404.\n",
      "Matched pattern: 2019\n",
      "13 14\n",
      "Mingda Chen, Qingming Tang, Sam Wiseman, and Kevin Gimpel. 2019.\n",
      "Matched pattern: 2019\n",
      "4 5\n",
      "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies, VolumeLong and Short Papers , pages 24532464, Minneapolis, Minnesota.\n",
      "Matched pattern: 24532464\n",
      "27 28\n",
      "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies, VolumeLong and Short Papers , pages 24532464, Minneapolis, Minnesota.\n",
      "Matched pattern: 2020\n",
      "12 13\n",
      "Ethan A. Chi, John Hewitt, and Christopher D. Manning. 2020.\n",
      "Matched pattern: 58th\n",
      "4 5\n",
      "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 55645577, Online.\n",
      "Matched pattern: 55645577\n",
      "15 16\n",
      "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 55645577, Online.\n",
      "Matched pattern: 2019\n",
      "0 1\n",
      "2019.\n",
      "Matched pattern: 2019\n",
      "4 5\n",
      "In Proceedings of the 2019 ACL Workshop BlackboxNLP Analyzing and Interpreting Neural Networks for NLP , pages 276286, Florence, Italy.\n",
      "Matched pattern: 276286\n",
      "17 18\n",
      "In Proceedings of the 2019 ACL Workshop BlackboxNLP Analyzing and Interpreting Neural Networks for NLP , pages 276286, Florence, Italy.\n",
      "Matched pattern: 2020\n",
      "0 1\n",
      "2020.\n",
      "Matched pattern: 1997\n",
      "0 1\n",
      "1997.\n",
      "Matched pattern: 35th\n",
      "1 2\n",
      "In 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics , pages 16 23, Madrid, Spain.\n",
      "Matched pattern: 8th\n",
      "11 12\n",
      "In 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics , pages 16 23, Madrid, Spain.\n",
      "Matched pattern: 16\n",
      "25 26\n",
      "In 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics , pages 16 23, Madrid, Spain.\n",
      "Matched pattern: 23\n",
      "26 27\n",
      "In 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics , pages 16 23, Madrid, Spain.\n",
      "Matched pattern: 2022\n",
      "0 1\n",
      "2022.\n",
      "Matched pattern: 60th\n",
      "4 5\n",
      "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics VolumeLong Papers , pages 46224632, Dublin, Ireland.\n",
      "Matched pattern: 46224632\n",
      "17 18\n",
      "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics VolumeLong Papers , pages 46224632, Dublin, Ireland.\n",
      "Matched pattern: 2018\n",
      "0 1\n",
      "2018.\n",
      "Matched pattern: 2018\n",
      "13 14\n",
      "In Proceedings of the Eleventh International Conference on Language Resources and Evaluation LREC 2018 , Miyazaki, Japan.\n",
      "Matched pattern: 2018\n",
      "16 17\n",
      "Alexis Conneau, German Kruszewski, Guillaume Lample, Loc Barrault, and Marco Baroni. 2018.\n",
      "Matched pattern: 56th\n",
      "4 5\n",
      "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics VolumeLong Papers , pages 21262136, Melbourne, Australia.\n",
      "Matched pattern: 21262136\n",
      "17 18\n",
      "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics VolumeLong Papers , pages 21262136, Melbourne, Australia.\n",
      "Matched pattern: 2020\n",
      "12 13\n",
      "Wietse de Vries, Andreas van Cranenburgh, and Malvina Nissim. 2020.\n",
      "Matched pattern: 2020\n",
      "9 10\n",
      "In Findings of the Association for Computational Linguistics EMNLP 2020 , pages 43394350, Online.\n",
      "Matched pattern: 43394350\n",
      "12 13\n",
      "In Findings of the Association for Computational Linguistics EMNLP 2020 , pages 43394350, Online.\n",
      "Matched pattern: 2018\n",
      "0 1\n",
      "2018.\n",
      "Matched pattern: 2019\n",
      "19 20\n",
      "CoRR , abs1810.04805. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\n",
      "Matched pattern: 2019\n",
      "4 5\n",
      "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies, VolumeLong and Short Papers , pages 41714186, Minneapolis, Minnesota.\n",
      "Matched pattern: 41714186\n",
      "27 28\n",
      "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies, VolumeLong and Short Papers , pages 41714186, Minneapolis, Minnesota.\n",
      "Matched pattern: 2021\n",
      "0 1\n",
      "2021.\n",
      "Matched pattern: 9160\n",
      "8 9\n",
      "Transactions of the Association for Computational Linguistics , 9160 175.\n",
      "Matched pattern: 175\n",
      "9 10\n",
      "Transactions of the Association for Computational Linguistics , 9160 175.\n",
      "Matched pattern: 2002\n",
      "2 3\n",
      "Nicol. 2002.\n",
      "Matched pattern: 174371404.John\n",
      "5 6\n",
      "Language and Cognitive Processes , 174371404.John Hewitt and Christopher D. Manning. 2019.\n",
      "Matched pattern: 2019\n",
      "12 13\n",
      "Language and Cognitive Processes , 174371404.John Hewitt and Christopher D. Manning. 2019.\n",
      "Matched pattern: 2019\n",
      "4 5\n",
      "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies, VolumeLong and Short Papers , pages 41294138, Minneapolis, Minnesota.\n",
      "Matched pattern: 41294138\n",
      "27 28\n",
      "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies, VolumeLong and Short Papers , pages 41294138, Minneapolis, Minnesota.\n",
      "Matched pattern: 2019\n",
      "0 1\n",
      "2019.\n",
      "Matched pattern: 2020\n",
      "0 1\n",
      "2020.\n",
      "Matched pattern: 58th\n",
      "4 5\n",
      "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 74727478, Online.\n",
      "Matched pattern: 74727478\n",
      "15 16\n",
      "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 74727478, Online.\n",
      "Matched pattern: 2019\n",
      "0 1\n",
      "2019.\n",
      "Matched pattern: 57th\n",
      "4 5\n",
      "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 36513657, Florence, Italy.\n",
      "Matched pattern: 36513657\n",
      "15 16\n",
      "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 36513657, Florence, Italy.\n",
      "Matched pattern: 2013\n",
      "6 7\n",
      "Diederik PKingma and Max Welling. 2013.\n",
      "Matched pattern: 2019\n",
      "0 1\n",
      "2019.\n",
      "Matched pattern: 2019\n",
      "31 32\n",
      "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.\n",
      "Matched pattern: 2021\n",
      "3 4\n",
      "Ziyang Luo. 2021.\n",
      "Matched pattern: 16th\n",
      "4 5\n",
      "In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics Student Research Workshop , pages 815, Online.\n",
      "Matched pattern: 815\n",
      "21 22\n",
      "In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics Student Research Workshop , pages 815, Online.\n",
      "Matched pattern: 2021\n",
      "0 1\n",
      "2021.\n",
      "Matched pattern: 2021\n",
      "9 10\n",
      "In Findings of the Association for Computational Linguistics EMNLP 2021 , pages 35473556, Punta Cana, Dominican Republic.\n",
      "Matched pattern: 35473556\n",
      "12 13\n",
      "In Findings of the Association for Computational Linguistics EMNLP 2021 , pages 35473556, Punta Cana, Dominican Republic.\n",
      "Matched pattern: 2023\n",
      "3 4\n",
      "Paola Merlo. 2023.\n",
      "Matched pattern: 2306.11444\n",
      "5 6\n",
      "ArXiv , cs.CL 2306.11444.\n",
      "Matched pattern: 2023\n",
      "6 7\n",
      "Vivi Nastase and Paola Merlo. 2023.\n",
      "Matched pattern: 10\n",
      "4 5\n",
      "In Proceedings of the 10  in large pre-trained language models.\n",
      "Matched pattern: 142152\n",
      "17 18\n",
      "In Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP , pages 142152, Abu Dhabi, United Arab Emirates Hybrid.\n",
      "Matched pattern: 12\n",
      "0 1\n",
      "12 A Appendix A.1 Sentence data To build the sentence data, we use a seed file that was used to generate the subject-verb agreement data.\n",
      "Matched pattern: 13\n",
      "44 45\n",
      "The algorithm to produce a dataset from the generated sentences is detailed in Data Nnegs forpatterns pdo forsiSpdo in si forsjSpdo outsj outsk, krange Nnegs , skSp Data Data in, out, out end for end for end for FigureData generation algorithm 13 A.2 Example of data for the agreement BLM Example subject NPs from Franck et al., 2002 Lordinateur avec le programme de lexperience The computer with the program of the experiments Manually expanded and completed sentences Lordinateur avec le programme de lexperience est en panne.\n",
      "Matched pattern: 2002\n",
      "62 63\n",
      "The algorithm to produce a dataset from the generated sentences is detailed in Data Nnegs forpatterns pdo forsiSpdo in si forsjSpdo outsj outsk, krange Nnegs , skSp Data Data in, out, out end for end for end for FigureData generation algorithm 13 A.2 Example of data for the agreement BLM Example subject NPs from Franck et al., 2002 Lordinateur avec le programme de lexperience The computer with the program of the experiments Manually expanded and completed sentences Lordinateur avec le programme de lexperience est en panne.\n",
      "Matched pattern: 2002\n",
      "62 63\n",
      "A seed for language matrix generation Jean suppose que lordinateur avec le programme de lexperience dont Jean se servait est en panne Jean thinks that the computer with the program of the experiment that John was using is down les ordinateurs avec les programmes sont en panne the computers with the programs are down TableExamples from Franck et al., 2002, manually completed and expanded sentences based on these examples, and seeds made based on these sentences for subject-verb agreement BLM dataset that contain all number variations for the nouns and the verb.\n",
      "Matched pattern: 2Les\n",
      "3 4\n",
      "N2 coord N3 2Les ordinateurs avec les programmes de lexperince sont en panne.\n",
      "Matched pattern: 1stattractor\n",
      "1 2\n",
      "for 1stattractor nounLes ordinateurs avec les programmes de les experinces sont en panne.\n",
      "Matched pattern: 2ndattractor\n",
      "1 2\n",
      "for 2ndattractor noun TableBLM instances for verb-subject agreement, withattractors programme, experince, and three clause structures.\n",
      "Matched pattern: 14\n",
      "0 1\n",
      "14 A.3 Example of data for the verb alternation BLM TYPE IEXAMPLE OF CONTEXT The buyer can load the tools in bags.\n",
      "Matched pattern: 0.001\n",
      "84 85\n",
      "EXAMPLE OF ANSWERS The buyer can load bags with the tools The buyer was loaded bags with the tools The buyer can load bags the tools The buyer can load in bags with the tools The buyer can load bags on sale The buyer can load bags under the tools Bags can load the buyer with the tools The tools can load the buyer in bags Bags can load the tools in the buyer A.4 Experimental details All systems used a learning rate of 0.001 and Adam optimizer, and batch size 100.\n",
      "Matched pattern: 100\n",
      "92 93\n",
      "EXAMPLE OF ANSWERS The buyer can load bags with the tools The buyer was loaded bags with the tools The buyer can load bags the tools The buyer can load in bags with the tools The buyer can load bags on sale The buyer can load bags under the tools Bags can load the buyer with the tools The tools can load the buyer in bags Bags can load the tools in the buyer A.4 Experimental details All systems used a learning rate of 0.001 and Adam optimizer, and batch size 100.\n",
      "Matched pattern: 300\n",
      "5 6\n",
      "The system was trained for 300 epochs for all experiments.\n",
      "Matched pattern: 64\n",
      "20 21\n",
      "The experiments were run on an HP PAIR Workstation Z4 G4 MT, with an Intel Xeon W-2255 processor, 64G RAM, and a MSI GeForce RTX 3090 VENTUS 3X OC 24G GDDR6X GPU.\n",
      "Matched pattern: 3090\n",
      "29 30\n",
      "The experiments were run on an HP PAIR Workstation Z4 G4 MT, with an Intel Xeon W-2255 processor, 64G RAM, and a MSI GeForce RTX 3090 VENTUS 3X OC 24G GDDR6X GPU.\n",
      "Matched pattern: 3X\n",
      "31 32\n",
      "The experiments were run on an HP PAIR Workstation Z4 G4 MT, with an Intel Xeon W-2255 processor, 64G RAM, and a MSI GeForce RTX 3090 VENTUS 3X OC 24G GDDR6X GPU.\n",
      "Matched pattern: 24\n",
      "33 34\n",
      "The experiments were run on an HP PAIR Workstation Z4 G4 MT, with an Intel Xeon W-2255 processor, 64G RAM, and a MSI GeForce RTX 3090 VENTUS 3X OC 24G GDDR6X GPU.\n",
      "Matched pattern: 106\n",
      "6 7\n",
      "Thesentence-level encoder decoder has 106 603 parameters.\n",
      "Matched pattern: 603\n",
      "7 8\n",
      "Thesentence-level encoder decoder has 106 603 parameters.\n",
      "Matched pattern: 32x24\n",
      "5 6\n",
      "The CNN input has shape 32x24.\n",
      "Matched pattern: 15x15\n",
      "5 6\n",
      "We use a kernel size 15x15 with stride 1x1, and 40 channels.\n",
      "Matched pattern: 1x1\n",
      "8 9\n",
      "We use a kernel size 15x15 with stride 1x1, and 40 channels.\n",
      "Matched pattern: 40\n",
      "11 12\n",
      "We use a kernel size 15x15 with stride 1x1, and 40 channels.\n",
      "Matched pattern: 240\n",
      "5 6\n",
      "The linearized CNN output has 240 units, which the FFNN compresses into the latent layer of size 55 meanstd.\n",
      "Matched pattern: 55\n",
      "18 19\n",
      "The linearized CNN output has 240 units, which the FFNN compresses into the latent layer of size 55 meanstd.\n",
      "Matched pattern: 32x24\n",
      "17 18\n",
      "The decoder is a mirror of the encoder, which expands a sampled latent of sizeinto a 32x24 representation.\n",
      "Matched pattern: 7x5\n",
      "8 9\n",
      "The input to the task layer is a 7x5 input sequence ofsentences, whose representation we obtain from the latent of the sentence level, which is compressed using a CNN with kernel 4x4 and stride 1x1 and 32 channels into ... units, which are compressed using a FFNN layer into a latent layer of size 55 meanstd.\n",
      "Matched pattern: 4x4\n",
      "33 34\n",
      "The input to the task layer is a 7x5 input sequence ofsentences, whose representation we obtain from the latent of the sentence level, which is compressed using a CNN with kernel 4x4 and stride 1x1 and 32 channels into ... units, which are compressed using a FFNN layer into a latent layer of size 55 meanstd.\n",
      "Matched pattern: 1x1\n",
      "36 37\n",
      "The input to the task layer is a 7x5 input sequence ofsentences, whose representation we obtain from the latent of the sentence level, which is compressed using a CNN with kernel 4x4 and stride 1x1 and 32 channels into ... units, which are compressed using a FFNN layer into a latent layer of size 55 meanstd.\n",
      "Matched pattern: 32\n",
      "38 39\n",
      "The input to the task layer is a 7x5 input sequence ofsentences, whose representation we obtain from the latent of the sentence level, which is compressed using a CNN with kernel 4x4 and stride 1x1 and 32 channels into ... units, which are compressed using a FFNN layer into a latent layer of size 55 meanstd.\n",
      "Matched pattern: 55\n",
      "57 58\n",
      "The input to the task layer is a 7x5 input sequence ofsentences, whose representation we obtain from the latent of the sentence level, which is compressed using a CNN with kernel 4x4 and stride 1x1 and 32 channels into ... units, which are compressed using a FFNN layer into a latent layer of size 55 meanstd.\n",
      "Matched pattern: 7200\n",
      "13 14\n",
      "The decoder consists of a FFNN which expands the sampled latent of sizeinto 7200 units, which are then processed through a CNN with kernel size 15x15 and stride 1x1, and produces a sentence embedding of size 32x24.\n",
      "Matched pattern: 15x15\n",
      "26 27\n",
      "The decoder consists of a FFNN which expands the sampled latent of sizeinto 7200 units, which are then processed through a CNN with kernel size 15x15 and stride 1x1, and produces a sentence embedding of size 32x24.\n",
      "Matched pattern: 1x1\n",
      "29 30\n",
      "The decoder consists of a FFNN which expands the sampled latent of sizeinto 7200 units, which are then processed through a CNN with kernel size 15x15 and stride 1x1, and produces a sentence embedding of size 32x24.\n",
      "Matched pattern: 32x24\n",
      "38 39\n",
      "The decoder consists of a FFNN which expands the sampled latent of sizeinto 7200 units, which are then processed through a CNN with kernel size 15x15 and stride 1x1, and produces a sentence embedding of size 32x24.\n",
      "Matched pattern: 178\n",
      "5 6\n",
      "The two level system has 178 126 parameters.\n",
      "Matched pattern: 126\n",
      "6 7\n",
      "The two level system has 178 126 parameters.\n",
      "Matched pattern: 15\n",
      "0 1\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 16\n",
      "22 23\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.9290.935\n",
      "37 38\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.0049\n",
      "38 39\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.8990.908\n",
      "41 42\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.0059\n",
      "42 43\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.6620.871\n",
      "45 46\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.0092\n",
      "46 47\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.948\n",
      "49 50\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.974\n",
      "51 52\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.0049\n",
      "52 53\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.879\n",
      "55 56\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.904\n",
      "57 58\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.0021\n",
      "58 59\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.7130.891\n",
      "61 62\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.0015\n",
      "62 63\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.851\n",
      "65 66\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.037\n",
      "66 67\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.611\n",
      "67 68\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.1268\n",
      "68 69\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.815\n",
      "71 72\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.0308\n",
      "72 73\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.620\n",
      "73 74\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.1304\n",
      "74 75\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.779\n",
      "77 78\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.0285\n",
      "78 79\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.602\n",
      "79 80\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.1195\n",
      "80 81\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.9890.995\n",
      "86 87\n",
      "15 A.5 Sentence-level analysis A.5.1 Sample confusion matrices for altered latent values A.5.2 Sentence-level analysis for English data 16 A.6 Detailed task results TRAIN ON TEST ON VAELEVEL VAE BLM agreement typeI typeI 0.9290.935 0.0049 typeI typeII 0.8990.908 0.0059 typeI typeIII 0.6620.871 0.0092 typeII typeI 0.948 e-10 0.974 0.0049 typeII typeII 0.879 e-10 0.904 0.0021 typeII typeIII 0.7130.891 0.0015 typeIII typeI 0.851 0.037 0.611 0.1268 typeIII typeII 0.815 0.0308 0.620 0.1304 typeIII typeIII 0.779 0.0285 0.602 0.1195 BLM verb alternation grouptypeI typeI 0.9890.995 e-10\n",
      "Matched pattern: 0.9070.912\n",
      "2 3\n",
      "typeI typeII 0.9070.912 0.0141 typeI typeIII 0.8090.804 0.0167 typeII typeI 0.9890.996 0.0013 typeII typeII 0.979 e-10 0.984 0.0016 typeII typeIII\n",
      "Matched pattern: 0.0141\n",
      "3 4\n",
      "typeI typeII 0.9070.912 0.0141 typeI typeIII 0.8090.804 0.0167 typeII typeI 0.9890.996 0.0013 typeII typeII 0.979 e-10 0.984 0.0016 typeII typeIII\n",
      "Matched pattern: 0.8090.804\n",
      "6 7\n",
      "typeI typeII 0.9070.912 0.0141 typeI typeIII 0.8090.804 0.0167 typeII typeI 0.9890.996 0.0013 typeII typeII 0.979 e-10 0.984 0.0016 typeII typeIII\n",
      "Matched pattern: 0.0167\n",
      "7 8\n",
      "typeI typeII 0.9070.912 0.0141 typeI typeIII 0.8090.804 0.0167 typeII typeI 0.9890.996 0.0013 typeII typeII 0.979 e-10 0.984 0.0016 typeII typeIII\n",
      "Matched pattern: 0.9890.996\n",
      "10 11\n",
      "typeI typeII 0.9070.912 0.0141 typeI typeIII 0.8090.804 0.0167 typeII typeI 0.9890.996 0.0013 typeII typeII 0.979 e-10 0.984 0.0016 typeII typeIII\n",
      "Matched pattern: 0.0013\n",
      "11 12\n",
      "typeI typeII 0.9070.912 0.0141 typeI typeIII 0.8090.804 0.0167 typeII typeI 0.9890.996 0.0013 typeII typeII 0.979 e-10 0.984 0.0016 typeII typeIII\n",
      "Matched pattern: 0.979\n",
      "14 15\n",
      "typeI typeII 0.9070.912 0.0141 typeI typeIII 0.8090.804 0.0167 typeII typeI 0.9890.996 0.0013 typeII typeII 0.979 e-10 0.984 0.0016 typeII typeIII\n",
      "Matched pattern: 0.984\n",
      "16 17\n",
      "typeI typeII 0.9070.912 0.0141 typeI typeIII 0.8090.804 0.0167 typeII typeI 0.9890.996 0.0013 typeII typeII 0.979 e-10 0.984 0.0016 typeII typeIII\n",
      "Matched pattern: 0.0016\n",
      "17 18\n",
      "typeI typeII 0.9070.912 0.0141 typeI typeIII 0.8090.804 0.0167 typeII typeI 0.9890.996 0.0013 typeII typeII 0.979 e-10 0.984 0.0016 typeII typeIII\n",
      "Matched pattern: 0.9150.928\n",
      "0 1\n",
      "0.9150.928 0.0178 typeIII typeI 0.9970.999 0.0013 typeIII\n",
      "Matched pattern: 0.0178\n",
      "1 2\n",
      "0.9150.928 0.0178 typeIII typeI 0.9970.999 0.0013 typeIII\n",
      "Matched pattern: 0.9970.999\n",
      "4 5\n",
      "0.9150.928 0.0178 typeIII typeI 0.9970.999 0.0013 typeIII\n",
      "Matched pattern: 0.0013\n",
      "5 6\n",
      "0.9150.928 0.0178 typeIII typeI 0.9970.999 0.0013 typeIII\n",
      "Matched pattern: 0.9770.986\n",
      "1 2\n",
      "typeII 0.9770.986 0.0027 typeIII typeIII 0.980.989 0.0003 BLM verb alternation grouptypeI typeI 0.9920.987 0.0033 typeI typeII 0.9110.931 0.0065 typeI typeIII\n",
      "Matched pattern: 0.0027\n",
      "2 3\n",
      "typeII 0.9770.986 0.0027 typeIII typeIII 0.980.989 0.0003 BLM verb alternation grouptypeI typeI 0.9920.987 0.0033 typeI typeII 0.9110.931 0.0065 typeI typeIII\n",
      "Matched pattern: 0.980.989\n",
      "5 6\n",
      "typeII 0.9770.986 0.0027 typeIII typeIII 0.980.989 0.0003 BLM verb alternation grouptypeI typeI 0.9920.987 0.0033 typeI typeII 0.9110.931 0.0065 typeI typeIII\n",
      "Matched pattern: 0.0003\n",
      "6 7\n",
      "typeII 0.9770.986 0.0027 typeIII typeIII 0.980.989 0.0003 BLM verb alternation grouptypeI typeI 0.9920.987 0.0033 typeI typeII 0.9110.931 0.0065 typeI typeIII\n",
      "Matched pattern: 0.9920.987\n",
      "12 13\n",
      "typeII 0.9770.986 0.0027 typeIII typeIII 0.980.989 0.0003 BLM verb alternation grouptypeI typeI 0.9920.987 0.0033 typeI typeII 0.9110.931 0.0065 typeI typeIII\n",
      "Matched pattern: 0.0033\n",
      "13 14\n",
      "typeII 0.9770.986 0.0027 typeIII typeIII 0.980.989 0.0003 BLM verb alternation grouptypeI typeI 0.9920.987 0.0033 typeI typeII 0.9110.931 0.0065 typeI typeIII\n",
      "Matched pattern: 0.9110.931\n",
      "16 17\n",
      "typeII 0.9770.986 0.0027 typeIII typeIII 0.980.989 0.0003 BLM verb alternation grouptypeI typeI 0.9920.987 0.0033 typeI typeII 0.9110.931 0.0065 typeI typeIII\n",
      "Matched pattern: 0.0065\n",
      "17 18\n",
      "typeII 0.9770.986 0.0027 typeIII typeIII 0.980.989 0.0003 BLM verb alternation grouptypeI typeI 0.9920.987 0.0033 typeI typeII 0.9110.931 0.0065 typeI typeIII\n",
      "Matched pattern: 0.8470.869\n",
      "0 1\n",
      "0.8470.869 0.0102 typeII typeI 0.9970.993 0.0025 typeII typeII 0.978 e-10 0.978 0.0017 typeII typeIII 0.9230.956 0.0023 typeIII typeI 0.979 e-10 0.981 0.0022 typeIII typeII 0.9720.975 0.0005 typeIII typeIII 0.9670.977 0.0022 TableAnalysis of systems average F1 std scores overruns for the VAE and 2xV AE systems.\n",
      "Matched pattern: 0.0102\n",
      "1 2\n",
      "0.8470.869 0.0102 typeII typeI 0.9970.993 0.0025 typeII typeII 0.978 e-10 0.978 0.0017 typeII typeIII 0.9230.956 0.0023 typeIII typeI 0.979 e-10 0.981 0.0022 typeIII typeII 0.9720.975 0.0005 typeIII typeIII 0.9670.977 0.0022 TableAnalysis of systems average F1 std scores overruns for the VAE and 2xV AE systems.\n",
      "Matched pattern: 0.9970.993\n",
      "4 5\n",
      "0.8470.869 0.0102 typeII typeI 0.9970.993 0.0025 typeII typeII 0.978 e-10 0.978 0.0017 typeII typeIII 0.9230.956 0.0023 typeIII typeI 0.979 e-10 0.981 0.0022 typeIII typeII 0.9720.975 0.0005 typeIII typeIII 0.9670.977 0.0022 TableAnalysis of systems average F1 std scores overruns for the VAE and 2xV AE systems.\n",
      "Matched pattern: 0.0025\n",
      "5 6\n",
      "0.8470.869 0.0102 typeII typeI 0.9970.993 0.0025 typeII typeII 0.978 e-10 0.978 0.0017 typeII typeIII 0.9230.956 0.0023 typeIII typeI 0.979 e-10 0.981 0.0022 typeIII typeII 0.9720.975 0.0005 typeIII typeIII 0.9670.977 0.0022 TableAnalysis of systems average F1 std scores overruns for the VAE and 2xV AE systems.\n",
      "Matched pattern: 0.978\n",
      "8 9\n",
      "0.8470.869 0.0102 typeII typeI 0.9970.993 0.0025 typeII typeII 0.978 e-10 0.978 0.0017 typeII typeIII 0.9230.956 0.0023 typeIII typeI 0.979 e-10 0.981 0.0022 typeIII typeII 0.9720.975 0.0005 typeIII typeIII 0.9670.977 0.0022 TableAnalysis of systems average F1 std scores overruns for the VAE and 2xV AE systems.\n",
      "Matched pattern: 0.978\n",
      "10 11\n",
      "0.8470.869 0.0102 typeII typeI 0.9970.993 0.0025 typeII typeII 0.978 e-10 0.978 0.0017 typeII typeIII 0.9230.956 0.0023 typeIII typeI 0.979 e-10 0.981 0.0022 typeIII typeII 0.9720.975 0.0005 typeIII typeIII 0.9670.977 0.0022 TableAnalysis of systems average F1 std scores overruns for the VAE and 2xV AE systems.\n",
      "Matched pattern: 0.0017\n",
      "11 12\n",
      "0.8470.869 0.0102 typeII typeI 0.9970.993 0.0025 typeII typeII 0.978 e-10 0.978 0.0017 typeII typeIII 0.9230.956 0.0023 typeIII typeI 0.979 e-10 0.981 0.0022 typeIII typeII 0.9720.975 0.0005 typeIII typeIII 0.9670.977 0.0022 TableAnalysis of systems average F1 std scores overruns for the VAE and 2xV AE systems.\n",
      "Matched pattern: 0.9230.956\n",
      "14 15\n",
      "0.8470.869 0.0102 typeII typeI 0.9970.993 0.0025 typeII typeII 0.978 e-10 0.978 0.0017 typeII typeIII 0.9230.956 0.0023 typeIII typeI 0.979 e-10 0.981 0.0022 typeIII typeII 0.9720.975 0.0005 typeIII typeIII 0.9670.977 0.0022 TableAnalysis of systems average F1 std scores overruns for the VAE and 2xV AE systems.\n",
      "Matched pattern: 0.0023\n",
      "15 16\n",
      "0.8470.869 0.0102 typeII typeI 0.9970.993 0.0025 typeII typeII 0.978 e-10 0.978 0.0017 typeII typeIII 0.9230.956 0.0023 typeIII typeI 0.979 e-10 0.981 0.0022 typeIII typeII 0.9720.975 0.0005 typeIII typeIII 0.9670.977 0.0022 TableAnalysis of systems average F1 std scores overruns for the VAE and 2xV AE systems.\n",
      "Matched pattern: 0.979\n",
      "18 19\n",
      "0.8470.869 0.0102 typeII typeI 0.9970.993 0.0025 typeII typeII 0.978 e-10 0.978 0.0017 typeII typeIII 0.9230.956 0.0023 typeIII typeI 0.979 e-10 0.981 0.0022 typeIII typeII 0.9720.975 0.0005 typeIII typeIII 0.9670.977 0.0022 TableAnalysis of systems average F1 std scores overruns for the VAE and 2xV AE systems.\n",
      "Matched pattern: 0.981\n",
      "20 21\n",
      "0.8470.869 0.0102 typeII typeI 0.9970.993 0.0025 typeII typeII 0.978 e-10 0.978 0.0017 typeII typeIII 0.9230.956 0.0023 typeIII typeI 0.979 e-10 0.981 0.0022 typeIII typeII 0.9720.975 0.0005 typeIII typeIII 0.9670.977 0.0022 TableAnalysis of systems average F1 std scores overruns for the VAE and 2xV AE systems.\n",
      "Matched pattern: 0.0022\n",
      "21 22\n",
      "0.8470.869 0.0102 typeII typeI 0.9970.993 0.0025 typeII typeII 0.978 e-10 0.978 0.0017 typeII typeIII 0.9230.956 0.0023 typeIII typeI 0.979 e-10 0.981 0.0022 typeIII typeII 0.9720.975 0.0005 typeIII typeIII 0.9670.977 0.0022 TableAnalysis of systems average F1 std scores overruns for the VAE and 2xV AE systems.\n",
      "Matched pattern: 0.9720.975\n",
      "24 25\n",
      "0.8470.869 0.0102 typeII typeI 0.9970.993 0.0025 typeII typeII 0.978 e-10 0.978 0.0017 typeII typeIII 0.9230.956 0.0023 typeIII typeI 0.979 e-10 0.981 0.0022 typeIII typeII 0.9720.975 0.0005 typeIII typeIII 0.9670.977 0.0022 TableAnalysis of systems average F1 std scores overruns for the VAE and 2xV AE systems.\n",
      "Matched pattern: 0.0005\n",
      "25 26\n",
      "0.8470.869 0.0102 typeII typeI 0.9970.993 0.0025 typeII typeII 0.978 e-10 0.978 0.0017 typeII typeIII 0.9230.956 0.0023 typeIII typeI 0.979 e-10 0.981 0.0022 typeIII typeII 0.9720.975 0.0005 typeIII typeIII 0.9670.977 0.0022 TableAnalysis of systems average F1 std scores overruns for the VAE and 2xV AE systems.\n",
      "Matched pattern: 0.9670.977\n",
      "28 29\n",
      "0.8470.869 0.0102 typeII typeI 0.9970.993 0.0025 typeII typeII 0.978 e-10 0.978 0.0017 typeII typeIII 0.9230.956 0.0023 typeIII typeI 0.979 e-10 0.981 0.0022 typeIII typeII 0.9720.975 0.0005 typeIII typeIII 0.9670.977 0.0022 TableAnalysis of systems average F1 std scores overruns for the VAE and 2xV AE systems.\n",
      "Matched pattern: 0.0022\n",
      "29 30\n",
      "0.8470.869 0.0102 typeII typeI 0.9970.993 0.0025 typeII typeII 0.978 e-10 0.978 0.0017 typeII typeIII 0.9230.956 0.0023 typeIII typeI 0.979 e-10 0.981 0.0022 typeIII typeII 0.9720.975 0.0005 typeIII typeIII 0.9670.977 0.0022 TableAnalysis of systems average F1 std scores overruns for the VAE and 2xV AE systems.\n",
      "Matched pattern: 2xV\n",
      "42 43\n",
      "0.8470.869 0.0102 typeII typeI 0.9970.993 0.0025 typeII typeII 0.978 e-10 0.978 0.0017 typeII typeIII 0.9230.956 0.0023 typeIII typeI 0.979 e-10 0.981 0.0022 typeIII typeII 0.9720.975 0.0005 typeIII typeIII 0.9670.977 0.0022 TableAnalysis of systems average F1 std scores overruns for the VAE and 2xV AE systems.\n",
      "Matched pattern: 17\n",
      "0 1\n",
      "17 A.7 Detailed error results N1alter and N2alter are sequence errors.\n",
      "Matched pattern: 18\n",
      "0 1\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "for sentence in docs.sents:\n",
    "    matches = matcher(sentence)\n",
    "    for match_id, start, end in matches:\n",
    "        span = sentence[start:end]\n",
    "        print(f\"Matched pattern: {span.text}\")\n",
    "        print(start,end)\n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
